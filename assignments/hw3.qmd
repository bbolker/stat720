---
title: "homework 3"
bibliography: "../stats720.bib"
date: today
date-format: "D MMM YYYY"
---

**Due:** Tuesday November 5. The ideal format for submission is an Rmarkdown or Quarto file, but you can submit it as a Jupyter notebook or as a code file with comments if you prefer. Push your submission to your github repository and send me a note on Piazza to let me know you've done it. You can use R or Python (although the instructions below are R-centric); if you want to use any other language, please check with me first.

**Please make sure your code is reproducible**:

* no absolute path names (e.g. `"C:\My Files\homework"`)
* include any `library()` statements at the top of your file (do not include `install.packages()` statements unless they are commented out)
* make sure I have access to any data files you use in your examples

See also the [R style guide](https://bbolker.github.io/stats720/R_style.html).

```{r pkg, message=FALSE}
library(mlmRev)
library(ggplot2); theme_set(theme_bw())
library(nlme)
library(lme4)
library(lmerTest)
library(glmmTMB)
library(broom.mixed)
library(pbkrtest)
library(patchwork)
```

If your code isn't reproducible I will return it immediately and ask you to fix it.

In general, if I ask you to compare results "qualitatively", you should consider the possibilities "identical or practically identical" (i.e., equivalent up to a tolerance of $\approx 10^{-4}$); "very similar" (equivalent up to a tolerance of 0.01); "slightly different" (equivalent up to 0.1) or "different". (If $p$-values are printed as e.g. `< 2e-16` for two different models/approaches, you can call them "identical or practically identical".)  `all.equal(..., tolerance = 0)` may be useful for making these comparisons, although different packages may yield slightly different structures that makes comparison difficult. (In this case, using `broom.mixed::tidy()` may be useful since it tends to lead to more similar data structures ...)

Consider the `ScotsSec` data set from the `mlmRev` package. 

```{r plot}
gg0 <- ggplot(ScotsSec, aes(y = attain, colour = sex)) + stat_sum(alpha = 0.5) +
    stat_summary(aes(group=interaction(sex, primary)), geom = "line", fun=mean)
gg1 <- gg0 + aes(x=factor(social))
gg2 <- gg0 + aes(x=verbal)
gg1 + gg2
```

a. Using `lmer` (from `lmerTest`, with the default `REML=TRUE`), fit a linear mixed model with educational attainment (`attain`) as the response variable, `social` (as a factor), `sex`, and `verbal` as (additive) fixed effects, primary school (`primary`) as the grouping variable, and with all three of the fixed effects also varying among primary schools.
b. You will get a singular fit. Simplify the model, stepwise, by one of two methods, until it's no longer singular. Either:
* find the random-effects term with the smallest estimated variance and discard it; **or**
* run `rePCA()` on the fitted model. Find the variable most strongly associated with the eigenvector of the smallest eigenvalue (i.e., look for the largest entry in the last column of the eigenvector matrix) and discard it.
c. Use both `performance::check_model()` and `DHARMa` to run diagnostics on your model. What do you conclude? What aspects 
c. Fit the same model with `nlme::lme()` and with `glmmTMB` (make sure to specify `REML=TRUE` to match the defaults for `lme` and `lmer`.
d. Create a named list of your three models. Use `purrr::map_dfr(mod_list, glance, .id = "model")` to compare the overall model fits.
e. Use `purrr::map_dfr(mod_list, ~tidy(., effects = "fixed"), .id = "model") |> dplyr::arrange(term)` to extract the coefficients. Qualitatively compare the estimates, standard errors, df, and p-values among packages. 
f. Use `ggplot2` with the table you created, or `dotwhisker::dwplot()` with `effects = "fixed"`, to generate a coefficient plot of the fixed effects. (Do *not* use `by_2sd=TRUE` [there appears to be a bug somewhere ...]; if you want to separate the scales, use `+ facet_wrap(~term, scale = "free")` to see each set of parameters in its own sub-plot.
g. Use `coef(summary(<lme4_model>))` with `ddf = "Satterthwaite"` and `ddf = "Kenward-Roger"` to compare these two approaches to computing denominator df.  How do these compare with each other, and with the results from `lme`? In this example, how practically important are these differences?
h. For the `lmer` fit, plot the random effect of age for each level (deviation of the slope from the population-level slope) against the corresponding random intercept. (You can use `ranef` or `tidy(..., effects = "ran_vals")`
e. Explain why it does not make sense to treat `social` as a random-effects grouping variable
f. Explain why it would be weird to leave the fixed effect of `social` out of the model while retaining the random variation of `sex` across schools (i.e., fitting a model like `attain ~ 1 + social + verbal + (1 + sex | primary)`).
g. Using `lmerTest` (and REML estimation), fit reduced models with all of the same fixed effects but with random effects intercept variation only. Use (parametric) likelihood ratio tests or AIC to compare the models (i.e., testing the effect of variation in sex effects across primary schools). Then, use parametric bootstrapping (with `pbkrtest::PBmodcomp` or implementing it yourself) to compare the models.  Why is standard LRT/AIC testing problematic here?

## References

