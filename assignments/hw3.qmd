---
title: "homework 3"
bibliography: "../stats720.bib"
date: today
date-format: "D MMM YYYY"
---

**Due:** Monday November 6. The ideal format for submission is an Rmarkdown or Quarto file, but you can submit it as a Jupyter notebook or as a code file with comments if you prefer. Push your submission to your github repository and send me a note on Piazza to let me know you've done it. You can use R or Python (although the instructions below are R-centric); if you want to use any other language, please check with me first.

**Please make sure your code is reproducible**:

* no absolute path names (e.g. `"C:\My Files\homework"`)
* include any `library()` statements at the top of your file (do not include `install.packages()` statements unless they are commented out)
* make sure I have access to any data files you use in your examples

See also the [R style guide](https://bbolker.github.io/stats720/R_style.html).

```{r pkg, message=FALSE}
library(mlmRev)
library(ggplot2); theme_set(theme_bw())
library(nlme)
library(lme4)
library(lmerTest)
library(glmmTMB)
library(broom.mixed)
```

If your code isn't reproducible I will return it immediately and ask you to fix it.

In general, if I ask you to compare results "qualitatively", you should consider the possibilities "identical or practically identical" (i.e., equivalent up to a tolerance of $\approx 10^{-4}$); "very similar" (equivalent up to a tolerance of 0.01); "slightly different" (equivalent up to 0.1) or "different". (If $p$-values are printed as e.g. `< 2e-16` for two different models/approaches, you can call them "identical or practically identical".)  `all.equal(..., tolerance = 0)` may be useful for making these comparisons, although different packages may yield slightly different structures that makes comparison difficult. (In this case, using `broom.mixed::tidy()` may be useful since it tends to lead to more similar data structures ...)

Consider the `Early` data set from the `mlmRev` package. (`mlmRev` cites @singerApplied2003 for this data set. Singer and Willett in turn cite @burchinalEarly1997. It's hard to tell exactly where/whether these data are described in the original paper; the summary statistics from this data set line up *approximately* with the summary statistics given for "Bayley MDI" cognitive scores in Table 1, but not exactly.

```{r plot}
ggplot(Early, aes(age, cog, colour = trt)) + geom_line(aes(group = id))
```

a. Using both `lmer` (from `lmerTest`) and `lme` (from `nlme`), Fit a random-slope linear mixed model with `cog` as the response variable, `age` and `trt` as fixed effects, `id` as the grouping variable and the intercept and effect of `age` varying among individuals. 
b. Extract the fixed-effect coefficients and create a coefficient plot (e.g., using `dotwhisker::dwplot()` or `broom.mixed` plus `ggplot2`), making sure to scale the coefficients (or the predictor variables). Looking at the coefficient summaries, how much do the estimated values and SEs (and therefore Wald CIs) vary between packages? How much do the estimated denominator degrees of freedom (`df`) vary?
c. Compare the estimated denominator df (`ddf`) with the Satterthwaite vs. Kenward-Roger approximations. In this particular example, how important are these differences?
d. Take a random subset of 70% (rounded) of the observatiohs, refit the same models, and compare the magnitudes of the differences with those from the previous two bullet points. (Please show me the code but *not* all of the output - a verbal description of the differences is sufficient.)
e. Going back to the model on the full data, how much do the estimated random effects differ? (It may be useful to use `tidy(..., "ran_pars")`
e. Explain why it does not make sense to treat `trt` as a random variable
f. Explain why it would be weird to leave the fixed effect of `age` out of the model while retaining the random effect of `age` across `id`.
g. Using *either* package, refit the model with maximum likelihood instead of the default REML estimation and compare the differences in the random effect and fixed effect estimates between ML and REML fits.
h. Using `lmerTest` (and REML estimation), fit reduced models with (1) independent intercept and age variation across subjects, (2) intercept variation only. Use (parametric) likelihood ratio tests or AIC to compare the models. Use parametric bootstrap (with `pbkrtest::PBmodcomp` or implementing it yourself) to do likelihood ratio tests comparing the nested series of models (correlated slope/intercept, independent slope/intercept, intercept only). Compare the results.  Why are standard LRT/AIC testing problematic in the second comparison (independent slope/intercept vs intercept only) but **not** the first (correlated vs independent slope/intercept)?
i. using `lme`, fit a model with that treats `age` as a categorical rather than a numeric predictor. In order to do this, you'll have to fix the residual SD to a small value; it will also help to use a different optimizer (i.e., specify `control = lmeControl(sigma = 1e-3, opt = "optim")` when fitting the model). Explain why you need to fix the residual SD in this case.  The results of `intervals(your_model)` are bad? Why, and how is this connected to your explanation?

## References

