---
title: "Generalized linear models"
bibliography: "../stats720.bib"
date: today
date-format: "D MMM YYYY"
format:
  pdf:
    mainfont: TeX Gyre Pagella
    include-in-header:
     - text: \usepackage{marginnote}
---

::: {.content-hidden}
$$
{{< include mathsymbols.tex >}}
$$
:::

<!-- typically renders in docs/ dir  -->

```{r pkgs, message = FALSE}
## it's nice to include packages at the top
## (and NOT automatically install them)
## try not to carry over packages you don't use
library(ggplot2); theme_set(theme_bw())
## diagnostics
library(performance)
library(DHARMa)
## downstream model evaluation
library(broom)
library(dotwhisker)
library(emmeans)
library(effects)
library(marginaleffects)
library(parameters)
## library(ggeffects)
```

## References

@farawayExtending2016, @mccullaghGeneralized1989 (classic), @woodGeneralized2017 [very rapid review]

## Basics

* assume $\y_i \sim \textrm{Dist}(g^{-1}((\X \bbeta)_i))$
* $g$ = **link function**
* $\eta$ = $\X \bbeta$ = **linear predictor**
* **link scale** or **linear predictor** scale vs. **data** or **response** scale
* GLMs inverse-transform $\eta$, they don't transform $y$
* allows:
   * separate control of heteroscedasticity and nonlinearity
   * almost as convenient/efficient as LMs
   * equivalent to MLE in many cases
* the vast majority of GLMs are logistic (binary data) or Poisson
* lots of inference, diagnostics, etc. inherited from LM framework

## Exponential family

* $f(x|\theta) = h(x) g(\theta) \exp(\eta(\theta) T(x))$
* e.g. Poisson:  $f(x|\theta) = \theta^x \exp(-\theta)/x! = (1/x!) \exp(-\theta) \exp(x \log(\theta))$
* $h(x)=1/x!$; $g(\theta) = \exp(-\theta)$; $\eta(\theta) = \log(\theta)$; $T(x) = x$ 
* models with $T(x)=x$ are in the **exponential dispersion** (sub) family [@jorgensenExponentialDispersionModels1987],
* $\eta(\theta)$ is the **canonical link** function for the family (nice mathematical properties, e.g. observed information = expected information [*Fisher( scoring*])
* binomial, Poisson, Gamma (inverse Gaussian, von Mises distribution ...)

## Computation

* iteratively reweighted least squares
   * if we know $\mu_i$, we know $V_i$ (up to a proportion)
   * we can do weighted least squares on the link scale
   * use new $\beta$ estimates to recalculate $\mu_i$ and $V$
* needs starting values, but almost always robust to them
* alternatives/more complex versions: `glmmTMB`, `VGAM` packages
* lots more detail on the computational issues [@mccullaghGeneralized1989; @marschner_glm2_2011; @myers_appendix_2010; @dobson_introduction_2008; @mountHowRobustLogistic2012; @gelmanWhassupGlm2011; @Robinson2010]
* basics (from @farawayExtending2016 p. 155)

## IRLS

Can show (Faraway p. 154) from definition of exponential (dispersion) family that the score equations are equivalent to

$$
\sum_i \frac{(y_i-\mu_i)}{V(\mu_i)} \frac{\partial \mu_i}{\partial \beta_j} = 0
$$

which is equivalent to minimizing

$$
\sum_i \frac{(y_i-\mu_i)^2}{V(\mu_i)}
$$

\begin{itemize}
\item compute \textbf{adjusted dependent variate}:

$$
Z_{(i)} = \hat \eta_{(i)} + (Y-\hat \mu_{(i)}) \left( \frac{d\eta}{d\mu} \right)_{(i)}
$$
(note: $\frac{d\eta}{d\mu} = \frac{d\eta}{d g(\eta)} = 1/g'(\eta)$: 
translate from raw to linear predictor scale)
\item compute \textbf{weights}
$$
W_{(i)}^{-1} = \left( \frac{d\eta}{d\mu}\right)_{(i)}^2 V(\hat \mu_{(i)})
$$
(translate variance from raw to linear predictor scale).
This is the inverse variance of $Z_{(i)}$.
\item regress $z_{(i)}$ on the covariates with weights $W_{(i)}$ to
get new $\bbeta$ estimates ($\to$ new $\eeta$, $\bmu$, $V(\mu)$ \ldots)
\end{itemize}
Tricky bits: starting values, non-convergence, etc.. (We will
worry about these later!)

```{r myglmfit}
myglmfit <- function(y, X, family, tol=1e-8, maxit=50) {
    mu <- y  ## set initial values
    ## set up 'oldbeta' and 'beta' so they're not identical
    oldbeta <- rep(0, ncol(X))
    beta    <- rep(1, ncol(X))
    it <- 1  ## number of iterations
    while (it < maxit && max(abs((1-beta/oldbeta)))>tol) {
        oldbeta <- beta 
        eta <- family$linkfun(mu)    ## calc. linear predictor
        mm <- family$mu.eta(eta)     ## calc. d(mu)/d(eta)
        adjdev <- eta + (y-mu)/mm    ## adjusted response
        W <- c(1/(mm^2*family$variance(mu)))  ## weights
        beta <- lm.wfit(X, adjdev, W)$coefficients  ## weighted least-squares
        mu <- family$linkinv(X %*% beta)          ## compute new mu
        it <- it+1                                ## update
    }
    beta
}
X <- model.matrix(~wool*tension, data=warpbreaks)
y <- warpbreaks$breaks
myglmfit(y,X,poisson())
coef(glm(breaks~wool*tension, data=warpbreaks, family=poisson))
```

## Mean-variance relations

* can show that we need only the link function and the **variance function** $V = f(\mu)$ for computation (may also depend multiplicatively on a **scale** or **dispersion parameter**, e.g. $V = \mu$ for Poisson, $V = \sigma^2$

## Link functions

* canonical doesn't always work best (e.g. Gamma/inverse link)
* probit vs logit; nearly the same shape, interpretational difference
* cloglog; *log-hazard* scale
* inverse link: linear changes in the *rate* of events

## Log-hazards and log-hazard offsets

* if hazard is $h$, probability is $1-\exp(-h)$
* $C(\mu) = \log(-\log(1-\mu))$
* $C^{-1}(\eta) = 1- \exp(-\exp(\eta))$
* $C^{-1}(\eta + \log(\Delta t)) = 1- \exp(-\exp(\eta) \cdot \Delta t)$
* $\to 1-(1-\mu_0)^{\Delta t}$

## in R

* "family" functions contain all of the components needed for GLM fitting, prediction, etc.
* some of the components are weird (e.g. `$aic`)
* canonical link is used by default

```{r family}
names(binomial())
```

## Offsets

* allow for differential search effort, ratios, etc. 
* typically add $\log(e)$
* e.g. $\y \sim \textrm{Poisson}(\X \bbeta + \log(A))$ is equivalent to modeling the response $\y/A$, but without messing up the mean-variance relationship

## Offset/link tricks

* fit an exponential curve with constant variance: `family = gaussian(link = "log")`
* Ricker function $y = a x \exp(-bx)$: log-link, `y ~ x + offset(log(x)`
* Michaelis-Menten $y = a x/(b + x) \to 1/y = (b/a)\cdot (1/x) + 1/a$: inverse-link, `y ~ I(1/x)`

# Model interpretation, visualization, testing

## Parameter interpretation

* log scale: easy
* logit scale: $\approx$ log for low baseline, $\approx \log(1-x)$ for high baseline, slope $\beta/4$ for intermediate values
* cloglog: **log-hazard** scale

## Inference

* Wald tests (no finite-size corrections!)
* approximate Wald CIs (compute then back-transform)
* profile CIs

## Overdispersion (diagnosis)

* too much variance
* SSQ of Pearson residuals  $\sim \chi^2(n-p)$
   * but note @venablesModern2002a p. 209 caution that this is **approximate**
   * simulation-based tests (e.g. `DHARMa`)
   
## Overdispersion (solutions)

* quasi-likelihood (also handles **underdispersion**)
* compounded models (negative binomial, beta-binomial)
* observation-level random effects (== lognormal-Poisson)

## Extended distributions

* `VGAM`, `glmmTMB` packages

## Complete separation

* there is some linear combination of predictors that separates 0 from 1 responses (or 0 from non-zero responses in the case of count models)
* infinite MLE
* Hauck-Donner effect screws up Wald tests
* likelihood ratio tests still OK (sort of)
* Firth logistic regression (`brglm2` package), Bayesian priors (`arm::bayesglm`)
* refs: @firthGeneralizedLinearModels1992, @heinzeSolution2002, @greenlandPenalizationBiasReduction2015, @kosmidisJeffreyspriorPenaltyFiniteness2021

## Zero-inflation/hurdle models

* finite mixture models

## Most common GLM problems

- binomial/Poisson models with non-integer data
- failing to specify `family` (default Gaussian: $\to$ linear model);
using `glm()` for linear models (unnecessary)
- predictions on effect scale
- using $(k,N)$ rather than $(k,N-k)$ with `family=binomial`
- back-transforming SEs rather than CIs
- neglecting overdispersion
- Poisson for *underdispersed* responses
- equating negative binomial with binomial rather than Poisson
- worrying about overdispersion unnecessarily (binary/Gamma)
- ignoring random effects

## Overdispersion in Bernoulli models?

> I think many analysts read that binary models cannot be overdispersed and just do not question it. This happened with the deviance dispersion being the appropriate statistic to measure count model extra-dispersion. Some analysts simply took this on faith, so to speak. But they were mistaken.

[Joseph Hilbe (2013)](http://www.highstat.com/Books/BGS/GLMGLMM/pdfs/HILBE-Can_binary_logistic_models_be_overdispersed2Jul2013.pdf)

## References

::: {#refs}
:::
