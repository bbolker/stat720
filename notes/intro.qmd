---
title: "Introduction<br>(week 1, part 1)"
bibliography: "../stat720.bib"
date: today
date-format: "D MMM YYYY"
format:
  pdf:
    mainfont: TeX Gyre Pagella
    include-in-header:
     - text: \usepackage{marginnote}

---

<!-- typically renders in docs/ dir  -->

# Basics

## Logistics

* (almost) everything at the [course web page](https://bbolker.github.io/stat720)
* communication/forums ([Piazza](https://piazza.com/mcmaster.ca/fall2023/stats720)); e-mail if necessary
* assignment marks (Avenue)
* Zoom/recordings (by request)

## Integrity

* [notes on honesty](../honesty.html)
* why copying code is good
* Stack Overflow, ChatGPT, and all that
* group work

## Prerequisites

From the course outline:

- basics of linear models (as in [STATS 3A03](https://academiccalendars.romcmaster.ca/preview_course_nopop.php?catoid=53&coid=266607)), with associated linear algebra
- basics of generalized linear models (as in [STATS 4C03/6C03](https://academiccalendars.romcmaster.ca/preview_course_nopop.php?catoid=53&coid=266618)), including knowledge of exponential family distributions
- inferential statistics: sampling distributions, Central Limit theorem, hypothesis testing, Wald tests, maximum likelihood estimation
- ideally, *basic* knowledge of Bayesian statistics and Markov chain Monte Carlo estimation
- intermediate knowledge of R

# Goals

* principles/practices of statistical modeling
    * choosing a model
	* diagnostics and troubleshooting
* good intermediate understanding of the tools (ridge/lasso, (G)(LA)MMs); unifying principles of regression modeling, shrinkage/penalized estimators
    * similarity of Bayesian and frequentist approaches
* awareness of computational foundations/scaling

## Scope

* the components:
   * linear model matrices/basis spaces
   * link functions
   * conditional distributions (GLM "families")
   * penalization/shrinkage
* includes a vast range of useful models

## Technical skills & tools

Not focal, but unavoidable and useful

* R (base + some [tidyverse](https://www.tidyverse.org/))
* reproducibility
   * version control (Git/GitHub)
   * documents: Quarto/Sweave/Jupyter notebooks

## about me

* weird background (physics/math u/g, Zoology PhD, epidemiological modeling)
* math biology (ecology/evolution/epidemiology)
* computational statistics (mixed models, Bayesian stats)

## things I like/obsess about

* scientific inference $\gg$ pure prediction (but see @navarroScience2019)
* generative models
* data visualization
* solving problems in context, practical issues
* bad statistical practice (p-value abuse, snooping, dichotomania, imbalance handling, ...)

# The modeling cycle

## Before you start

* you need to know what the question is!
* this is hard for statisticians
* what is a large effect? what is an interesting effect?
* a low $p$-value is not inherently interesting!

## Effect sizes

* standardized measures like Cohen's $d$ ($(\bar x_1 - \bar x_2)/s$, where $s$ is some measure of pooled standard deviation: [Wikipedia](https://en.wikipedia.org/wiki/Effect_size#Standardized_and_unstandardized_effect_sizes)) are common ...
* but shouldn't be used mindlessly. Real-world, unstandardized effects are usually more meaningful 
* effects estimated on the log or logit scale are unitless and hence *may* be easier to generalize
* scaling predictors and responses may help [@schielzethSimple2010]

## An iterative process ...

```{r tidyfig, echo = FALSE}
#| fig-cap: "original from [Mine Çetinkaya-Rundel](https://www.tidyverse.org/blog/2023/08/teach-tidyverse-23/)"
knitr::include_graphics("pix/tidyverse_model.png")
```

```{r boxfig, echo = FALSE}
#| out-width: 7in
#| fig-cap: "From @boxScience1976b"
#| fig-scap: "Box"
knitr::include_graphics("pix/box_cycle.png")
```

\  <!-- hack for spacing -->

```{r MNfig, echo = FALSE}
#| out-width: 7in
#| fig-cap: "From @mccullaghGeneralized1989 p. 392: 'The introduction of this loop changes profoundly the process of analysis and the reliability of the final models found.'" 
#| fig-scap: "MN"
knitr::include_graphics("pix/MN_cycle.png")
```

## Beware the garden of forking paths!

```{r forking_paths, echo=FALSE}
#| fig-cap: "from [Art Share LA](https://artsharela.org/event/carillon-quartet-the-garden-of-forking-paths/)"
knitr::include_graphics("pix/forking_paths.jpg")
```
* "researcher degrees of freedom", "HARKing", etc.
* @simmons_false-positive_2011; @gelman_statistical_2014

## Solutions?

* pre-registration (formal or informal); report deviations from planned analysis
* choose model complexity (see Harrell RMS ch. 3), do diagnostics etc., **without reference to response variable** or metrics of significance

## Choosing model complexity

* see Harrell ch. 3
* rules of thumb for inferential models with adequate power
    * e.g. $p < n/10$ or $n/20$
	* effective $n$ depends on data type (binary < small counts < continuous)
	* how does clustering/correlation in data change effective $n$?
* simplest if done **a priori**
    * data-driven choice of model complexity (e.g. by cross-validation), *while maintaining valid inference*, is delicate

## Model diagnostics

* all models make assumptions
* results *may* be sensitive to **misspecification**: bias, inefficiency, inflated/deflated type I error, poor coverage ...
* hypothesis tests (e.g. Shapiro-Wilk) are deprecated

Harvey Motulsky on [CrossValidated](https://stats.stackexchange.com/a/2501/2126):

> The question normality tests answer: Is there convincing evidence of any deviation from the Gaussian ideal? With moderately large real data sets, the answer is almost always yes.

> The question scientists often expect the normality test to answer: Do the data deviate enough from the Gaussian ideal to "forbid" use of a test that assumes a Gaussian distribution? Scientists often want the normality test to be the referee that decides when to abandon conventional (ANOVA, etc.) tests and instead analyze transformed data or use a rank-based nonparametric test or a resampling or bootstrap approach. For this purpose, normality tests are not very useful.

* "is there a statistically significant deviation from the model assumptions?" vs. "are the violations of the assumptions large enough to mess up my conclusions?" (**never** "are the data normally distributed?")
    * **data "too big"**: will reject assumptions even when it's OK
	* **data "too small"**: will fail to reject assumptions even when they're problematic (???)
* **two-stage testing** often has bad properties [@campbell_consequences_2014; @campbellconsequences2021a; @rochon_test_2012; @zimmermannote2004]

* graphical diagnostics are often recommended
* but how do we judge whether deviations are too large? ([Q-Q plot survey](https://ms.mcmaster.ca/~bolker/misc/qq_analysis.html))

## Untestable issues are often the worst problems

* biased/non-representative sampling [@mengStatistical2018]
* lack of randomization 
* problems with causal inference: unobserved confounders, etc.
* non-independence
  * pseudo-replication [@hurlbertPseudoreplication1984]
  * temporal/spatial structure

## More on model diagnostics

* assumptions apply to **conditional** distribution of the response (not the marginal distribution, not the predictor variables)
* test assumptions in order of importance (George Box: "It is inappropriate to be concerned about mice when there are tigers abroad.")
   * bias/nonlinearity (residuals vs fitted plot)
   * heteroscedasticity (scale-location plot)
   * influential points/outliers (Cook's distance, leverage, etc.)
   * distributional assumptions (Q-Q plot)
   * ¿¿ correlated predictors ??
* `performance`, `DHARMa` packages: more later

## References


::: {#refs}
:::
