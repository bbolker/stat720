---
title: "Introduction<br>(week 1, part 1)"
bibliography: "../stats720.bib"
date: today
date-format: "D MMM YYYY"
format:
  pdf:
    mainfont: TeX Gyre Pagella
    include-in-header:
     - text: \usepackage{marginnote}
---

<!-- typically renders in docs/ dir  -->

# Basics

## Land acknowledgement

McMaster University (and this class) is on the traditional territories of the Mississauga and Haudenosaunee nations, and within the lands protected by the “Dish with One Spoon” wampum agreement. (**Why?** See ["Beyond Territorial Acknowledgements"](https://apihtawikosisan.com/2016/09/beyond-territorial-acknowledgments/))

## Logistics

* (almost) everything at the [course web page](https://bbolker.github.io/stats720)
* communication/forums ([Piazza](https://piazza.com/mcmaster.ca/fall2024/stats720)); e-mail if necessary
* assignment marks (Avenue)
* Zoom/recordings (can be set up by request)

## Integrity

* [notes on honesty](../honesty.html)
* why copying code is good
* Stack Overflow, ChatGPT, and all that
* group work

## Prerequisites

From the course outline:

- basics of linear models (as in [STATS 3A03](https://academiccalendars.romcmaster.ca/preview_course_nopop.php?catoid=53&coid=266607)), with associated linear algebra
- basics of generalized linear models (as in [STATS 4C03/6C03](https://academiccalendars.romcmaster.ca/preview_course_nopop.php?catoid=53&coid=266618)), including knowledge of exponential family distributions
- inferential statistics: sampling distributions, Central Limit theorem, hypothesis testing, Wald tests, maximum likelihood estimation
- ideally, *basic* knowledge of Bayesian statistics and Markov chain Monte Carlo estimation
- intermediate knowledge of R

# Goals

* principles/practices of statistical modeling
    * choosing a model
	* diagnostics and troubleshooting
	* interpreting and communicating results
* understanding the tools ((penalized) (G)L(A)(M)Ms; unifying principles of regression modeling, shrinkage/penalized estimators
    * both frequentist and Bayesian approaches
* awareness of computational foundations/scaling

## Scope

* **regression** modeling
* the components:
   * linear model matrices/basis spaces
   * link functions
   * conditional distributions (GLM "families")
   * penalization/shrinkage
* includes a vast range of useful models
* **not** clustering, neural nets, tree-based models

## Technical skills & tools

Not focal, but unavoidable and useful

* R (base + some [tidyverse](https://www.tidyverse.org/))
* reproducibility (@bryanProjectoriented2017; @bryanExcuse2017)
   * version control (Git/GitHub)
   * documents: [Quarto](http://quarto.org)

## about me

* weird background (physics/math u/g, Zoology PhD, epidemiological modeling)
* math biology (ecology/evolution/epidemiology)
* computational statistics (mixed models, Bayesian stats)

## things I like/obsess about

* scientific inference $\gg$ pure prediction (but see @navarroScience2019)
* data visualization
* solving problems in context, practical issues
* battling bad statistical practice (p-value abuse, snooping, dichotomania, imbalance handling, ...)

## a tiny bit of philosophy

* frequentist vs. Bayesian
* *computational* or *pragmatic* Bayesian
* in a perfect world either choice would be available for any problem
   * Bayesian approaches generally more flexible but slower
   * priors!
* hierarchical/latent/mixed models make the boundaries fuzzy (*empirical Bayesian*)

## strong feelings about p-values

* please use frequentist methods correctly
* $H_0$ is almost never true
* **never** accept the null hypothesis (use *equivalence tests* if you really care)
* a low $p$ value doesn't mean the effect is large or important
* may help to frame results in terms of **clarity** [@dushoffcan2018]

# The modeling cycle

## Before you start

* you need to know what the question is!
* this is hard for statisticians
   * when mining data, go back to the original paper(s)
* a low $p$-value is not inherently interesting!
* what is a large effect? what is an interesting effect?

## From X/Twitter

```{r twitQ, echo = FALSE}
#| fig-cap: "from [user rheum_cat on X](https://x.com/rheum_cat/status/1699952880763941310?s=20)"
knitr::include_graphics("pix/twitter_question.png")
```

## Effect sizes

* standardized measures like Cohen's $d$ ($(\bar x_1 - \bar x_2)/s$, where $s$ is some measure of pooled standard deviation: [Wikipedia](https://en.wikipedia.org/wiki/Effect_size#Standardized_and_unstandardized_effect_sizes)) are common ...
* but shouldn't be used mindlessly. Real-world, unstandardized effects are usually more meaningful 
* effects estimated on the log or logit scale are unitless and hence *may* be easier to generalize
* scaling predictors and responses may help [@schielzethSimple2010]

## An iterative process ...

```{r tidyfig, echo = FALSE}
#| fig-cap: "original from [Mine Çetinkaya-Rundel](https://www.tidyverse.org/blog/2023/08/teach-tidyverse-23/)"
knitr::include_graphics("pix/tidyverse_model.png")
```

```{r boxfig, echo = FALSE}
#| out-width: 7in
#| fig-cap: "From @boxScience1976b"
#| fig-scap: "Box"
knitr::include_graphics("pix/box_cycle.png")
```

\  <!-- hack for spacing -->

```{r MNfig, echo = FALSE}
#| out-width: 7in
#| fig-cap: "From @mccullaghGeneralized1989 p. 392: 'The introduction of this loop changes profoundly the process of analysis and the reliability of the final models found.'" 
#| fig-scap: "MN"
knitr::include_graphics("pix/MN_cycle.png")
```

## Beware the garden of forking paths!

```{r forking_paths, echo=FALSE}
#| fig-cap: "from [Art Share LA](https://artsharela.org/event/carillon-quartet-the-garden-of-forking-paths/)"
knitr::include_graphics("pix/forking_paths.jpg")
```
* "researcher degrees of freedom", "HARKing", etc.
* @simmons_false-positive_2011; @gelman_statistical_2014

## Solutions?

* pre-registration (formal or informal); report deviations from planned analysis
* choose model complexity (see Harrell RMS ch. 4), do diagnostics etc., **without reference to response variable** or metrics of significance

## Choosing model complexity

* see Harrell ch. 3
* rules of thumb for inferential models with adequate power
    * e.g. $p < n/10$ or $n/20$
	* effective $n$ depends on data type (binary < small counts < continuous)
	* how does clustering/correlation in data change effective $n$?
* simplest if done **a priori**
    * data-driven choice of model complexity (e.g. by cross-validation), *while maintaining valid inference*, is delicate

## Comparing models with reality

```{r hennig, echo=FALSE}
#| fig-cap: "one view of models and reality, from @hennigTesting2022"
knitr::include_graphics("pix/hennig_models.png")
```

## Model diagnostics

* all models make assumptions
* results *may* be sensitive to **misspecification**: bias, inefficiency, inflated/deflated type I error, poor coverage ...
* hypothesis tests (e.g. Shapiro-Wilk) are deprecated

Harvey Motulsky on [CrossValidated](https://stats.stackexchange.com/a/2501/2126):

> The question normality tests answer: Is there convincing evidence of any deviation from the Gaussian ideal? With moderately large real data sets, the answer is almost always yes.

> The question scientists often expect the normality test to answer: Do the data deviate enough from the Gaussian ideal to "forbid" use of a test that assumes a Gaussian distribution? Scientists often want the normality test to be the referee that decides when to abandon conventional (ANOVA, etc.) tests and instead analyze transformed data or use a rank-based nonparametric test or a resampling or bootstrap approach. For this purpose, normality tests are not very useful.

* "is there a statistically significant deviation from the model assumptions?" vs. "are the violations of the assumptions large enough to mess up my conclusions?" (**never** "are the data normally distributed?")
    * **data "too big"**: will reject assumptions even when it's OK
	* **data "too small"**: will fail to reject assumptions even when they're problematic (???)
* **two-stage testing** often has bad properties [@campbell_consequences_2014; @campbellconsequences2021a; @rochon_test_2012; @zimmermannote2004; @shamsudheenShould2021]

* graphical diagnostics are often recommended
* but how do we judge whether deviations are too large? ([Q-Q plot survey](https://ms.mcmaster.ca/~bolker/misc/qq_analysis.html))
* **open question**

## Untestable issues are often the worst problems

* biased/non-representative sampling [@mengStatistical2018]
* lack of randomization 
* problems with causal inference: unobserved confounders, etc.
* non-independence
  * pseudo-replication [@hurlbertPseudoreplication1984]
  * temporal/spatial structure

## More on model diagnostics

* assumptions apply to **conditional** distribution of the response (not the marginal distribution, not the predictor variables)
* test assumptions in order of importance (George Box: "It is inappropriate to be concerned about mice when there are tigers abroad.")
   * bias/nonlinearity (residuals vs fitted plot)
   * heteroscedasticity (scale-location plot)
   * influential points/outliers (Cook's distance, leverage, etc.)
   * distributional assumptions (Q-Q plot)
   * ¿¿ correlated predictors ??
* `performance`, `DHARMa` packages: more later

## References


::: {#refs}
:::
