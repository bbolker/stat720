---
title: "Review of linear models"
bibliography: "../stats720.bib"
date: today
date-format: "D MMM YYYY"
format:
  pdf:
    mainfont: TeX Gyre Pagella
    include-in-header:
     - text: \usepackage{marginnote}
---

::: {.content-hidden}
$$
{{< include mathsymbols.tex >}}
$$
:::

<!-- typically renders in docs/ dir  -->

```{r pkgs, message = FALSE}
## it's nice to include packages at the top
## (and NOT automatically install them)
## try not to carry over packages you don't use
library(faux)
library(brglm2) ## for lizards data
library(ggplot2); theme_set(theme_bw())
## diagnostics
library(performance)
library(DHARMa)
## downstream model evaluation
library(broom)
library(dotwhisker)
library(emmeans)
library(effects)
library(marginaleffects)
library(parameters)
## library(ggeffects)
```

## Basics

* assume $\y \sim \textrm{Normal}(\X \bbeta, \sigma)$^[Notation-abuse warning ...]
* $\X$ is the *model matrix*, can be anything we want it to be
* the *Gauss-Markov theorem* ([Wikipedia](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem)) makes weaker assumptions: $\y = \X \bbeta + \beps$; as long as $\beps$ is mean-zero, homoscedastic with finite variance, and uncorrelated ... then the OLS solution
$$
\hat{\bbeta} = (\t{\X} \X)^{-1} \t{\X} \y
$$
is the BLUE (or MVUE).
* we'll embrace the assumptions (which are needed for inference!)

## Computation

* matrix decompositions (QR with pivoting; see [here](https://stackoverflow.com/questions/9071020/compute-projection-hat-matrix-via-qr-factorization-svd-and-cholesky-factoriz))
* big problems: `biglm`, `speedglm`, `RcppEigen::fastLm`
   * optimized BLAS, kernel trick, etc.
   * memory vs speed vs robustness ...
   * $p$ vs. $n$ vs. many-small-regressions vs. ...

## Inference

* $\sigma^2$ (residual variance) is $\textrm{RSS}/(n-p)$
* The covariance matrix is $\Sigma = \sigma^2 (\t{\X} \X)^{-1}$. 
* Individual coefficients are $t$-distributed
* Linear combinations of coefficients (contrasts or predictions) are $t$-distributed with covariance matrix $\t{\C} \Sigma^{-1} \C$
* Joint hypotheses on coefficients are $F$-distributed
* Wald and likelihood ratio test comparisons are equivalent  
(but need to be careful about marginality)

## Model matrices

* model definition converted to $\X$ before we start
* **input variables** vs **predictor variables** (@schielzethSimple2010, @gelmanData2006, [CV](https://stats.stackexchange.com/questions/511455/terminology-for-raw-vs-derived-predictor-variables))
   * transformations
   * encoding of categorical variables: **contrasts**
   * interactions
   * basis expansions (e.g. polynomials)

## Wilkinson-Rogers formulas 

- @wilkinsonSymbolic1973a, updated by @chambersStatistical1991 [ch. 2]
- operators: `+`, `*`, `:`, `/`, `-`, `^`
- `I()`

## Contrasts

### treatment contrasts

* intercept = baseline, subsequent values are differences
* $\{\beta_0 = \mu_0, \beta_i = \mu_i - \mu_0~\textrm{for}~i>0\}$
* equivalently: $\{\mu_0 = \beta_0, \mu_i = \beta_0 + \beta_i~\textrm{for}~i>0\}$
* **contrast matrix**:

$$
\C \bbeta = \left(\begin{array}{cccc}
1 & 0 & 0 & \ldots \\
1 & 1 & 0 & \ldots \\
1 & 0 & 1 & \ldots \\
\vdots & \vdots & \vdots & \ddots
\end{array}
\right)
\left(\begin{array}{c}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \ldots
\end{array}
\right) =
\left(\begin{array}{c}
\mu_1 \\ \mu_2 \\ \mu_3 \\ \ldots
\end{array}\right)  \quad .
$$

Maybe easier to start from the **inverse** contrast matrix: $\bbeta = \C^{-1} \bmu$.

```{r invmat}
C <- cbind(1, contr.treatment(3))  ## R omits the intercept by default
solve(C)
```

We have to specify the baseline level (`contr.treatment` uses first level of a factor; `contr.SAS()` uses the last level).

It's nice when contrasts are *orthogonal*, i.e. all rows are independent $\to \t{\C} \C$ is diagonal.

### Sum-to-zero contrasts

* intercept is the (unweighted!) average rather than baseline value ($\sum \mu_i/n$)
* other parameters are differences between mean of level $i$ and intercept ($\mu_i - \sum_j \mu_j/n$)
* **last** level is dropped 

```{r sumtozero}
mfun <- function(C) MASS::fractions(solve(C))
(C <- cbind(1,contr.sum(3)))
mfun(C)
```

### Helmert contrasts

* Weird but orthogonal
* intercept, diff of first two levels, diff of level 3 from 1 & 2, ...

```{r helm}
(C <- cbind(1,contr.helmert(3)))
mfun(C)
```

### others

* `MASS::contr.sdif()` (successive-differences)
* `contr.poly()` (orthogonal polynomial contrasts)
* custom (e.g., "none" vs "symbiont effect" vs "crabs vs shrimp" vs "two-symbiont effect") (@mckeonMultiple2012b; data [here](https://github.com/bbolker/mixedmodels-misc/blob/master/data/culcita.RData))

```{r symbionts}
#| code-fold: true
cc_inv <- matrix(c(1/4,1/4,1/4,1/4,
               1,-1/3,-1/3,-1/3,
               0,1,-1,0,
               0,1/2,1/2,-1),
             byrow=TRUE,
             nrow=4,
             dimnames=list(c("intercept","avg_symb","C.vs.S","twosymb"),
                           c("none","C","S","CS")))
## inverse contrast matrix
MASS::fractions(cc_inv)
## contrast matrix
mfun(cc_inv)
```

## practical issues

* too many ways to set contrasts (`options()`, `contrasts(f) <-`, `lm(..., contrasts = list(...))`
* terrible naming conventions: you can get used to it or use the `faux` package
* OK to fit models and later use `emmeans` to recover desired contrasts (switching linear bases)

```{r contrasts}
mtcars$fcyl <- factor(mtcars$cyl)
lm0 <- lm(mpg ~ fcyl, mtcars)
cn <- function(x) names(coef(x))
cn(lm0)
update(lm0, contrasts = list(fcyl = contr.sum(3))) |> cn()
update(lm0, contrasts = list(fcyl = contr.helmert(3))) |> cn()
```

## using `faux`

```{r faux, message = FALSE}
update(lm0, data = transform(mtcars, fcyl = contr_code_sum(fcyl))) |> cn()
update(lm0, data = transform(mtcars, fcyl = contr_code_helmert(fcyl))) |> cn()
```

## Interactions

* differences in differences
* parameter values of main effects (and $p$ values etc.) depend on contrasts/centering!
* overall model fit ($R^2$, predictions, etc.) is invariant

Lizard data (@schoenerNonsynchronous1970, from the `brglm2` package):

```{r lizards_dataplot}
#| code-fold: true
data("lizards", package = "brglm2")
ggplot(lizards, aes(time, grahami, colour = light)) +
    stat_summary(fun.data = mean_cl_boot,
                 position = position_dodge(width = 0.25))
```

```{r lizards_addplot}
#| code-fold: true
data("lizards", package = "brglm2")
lmTL1 <- lm(grahami~time+light,data=lizards)
pp <- with(lizards,expand.grid(time=levels(time),light=levels(light)))
pp$grahami <- predict(lmTL1,newdata=pp)
cc <- as.list(plyr::rename(coef(lmTL1),c(`(Intercept)`="int")))
labelpos <- with(cc,
  list(x=c(1,2,3,1),xend=c(1,2,3,1),
      y=c(int,int,int,int),
      yend=c(int,int+timemidday,int+timelate,int+lightshady)))
xpos <- -0.1
ggplot(pp,aes(x=time,y=grahami,colour=light))+geom_point()+
  geom_line(aes(group=light))+
  annotate("segment",x=labelpos$x,xend=labelpos$xend,y=labelpos$y,
           yend=labelpos$yend,alpha=0.5,
           arrow=arrow(length = unit(0.3,"cm"),ends="both"))+
  annotate("text",x=with(labelpos,(x+xend)/2)+xpos,y=with(labelpos,(y+yend)/2),
label=paste0("beta[",1:4,"]"),parse=TRUE)+
  annotate("segment",x=labelpos$x[1],xend=labelpos$x[3],y=labelpos$y[1],
           yend=labelpos$y[1],alpha=0.3,lty=2) +
    labs(title = "additive model")
```

```{r intplot}
#| code-fold: true
lmTL2 <- lm(grahami~time*light,data=lizards)
gg_color_hue <- function(n) {
  hues = seq(15, 375, length=n+1)
  hcl(h=hues, l=65, c=100)[1:n]
}
pp2 <- pp
pp2$grahami <- predict(lmTL2,newdata=pp)
cc <- as.list(plyr::rename(coef(lmTL2),c(`(Intercept)`="int",
        `timemidday:lightshady`="midshady",`timelate:lightshady`="lateshady")))
labelpos <- with(cc,
  list(x=c(1,2,3,1,2,3),xend=c(1,2,3,1,2,3),
      y=c(int,int,int,int,int+lightshady+timemidday,int+lightshady+timelate),
      yend=c(int,int+timemidday,int+timelate,int+lightshady,
             int+timemidday+lightshady+midshady,int+timelate+lightshady+lateshady)))
xpos <- -0.1
ggplot(pp2,aes(x=time,y=grahami,colour=light))+geom_point()+
  geom_line(aes(group=light))+
  annotate("segment",x=1:2,xend=2:3,
           y=with(cc,c(int+lightshady,int+timemidday+lightshady)),
           yend=with(cc,c(int+timemidday+lightshady,int+timelate+lightshady)),
           colour=gg_color_hue(2)[2],lty=2)+
  annotate("segment",x=labelpos$x,xend=labelpos$xend,y=labelpos$y,
           yend=labelpos$yend,alpha=0.5) +
           ## arrow=arrow(length = unit(0.3,"cm"),ends="both"))+
  annotate("text",x=with(labelpos,(x+xend)/2)+xpos,y=with(labelpos,(y+yend)/2),
label=paste0("beta[",1:6,"]"),parse=TRUE)+
  annotate("segment",x=rep(labelpos$x[1],2),
                     xend=rep(labelpos$x[3],2),
                     y=labelpos$yend[c(1,4)],
           yend=labelpos$yend[c(1,4)],alpha=0.3,lty=2) +
    labs(title = "interaction model")
```

## Marginality

* @venablesExegeses1998
* 'type (X) sums of squares'
* scaling and centering [@schielzethSimple2010] alleviates many problems; sum-to-zero contrasts (weighted or unweighted?)

# Model interpretation, visualization, testing

## Diagnostics

* linearity > heteroscedasticity, outliers > normality
* upstream problems can induce downstream problems first
* universal plots are universal, but less interpretable than problem-specific exploration (try to identify problematic predictors/groups/etc.)

## Graphical diagnostics

* base R: `stats::plot.lm()`
* `performance::check_model()`
* `DHARMa` (`simulateResiduals(., plot = TRUE)`)
   * (`plotResiduals(simout, form = pred_var)`)
* `broom::augment()` + plot-your-own (`ggplot2`)

```{r check}
performance::check_model(lmTL2)
```

```{r DHARMa}
ss <- simulateResiduals(lmTL2)
plot(ss)
plot(ss, form = lizards$time)
```

## Solutions to problems

- **nonlinearity**: transformation, add covariates (??), add interactions, add polynomial terms etc.
- **outliers**: drop values (report both!), use robust regression
- **heteroscedasticity**: transformation, model dispersion explicitly, GLMs
- **non-Normality**: transformation, GLMs

## Transformation

* May do too much at once (GLMs and GAMs allow more flexibility)
* Log-transformation is often interpretable and solves problems
* Transforming boundary values (e.g. $\log(0)$) is problematic
* **Box-Cox transformations**: $y \to \frac{y^\lambda - 1}{\lambda}$ (include Jacobian term $\textrm{GM}^{\lambda-1}$ in denominator to keep log-likelihood comparable)
   * flexible 
   * in practice people often use 'round numbers': $\lambda = 0$ (log), 1/2 (square root), etc.
   * `MASS::boxcox()`
   * hard to interpret!

## What about correlated predictors?

- Can compute *variance inflation factors* (VIFs)
- Dropping correlated factors is dubious: @grahamConfronting2003, @dormannCollinearity2012, @morrisseyMultiple2018a, @vanhoveCollinearity2021
- perfect collinearity gets handled automatically by R's pivoting, but may want to change contrasts/model setup

```{r interpret}
summary(lmTL2)
broom::tidy(lmTL2)
## automatically drop intercept; optional by_2sd argument
dotwhisker::dwplot(lmTL2) + geom_vline(xintercept = 0, lty = 2) +
    labs(x="difference in grahami count")
```

## Interpretation and testing

* Look at coefficient tables: `summary()` or `coef(summary())`
* model comparison: `drop1()`, `anova()`, `car::Anova()`
* coefficient *plots*: `broom` + `ggplot2`, `dotwhisker`

## Downstream methods

* plot predictions **with data**
* partial residuals plots (e.g. [remef package](https://github.com/hohenstein/remef))
* prediction, effects plots
* uncertainty of predictions
* `emmeans`, `marginaleffects`, `effects`, `sjPlot` ...

## References

::: {#refs}
:::
