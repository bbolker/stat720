---
title: "Generalized additive (mixed) models"
bibliography: "../stats720.bib"
---

::: {.content-hidden}
$$
{{< include mathsymbols.tex >}}
$$
:::

```{r pkgs, message = FALSE}
library(mgcv)
library(gratia)
library(tidyverse)
```

## Additive models

* generally a way to specify more complex (smooth) terms based on *individual* covariates: $\mu = \beta_0 + f_1(x_1) + f_2(x_2) + \ldots$
* lots of ways to generate $f_i(x_i)$: kernel estimators, locally weighted polynomials, ... see @hastieGeneralized1990,  @hastieElements2009 (*backfitting algorithm* etc.)
* we will focus on 

## Basis expansions

* (theoretically) infinitely expandable
* e.g. polynomials ('regular'/raw, orthogonal, Legendre, Hermite)
* wavelet, Fourier
- splines: **piecewise polynomial** with continuity/smoothness constraints

## Spline degree

```{r}
xvec <- seq(0, 1, length.out = 101)
sfun <- function(d = 3, type = "bs", ...) {
    if (type == "bs") {
        X <- splines::bs(xvec, df = 10, degree = d)
    } else {
        X <- splines::ns(xvec, df = 10)
    }
	par(bty = "l", las = 1) 
	matplot(xvec, X, type = "l", lty = 1, ...)
}
sfun(d = 1, main = "degree-1")
sfun(d = 2, main = "degree-2")
sfun(d = 3, main = "degree-3")
```

## spline terminology

* **knots**: breakpoints (boundary, interior)
* order-M (ESL): continuous derivatives up to order $M-2$ (cubic, $M=4$)
* typically $M=1$, 2, 4
* number of knots = df (degrees of freedom) -1 -intercept

## Spline choices

* continuous derivatives up to $d-1$
* truncated polynomial basis (simple)
* B-splines: complex, but *minimal support*/maximum sparsity
* *natural* splines: extra constraint, derivatives > 1 vanish at boundaries

```{r b_vs_n, fig.width = 10}
par(mfrow = c(1,2))
sfun()
sfun(type = "ns")
```

## choosing knot locations

* generally not that important: evenly spaced, *or* evenly spaced based on quantiles

## choosing basis dimension

* in principle could expand dimension to match total number of points (*interpolation spline*)
* ... but that would overfit
* AIC, adjusted $R^2$, cross-validation ...

## smoothing splines

* as many knots as data points
* plus squared-second-derivative ("wiggliness") penalty

$$
\textrm{RSS} + \lambda \int (f''(t))^2 \, dt
$$

* defined on an infinite-dimensional space
* minimizer is a natural cubic spline with knots at $x_i$

$$
(\y - \Z  \bb)^\top (\y - \Z \bb) + \lambda \bb^\top \OOmega \bb
$$
with $\{\OOmega\}_{jk} = \int \Z_j''(t) \Z_k''(t) \, dt$
$$

* **generalized** ridge regression: penalize by $\lambda \OOmega_N$ rather than $\lambda I$
* same data augmentation methods as before except that now we use $\sqrt{\lambda} C$ where $C$ is a matrix, and the "square root" (Cholesky factor) of $\OOmega_N$

See @woodGeneralized2017, @perperogloureview2019a

## connection to mixed models

* note that $\lambda \bb^\top \OOmega \bb$ is equivalent to $(1/\sigma^2) \bb \Sigma'^{-1} \bb^\top$; if $\Sigma'$ is a *scaled* covariance matrix (i.e. $\Sigma = \sigma^2 \Sigma$), then this is the core of the MVN log-likelihood $\log {\cal L}(\bb|\Sigma)$ (all we're missing is a factor of $\textrm{Det}(\Sigma)^{-1/2}$ and a normalization constant)
* So we can fit this with any of the mixed model machinery, provided we can set up the correct covariance matrix

## generalized cross-validation score

* very close to AIC
* @larsenGAM2015, @golubGeneralized1979
* minimize $\textrm{RSS}/(\textrm{Tr}(\I-\bS(\lambda)))^2$, where $S$ is 
* "a rotation-invariant version of PRESS" ($\sum (e_i/(1-h_{ii}))^2$)
* replace RSS with approximation of deviance,
$$
|| \sqrt{\W} (\z - \X \bbeta)||^2
$$
for generalized (non-Gaussian) models

## ML criterion, REML criterion

* treat spline smoothing as a *mixed model* problem
* spline (penalized) parameters are $\uu$
* $y|u \sim N(\X\bbeta + \Z \uu, \sigma^2 \I)$; $\uu \sim N(0, (\sigma^2/\lambda) \W^{-1})$
* where the $\W$ is the penalty matrix
* corresponds to minimizing $||\y - \X\bbeta - \Z \uu||^2 + \lambda \uu^\top \W \uu$
* "fixed effects are viewed as random effects with improper uniform priors and are integrated out" (Wood 2011)
* Laplace approximation

## practical stuff

* Simon Wood is insanely smart, and `mgcv` is insanely powerful and flexible
* [gratia package](https://gavinsimpson.github.io/gratia/) (named after [Grace Wahba](https://en.wikipedia.org/wiki/Grace_Wahba)
* available 'smooths' (bases + penalty terms): look for strings of the form `smooth.construct.*.smooth.spec`
* although you can *theoretically* have as many knots as data points, fewer is often good enough/computationally efficient

```{r basis_choices, echo = FALSE}
apropos("smooth.construct") |> gsub(pattern = "smooth\\.construct\\.|\\.smooth\\.spec", replacement = "") |> grep(pattern = "smooth", invert = TRUE, value = TRUE)
```

```{r gam1}
g1 <- gam(mpg ~ s(hp), data = mtcars)
summary(g1)
```

Plot:

```{r gam_plot1, fig.width = 12, fig.height = 6}
plot(g1)
```

Check:

```{r gam_check}
gam.check(g1)
```

```{r draw_appraise}
draw(g1)
appraise(g1)
```

```{r gam2}
g2 <- gam(mpg ~ s(hp), data = mtcars, fit = FALSE)
```

**concurvity**: [CV question](https://stats.stackexchange.com/questions/401401/what-is-the-acceptable-level-of-concurvity), @ramsayEffect2003

```{r}
g3 <- gam(mpg ~ s(hp) + s(wt), data = mtcars)
draw(g3)
concurvity(g3)
```

Many options: simple random effects (`bs = "re"`); *cyclic* splines (make $x(0) = x(T)$; `bs="cc"`) ; multidimensional splines (thin-plate, *tensor product* (`te()`); spherical (*Duchon*) splines (`bs = "sos"`); Markov random fields (`bs = "mrf"`); Gaussian processes (`bs = "gp"`); splines by category (`by=` argument); hierarchical splines [@pedersenHierarchical2019]; constrained splines (`scam` package, @pyaShape2015); *soap film* splines; etc etc etc etc ...

![](pix/hgam.png)


## Duality between $\Z$ and correlation structure

* @hefleyBasis2017
* "first-order specification": $\y \sim N(\X \bbeta + \Z \bb, \sigma^2_\epsilon \I)$
* "second-order specification: $\y \sim N(\X \bbeta, \sigma^2_\epsilon \I + \sigma^2_\bb \Sigma)$
* if $\bb$ are iid Normal, integrating first-order specification shows that $\Sigma = \Z \Z^\top$
* e.g. latent-variable specification of an AR1 correlation structure
* e.g. `phyloglmm`

## Penalty matrices as 

- @woodStable2004

## Computational tricks

* work with *precision matrix* where possible $\Sigma^{-1}$
* for a **multivariate normal** response, $\Sigma^{-1}_{ij} = \Sigma^{-1}_{ji} = 0 \leftrightarrow$ $x_i$ and $x_j$ are **conditionally independent**
* e.g. precision matrix of AR1 is tridiagonal with diagonal $1+\rho^2$, first off-diagonal elements $-\rho$ (see [here](https://haakonbakkagit.github.io/btopic120.html))
* work with *reduced-rank* forms where necessary

