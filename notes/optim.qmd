---
title: "Basics of optimization/likelihood minimization"
bibliography: "../stats720.bib"
date: today
date-format: "D MMM YYYY"
format:
  pdf:
    mainfont: TeX Gyre Pagella
    include-in-header:
     - text: \usepackage{marginnote}
---

::: {.content-hidden}
$$
{{< include mathsymbols.tex >}}
$$
:::

<!-- typically renders in docs/ dir  -->

```{r pkgs, message = FALSE}
library(RTMB)
library(bbmle)
```

## References

@bolkerEcological2008, @bolkerStrategiesFittingNonlinear2013

## Minimization in statistics

* fitting models to data
* 'best fit'
* *objective function* or *loss function*; differs by application/problem type
* least squares, minimum absolute deviation, cross-entropy ...
* many of these are special cases of **negative log-likelihood**

## Maximum likelihood

* nice properties (efficient, consistent, asymptotically unbiased)
* unifying principle
* usually minimize negative log-likelihood instead
*  "when it can do the job, it's rarely the best tool for the job but it's rarely much worse than the best" (S. Ellner)

## Minimization

* closed form solution of *score equations* (direct or via linear algebra)
* iteratively reweighted least squares
* gradient descent
* more complex iterative solutions
   * Nelder-Mead
   * quasi-Newton methods (BFGS, L-BFGS)
* automatic differentiation

## Minimization in R

* `optim()` (`nlminb`)
* Nelder-Mead
* various quasi-Newton methods [@pressNumerical2007]

```{r}
X <- model.matrix(~wool*tension, data=warpbreaks)
y <- warpbreaks$breaks
nll <- function(beta) {
    mu <- exp(X %*% beta)
    -sum(dpois(y, mu, log=TRUE))
}
par0 <- rep(0, ncol(X))
opt1 <- optim(par0, nll, control = list(maxit = 1000))
par2 <- coef(glm(breaks~wool*tension, data=warpbreaks, family=poisson))
all.equal(opt1$par, unname(par2), tolerance = 1e-3)
```

## with `mle2`

* wrapper for `optim`, variant of `stats4::mle()`
* `data` argument
* better accessor methods (`coef()`, `vcov()`, `broom::tidy()`, `profile()`, `confint()`, `predict()`, ...)

```{r}
names(par0) <- parnames(nll) <- paste0("beta", 1:ncol(X))
confint(mle2(nll, par0))
```

## formula notation

```{r}
mle2(breaks~dpois(exp(eta)),
     parameters = list(eta ~ wool*tension),
     start = list(eta = 0),
     data = warpbreaks)
```

## `mle2` notes

* uses `BFGS` (with finite difference gradients!) by default; may want Nelder-Mead
* probably want to provide a link function for dispersion parameters, e.g. fit negative binomial with `size = exp(logsize)`

## Template Model Builder and autodiff

* everything is better with gradients
* **finite difference** gradients (R default) are terrible (expensive and inaccurate)
* FD vs symbolic vs automatic/algorithm
* magic/chain rule; *cheap gradient* principle [@kristensenTMB2016]

## deriv() in base R

Useful, illustrative, but limited

```{r}
deriv(expression(cos(x*sin(x^2))), "x")
```

## RTMB package

```{r}
data("prussian", package = "pscl")
prussmin <- prussian[1:3,]
X2 <- model.matrix(~ factor(year), data = prussmin)
par2 <- list(beta = rep(0, ncol(X2)))
f2 <- function(pars) {
    getAll(pars)  ## this is like with() or attach()
    mu <- exp(X2 %*% beta)
    -sum(dpois(prussmin$y, lambda = mu, log = TRUE))
}
```

```{r echo=FALSE}
tape2 <- RTMB::MakeTape(f2, par2)
plot(igraph::graph_from_adjacency_matrix(tape2$graph()),
     layout=igraph::layout_as_tree, vertex.size = 5)
```

```{r echo=FALSE}
X2 <- model.matrix(~ factor(year), data = prussian)
par2 <- list(beta = rep(0, ncol(X2)))
f2 <- function(pars) {
    getAll(pars)  ## this is like with() or attach()
    mu <- exp(X2 %*% beta)
    -sum(dpois(prussian$y, lambda = mu, log = TRUE))
}
```

```{r}
ff2 <- RTMB::MakeADFun(f2, par2)
ff2$fn()
ff2$gr()
```

Can use `f2$fn()`, `f2$gr()` to get function and gradient vector efficiently.

## References

::: {#refs}
:::
