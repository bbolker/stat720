---
title: "Ridge regression and mixed models"
bibliography: "../stats720.bib"
date: today
date-format: "D MMM YYYY"
format:
  pdf:
    mainfont: TeX Gyre Pagella
    include-in-header:
     - text: \usepackage{marginnote}
---

::: {.content-hidden}
$$
{{< include mathsymbols.tex >}}
$$
:::

<!-- typically renders in docs/ dir  -->

```{r pkgs, message = FALSE, echo = FALSE}
## it's nice to include packages at the top
## (and NOT automatically install them)
## try not to carry over packages you don't use
library(ggplot2); theme_set(theme_bw())
## diagnostics
library(performance)
library(DHARMa)
## downstream model evaluation
library(broom)
library(dotwhisker)
library(emmeans)
library(effects)
library(marginaleffects)
library(parameters)
## library(ggeffects)
```

## Ridge in a nutshell

- **penalized** models: instead of minimizing SSQ = $\sum ((\y-\X\bbeta)_i)^2$, minimize SSQ + $\lambda ||\bbeta||_2$ (ridge)
- or + $||\bbeta||_1$ (lasso)
- optimize *bias-variance tradeoff*
- equivalent to imposing iid Gaussian priors on each element of $\bbeta$
- lasso (and elastic net, which is a convex combination of L2 and L1 penalties) are popular because they **induce sparsity**
   - *likelihood surfaces* are non-convex with cusps at zero
   - optimization with non-convex surfaces is a nuisance because it makes the basic optimization problem nonlinear; we need to use a different algorithm (coordinate descent/soft thresholding, see @friedmanRegularization2010b); can't use *only* linear algebra
- can generalize from penalized LM to penalized GLM

## Variable selection 

Variable selection has some characteristics in common with multicollinearity and conditional Normality, i.e. that people generally overestimate its importance.

[Andrew Gelman on variable selection](https://statmodeling.stat.columbia.edu/2023/07/18/when-your-regression-model-has-interactions-do-you-need-to-include-all-the-corresponding-main-effects/#comment-2238997):

> Variable selection (that is, setting some coefficients to be exactly zero) can be useful for various reasons, including:

> * It’s a simple form of regularization.
> * It can reduce costs in future data collection.
> Variable selection can be fine as a means to an end. Problems can arise if it’s taken too seriously, for example as an attempt to discover a purported parsimonious true model.

Variable selection can also be viewed as **inducing sparsity** in a model; thus it can also be computationally useful ...

## Choosing penalty strength

* typically by *cross-validation*
* leave-one-out (LOOCV) vs $k$-fold: bias/variance tradeoff (@jamesintroduction2013 \S 5.1.4)

## Practical points

* Predictors **must** be standardized
* Intercept should usually be unpenalized
* Avoid **data leakage**
   * don't include variables that are 'future' indicators of the outcome (e.g. see [here](https://towardsdatascience.com/data-leakage-in-machine-learning-how-it-can-be-detected-and-minimize-the-risk-8ef4e3a97562))
   * full pipeline must be cross-validated (i.e. don't do data-dependent variable selection *before* cross-validating, or use the full data set to select a pipeline)
   * cross-validation must account for structure in the data
    * **either** ensure that residuals are *conditionally* independent
	* **or** take account of grouping structures in the data (block bootstrap, spatial stratification, etc. [@robertsCrossvalidation2017, @wengerAssessing2012]

## Ridge vs lasso

- In practice people just try both (or elastic net)
- Conjecture: whether ridge or lasso is a better *predictive* model in a particular case depends on the *effect size spectrum*

## Ridge by data augmentation

* set
$$
\B = \left(
    \begin{array}{c}
     \X \\
    \sqrt{\lambda} \I
    \end{array}
\right)
$$

* and $\y^* = (\y \: \bzero)$
* so that $\B^\top \B = \X^\top \X + \lambda I$ and the residual sum of squares is unchanged

## Inference

* inference from penalized models is really hard
* classical CIs for ridge are **identical** to OLS [@obenchainClassical1977]
   > ridge techniques do not generally yield ``new'' normal theory statistical inferences: in particular, ridging does not necessarily produce ``shifted'' confidence regions.
* **no free lunch** (i.e., no true narrowing of CIs/decreased uncertainty without additional assumptions)
* post-selection inference is a big deal but requires very strong assumptions (asymptotic, 'gap')
* prediction intervals are often neglected (conformal prediction, jackknife+ [@barberPredictive2021]): [MAPIE](https://mapie.readthedocs.io/en/latest/)

## Practical

* `glmnet` is very good
* `ridge`, `lmridge`, ... (`library(sos); findFn("{ridge regression}")1)
* need to give `y` and `X` directly (although see [glmnetUtils package](https://CRAN.R-project.org/package=glmnetUtils))

## More on penalization

* we've already seen it 
* also described as *regularization*, *shrinkage* estimator, or as equivalent to imposing a Bayesian prior
* complete separation
* could use it for forcing negative binomial parameter away from $\theta = \infty$ ?
* latter will use to mitigate *singular* mixed-model fits (variance = 0)

## Tangent: how do I know if an R package is any good?

* how old is it/how many releases has it had?
* is it actively developed?
* does the documentation give literature citations?
* does it have reverse dependencies?
* what is its ranking on CRAN? `packageRank::packageRank("lmridge")` (80th percentile)

## James-Stein estimators

* more formally, why is ridge better?
* based on a single observation, $\y$, of a *multivariate* response with dimension $m \ge 3$, shrinking the value (usually toward zero) is a better estimate of the mean than the value itself (!)

$$
\hat \mu(X_1, \ldots, X_n) = \left(1-\frac{(p-2) \sigma^2/n}{||\bar X_n||^2}\right) \bar X_n
$$

* (connected to recurrence of random walks in $d \le 2$, non-recurrence in $d\ge 3$ ...)
* "paradox": the quantities in the vector don't have to have anything to do with each other (and, we can shrink to any point, not necessarily zero ...)

From @antogniniUnderstanding2021:

![](../pix/stein1.png)

From @harrisVisualizing2013:

![](../pix/stein2.png)

- @vanhouwelingenShrinkage2001 gives a very nice explanation/transition from James-Stein to penalized regression etc.


## From ridge to mixed models

i.e. treat this as an *empirical Bayesian* problem (we estimate the
$\bbeta$ values, but do not put a prior on $\sigma^2$ or a hyperprior on $\sigma_g^2$ (= $1/\lambda$)

From @vanhouwelingenShrinkage2001 (ultimately from Efron and Morris 1972):

If we use a prior with $\mu_i \sim N(\mu, \tau^2)$ (assuming residual variance is 1 wlog), then

$$
\begin{split}
E(\mu_i|X_i) & = \mu + \frac{\tau^2}{\tau^2+1}(X_i - \mu) \\
var(\mu_i|X_i) & = \frac{\tau^2}{\tau^2+1}
\end{split}
$$

But we still have to estimate $\tau$ (or $\tau^2/(\tau^2+1)$ from the data).

## MVN version

We can be much more general:

$$
\begin{split}
\y & \sim \textrm{Normal}(\X \beta, \sigma^2) \\
\bbeta & \sim \textrm{MVN}(\bzero, \sigma_g^2 \I)
\end{split}
$$

## Back to 1D

The simplest case (described in an R formula as `y ~ 1 + (1|g)`) is a model with a population-level intercept $\beta_0$ and group-level deviations from the population mean $b_i$.

This case, and more complex cases, can be written as

$$
\begin{split}
y_i & \sim \textrm{Normal}((\X \bbeta + \Z \bb)_i, \sigma^2_r) \\
\bb & \sim \textrm{MVN}(\bzero, \Sigma(\btheta))
\end{split}
$$
where $\btheta$ is a vector of parameters that defines the covariance matrix $\Sigma$.

## How do we estimate this?

* can use EM algorithm (e.g. see [here](https://stt.msu.edu/users/pszhong/Lecture_23_Spring_2017.pdf), or the [lmm package](https://github.com/jinghuazhao/R/tree/master/lmm))
* Or by linear algebra. For LMMs, we do a more complicated version of *data augmentation*.
* given a value for the random-effects variance, we can calculate the log-likelihood in one step (see
* large, sparse matrix computation
* has to be done *repeatedly*
* most efficient if we analyze the matrix and permute to optimize structure [@batesFitting2015a]
* then we need to do some kind of search over the space of variances
* derivatives are available in particular special cases

## The general case

Given a model of the form

$$
\begin{split}
y_i & \sim \textrm{Normal}((\X \bbeta + \Z \bb)_i, \sigma^2_r) \\
\bb & \sim \textrm{MVN}(\bzero, \Sigma(\btheta))
\end{split}
$$

* How do we specify and set up $Z$?
* How do we specify and set up $\Sigma$?

## constructing the random-effects model matrix

* specify as `(t|g)`; `t` is the *term* and `g` is the *grouping factor*
* for intercepts, just the indicator matrix
* for more complex models (random slopes), take the *Khatri-Rao* product of the model matrix of the term with the indicator matrix of `g`
* concatenate multiple random effects terms into a single `Z` matrix

## constructing the covariance matrix

* blockwise
* what's the best way to parameterize a positive-(semi)definite matrix? [@pinheiroUnconstrained1996a]
* Cholesky decomposition with 
* scaled or unscaled?
* Cholesky or log-Cholesky scale?
* separating correlation and SD vectors: [glmmTMB](https://glmmtmb.github.io/glmmTMB/articles/covstruct.html#unstructured):

$$
\Sigma = D^{-1/2} L L^\top D^{-1/2}, \quad D = \textrm{diag}(L L^\top)
$$

## References

::: {#refs}
:::

