---
title: "Ridge regression and mixed models"
bibliography: "../stats720.bib"
date: today
date-format: "D MMM YYYY"
format:
  pdf:
    mainfont: TeX Gyre Pagella
    include-in-header:
     - text: \usepackage{marginnote}
---

::: {.content-hidden}
$$
{{< include mathsymbols.tex >}}
$$
:::

<!-- typically renders in docs/ dir  -->

```{r pkgs, message = FALSE, echo = FALSE}
## it's nice to include packages at the top
## (and NOT automatically install them)
## try not to carry over packages you don't use
library(ggplot2); theme_set(theme_bw())
## diagnostics
library(performance)
library(DHARMa)
## downstream model evaluation
library(broom)
library(dotwhisker)
library(emmeans)
library(effects)
library(marginaleffects)
library(parameters)
## library(ggeffects)
```

## Ridge in a nutshell

- **penalized** models: instead of minimizing SSQ = $\sum ((\y-\X\bbeta)_i)^2$, minimize SSQ + $\lambda ||\bbeta||_2$ (ridge)
- or + $||\bbeta||_1$ (lasso)
- optimize *bias-variance tradeoff*
- equivalent to imposing iid Gaussian priors on each element of $\bbeta$
- lasso (and elastic net, which is a convex combination of L2 and L1 penalties) are popular because they **induce sparsity**
   - *likelihood surfaces* are non-convex with cusps at zero
   - optimization with non-convex surfaces is a nuisance because it makes the basic optimization problem nonlinear; we need to use a different algorithm (coordinate descent/soft thresholding); can't use *only* linear algebra
- can generalize from penalized LM to penalized GLM

##  [Andrew Gelman on variable selection](https://statmodeling.stat.columbia.edu/2023/07/18/when-your-regression-model-has-interactions-do-you-need-to-include-all-the-corresponding-main-effects/#comment-2238997)

> Variable selection (that is, setting some coefficients to be exactly zero) can be useful for various reasons, including:
> * It’s a simple form of regularization.
> * It can reduce costs in future data collection.
> Variable selection can be fine as a means to an end. Problems can arise if it’s taken too seriously, for example as an attempt to discover a purported parsimonious true model.

## Choosing penalty strength

* typically by *cross-validation*
* leave-one-out (LOOCV) vs $k$-fold

## Practical points

* Predictors **must** be standardized
* Intercept should usually be unpenalized
* Avoid **data leakage**
   * don't include variables that are 'future' indicators of the outcome (e.g. see [here](https://towardsdatascience.com/data-leakage-in-machine-learning-how-it-can-be-detected-and-minimize-the-risk-8ef4e3a97562))
   * full pipeline must be cross-validated (i.e. don't do data-dependent variable selection *before* cross-validating, or use the full data set to select a pipeline)
   * cross-validation must account for structure in the data
    * **either** ensure that residuals are *conditionally* independent
	* **or** take account of grouping structures in the data (block bootstrap, spatial stratification, etc. [@robertsCrossvalidation2017, @wengerAssessing2012]

## Ridge vs lasso

- In practice people just try both (or elastic net)
- Conjecture: whether ridge or lasso is a better *predictive* model in a particular case depends on the *effect size spectrum*

## Ridge by data augmentation

* set
$$
\B = \left(
    \begin{array}{c}
     \X \\
    \sqrt{\lambda} \I
    \end{array}
\right)
$$

* and $\y^* = (\y \: \bzero)$
* so that $\B^\top \B = \X^\top \X + \lambda I$ and the residual sum of squares is unchanged

## Inference

* inference from penalized models is really hard
* classical CIs for ridge are **identical** to OLS [@obenchainClassical1977]
   > ridge techniques do not generally yield ``new'' normal theory statistical inferences: in particular, ridging does not necessarily produce ``shifted'' confidence regions.
* **no free lunch** (i.e., no true narrowing of CIs/decreased uncertainty without additional assumptions)
* post-selection inference is a big deal but requires very strong assumptions (asymptotic, 'gap')
* prediction intervals are often neglected (conformal prediction, jackknife+ [@barberPredictive2021]): [MAPIE](https://mapie.readthedocs.io/en/latest/)

## Practical

* `glmnet` is very good
* `ridge`, `lmridge`, ... (`library(sos); findFn("{ridge regression}")1)
* need to give `y` and `X` directly (although see [glmnetUtils package](https://CRAN.R-project.org/package=glmnetUtils))

## Tangent: how do I know if an R package is any good?

* how old is it/how many releases has it had?
* is it actively developed?
* does the documentation give literature citations?
* does it have reverse dependencies?
* what is its ranking on CRAN? `packageRank::packageRank("lmridge")` (80th percentile)

## James-Stein estimators

* more formally, why is ridge better?
* based on a single observation, $\y$, of a *multivariate* response with dimension $m \ge 3$, shrinking the value (usually toward zero) is a better estimate of the mean than the value itself

## From ridge to mixed models

* what if we say

$$
\begin{split}
\y & \sim \textrm{Normal}(\X \beta, \sigma^2) \\
\bbeta & \sim \textrm{MVN}(\bzero, \sigma_g^2 \I)
\end{split}
$$

?

i.e. treat this as an *empirical Bayesian* problem (we estimate the
$\bbeta$ values, but do not put a prior on $\sigma^2$ or a hyperprior on $\sigma_g^2$ (= $1/\lambda$)

## 
## References

::: {#refs}
:::
