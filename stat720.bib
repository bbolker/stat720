@misc{shamsudheenShould2021,
  title = {Should We Test the Model Assumptions before Running a Model-Based Test?},
  author = {Shamsudheen, M. Iqbal and Hennig, Christian},
  year = {2021},
  month = mar,
  number = {arXiv:1908.02218},
  eprint = {1908.02218},
  primaryclass = {stat},
  institution = {{arXiv}},
  urldate = {2022-05-21},
  abstract = {Statistical methods are based on model assumptions, and it is statistical folklore that a method's model assumptions should be checked before applying it. This can be formally done by running one or more misspecification tests testing model assumptions before running a method that makes these assumptions; here we focus on model-based tests. A combined test procedure can be defined by specifying a protocol in which first model assumptions are tested and then, conditionally on the outcome, a test is run that requires or does not require the tested assumptions. Although such an approach is often taken in practice, much of the literature that investigated this is surprisingly critical of it. Our aim is to explore conditions under which model checking is advisable or not advisable. For this, we review results regarding such "combined procedures" in the literature, we review and discuss controversial views on the role of model checking in statistics, and we present a general setup in which we can show that preliminary model checking is advantageous, which implies conditions for making model checking worthwhile.},
  archiveprefix = {arxiv},
  keywords = {62F03,Statistics - Methodology},
  file = {/home/bolker/Zotero/storage/2WKT8FX9/Shamsudheen and Hennig - 2021 - Should we test the model assumptions before runnin.pdf;/home/bolker/Zotero/storage/V4P9CJQ7/1908.html}
}


@misc{rossFasteR2013,
  title = {{{FasteR}}! {{HigheR}}! {{StrongeR}}! - {{A Guide}} to {{Speeding Up R Code}} for {{Busy People}}},
  author = {Ross, Noam},
  year = {2013},
  month = apr,
  journal = {Noam Ross},
  urldate = {2016-09-02},
  howpublished = {http://www.noamross.net/blog/2013/4/25/faster-talk.html},
  file = {/home/bolker/Zotero/storage/64SSBRVJ/faster-talk.html}
}

@techreport{bryanExcuse2017,
  title = {Excuse Me, Do You Have a Moment to Talk about Version Control?},
  author = {Bryan, Jennifer},
  year = {2017},
  month = aug,
  number = {e3159v2},
  institution = {{PeerJ Inc.}},
  issn = {2167-9843},
  doi = {10.7287/peerj.preprints.3159v2},
  urldate = {2022-10-18},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  langid = {english},
  file = {/home/bolker/Zotero/storage/9YR9ZKL3/Bryan - 2017 - Excuse me, do you have a moment to talk about vers.pdf;/home/bolker/Zotero/storage/M9SGPHLK/3159v2.html}
}

@misc{bryanProjectoriented2017,
  title = {Project-Oriented Workflow},
  author = {Bryan, Jenny},
  year = {2017},
  month = dec,
  journal = {Tidyverse},
  urldate = {2021-01-14},
  abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
  howpublished = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
  langid = {american},
  file = {/home/bolker/Zotero/storage/2UCXRWUV/workflow-vs-script.html}
}

@article{wilsonGood2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2017},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {6},
  pages = {e1005510},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  urldate = {2019-03-16},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  langid = {english},
  keywords = {Computer software,Control systems,Data management,Data processing,Programming languages,Reproducibility,Software tools,Source code},
  file = {/home/bolker/Zotero/storage/7FVVG9HG/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/home/bolker/Zotero/storage/MRE6NA97/article.html}
}


@book{harrellRegression2015,
  title = {Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis},
  shorttitle = {Regression {{Modeling Strategies}}},
  author = {Harrell Jr., Frank E.},
  year = {2015},
  month = aug,
  edition = {2d},
  publisher = {Springer},
  isbn = {978-3-319-19424-0},
  note = {This book presents the best blend I've ever found of practical yet rigorous advice for statistical modeling (testing model assumptions, limiting model complexity in a way that preserves reliable inference, dealing with missing data, etc.). It covers linear and generalized linear regression, logistic regression, ordinal responses, and survival analysis thoroughly; it has a chapter on longitudinal data. It does not go deeply into mixed models, nor does it cover more exotic responses (count or non-binary binomial responses, zero-inflation). It uses the author's own \texttt{rms} package, which is useful but doesn't always fit in perfectly with other frameworks.}
  }

@book{farawayExtending2016,
  title = {Extending the {{Linear Model}} with {{R}}: {{Generalized Linear}}, {{Mixed Effects}} and {{Nonparametric Regression Models}}, {{Second Edition}}},
  shorttitle = {Extending the {{Linear Model}} with {{R}}},
  author = {Faraway, Julian J.},
  year = {2016},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Start Analyzing a Wide Range of Problems  Since the publication of the bestselling, highly recommended first edition, R has considerably expanded both in popularity and in the number of packages available. Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models, Second Edition takes advantage of the greater functionality now available in R and substantially revises and adds several topics. New to the Second Edition  Expanded coverage of binary and binomial responses, including proportion responses, quasibinomial and beta regression, and applied considerations regarding these models  New sections on Poisson models with dispersion, zero inflated count models, linear discriminant analysis, and sandwich and robust estimation for generalized linear models (GLMs)  Revised chapters on random effects and repeated measures that reflect changes in the lme4 package and show how to perform hypothesis testing for the models using other methods New chapter on the Bayesian analysis of mixed effect models that illustrates the use of STAN and presents the approximation method of INLA  Revised chapter on generalized linear mixed models to reflect the much richer choice of fitting software now available Updated coverage of splines and confidence bands in the chapter on nonparametric regression New material on random forests for regression and classification  Revamped R code throughout, particularly the many plots using the ggplot2 package Revised and expanded exercises with solutions now included Demonstrates the Interplay of Theory and Practice This textbook continues to cover a range of techniques that grow from the linear regression model. It presents three extensions to the linear framework: GLMs, mixed effect models, and nonparametric regression models. The book explains data analysis using real examples and includes all the R commands necessary to reproduce the analyses. The level is good for upper-level undergraduate statisticians and beyond, possibly tough for biologists.},
  googlebooks = {XAzYCwAAQBAJ},
  isbn = {978-1-4987-2098-4},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
  note = {A good, broad book that covers most of the material we'll discuss in this class. It starts with a little bit more basic coverage of (G)LMs, but has good discussions of intermediate-level GLM issues (conditional logistic regression, alternative link functions, etc.). It goes on to cover mixed models and additive models, as well as a brief introduction to tree-based models and neural networks (which we'll skip in this class).}
}

@book{gelmanData2006,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, England}},
  note = {Clear, comprehensive discussion of multilevel/hierarchical models. 99\% Bayesian treatment; examples are mostly from social science. Covers linear, logistic, count-data (Poisson/negative binomial) regressions. Causal inference, regression diagnostics, etc.. Reliance on the BUGS language is now slightly old-fashioned (Gelman et al 2020 is an update for the non-mixed model material; these authors and others are working on an updated mixed model book, nominally available \href{http://www.stat.columbia.edu/~gelman/armm/}{around the end of 2024}).}
}

@book{gelmanRegression2020,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year = {2020},
  month = jul,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore}},
  abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
  isbn = {978-1-107-67651-0},
  langid = {english},
  note = {I haven't read this book yet but based on the authors' past work I expect it to be excellent.}
}


@book{hodgesRichly2013,
  title = {Richly Parameterized Linear Models: Additive, Time Series, and Spatial Models Using Random Effects},
  shorttitle = {Richly Parameterized Linear Models},
  author = {Hodges, James S.},
  year = {2013},
  publisher = {{CRC Press}},
  note = {A technical, in-depth exploration of the ways that hierarchical Gaussian linear models can be extended to a huge variety of different kinds of correlation structures and smooth functions. Worked examples of tricky real-world examples; discusses details you won't find anywhere else.}
}

@book{mcelreathStatistical2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {6H\_WDwAAQBAJ},
  isbn = {978-0-429-63914-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
  note = {Bayesian statistics from first principles. Aimed at non-statisticians. Relies on Stan for analysis (the \texttt{rethinking} package provides a nice front end for graphical models). Heavy on mechanistic models and causal inference.}
}

@book{woodGeneralized2017,
  title = {Generalized {{Additive Models}}: {{An Introduction}} with {{R}}},
  author = {Wood, Simon N.},
  year = {2017},
  series = {{{CRC Texts}} in {{Statistical Science}}},
  publisher = {{Chapman \& Hall}},
  urldate = {2017-11-28},
  file = {/home/bolker/Zotero/storage/Q79P94BB/ref=sr_1_1.html},
  note = {Comprehensive coverage of additive models from a penalized-basis point of view, from the master (the author of R's \texttt{mgcv} package). Includes brief reviews of the theory behind linear and GLMs. Technical bits are tough for non-statisticians.}
}


@misc{navarroScience2019,
  title = {Science and Statistics},
  author = {Navarro, Danielle},
  year = {2019},
  month = mar,
  address = {{Aarhus University}},
  abstract = {http://interactingminds.au.dk/events/single-events/artikel/2-day-workshop-open-science-and-reproducibility/},
  langid = {english},
  file = {/home/bolker/Zotero/storage/UX69QA8G/scienceandstatistics.html},
  url = {https://slides.com/djnavarro/scienceandstatistics}
}


@article{boxScience1976b,
	title = {Science and {Statistics}},
	volume = {71},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949},
	doi = {10.1080/01621459.1976.10480949},
	abstract = {Aspects of scientific method are discussed: In particular, its representation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires sufficient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
	number = {356},
	urldate = {2023-09-02},
	journal = {Journal of the American Statistical Association},
	author = {Box, George E. P.},
	month = dec,
	year = {1976},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1976.10480949},
	pages = {791--799}
}


@book{mccullaghGeneralized1989,
	address = {London},
	title = {Generalized {Linear} {Models}},
	publisher = {Chapman and Hall},
	author = {McCullagh, P. and Nelder, J. A.},
	year = {1989},
}


@article{gelman_statistical_2014,
	title = {The statistical crisis in science: data-dependent analysis--a "garden of forking paths"--explains why many statistically significant comparisons don't hold up},
	volume = {102},
	issn = {0003-0996},
	shorttitle = {The statistical crisis in science},
	url = {http://link.galegroup.com/apps/doc/A389260653/AONE?u=ocul_mcmaster&sid=AONE&xid=4f4562c0},
	language = {English},
	number = {6},
	urldate = {2019-01-07},
	journal = {American Scientist},
	author = {Gelman, Andrew and Loken, Eric},
	year = {2014},
	note = {460},
	keywords = {Periodical publishing, Science journals},
	pages = {460--}
}

@article{simmons_false-positive_2011,
	title = {False-Positive Psychology: Undisclosed Flexibility  in Data Collection and Analysis Allows Presenting Anything as Significant},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/11/1359},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2015-11-08},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	keywords = {disclosure, methodology, motivated reasoning, publication},
	pages = {1359--1366},
	file = {Full Text PDF:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/TNSUZHFS/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility .pdf:application/pdf;Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/BATW66XJ/1359.html:text/html}
}


@article{schielzethSimple2010,
	title = {Simple means to improve the interpretability of regression coefficients: {Interpretation} of regression coefficients},
	volume = {1},
	issn = {2041210X, 2041210X},
	shorttitle = {Simple means to improve the interpretability of regression coefficients},
	url = {http://doi.wiley.com/10.1111/j.2041-210X.2010.00012.x},
	doi = {10.1111/j.2041-210X.2010.00012.x},
	language = {en},
	number = {2},
	urldate = {2016-06-08},
	journal = {Methods in Ecology and Evolution},
	author = {Schielzeth, Holger},
	month = feb,
	year = {2010},
	pages = {103--113},
	file = {j.2041-210X.2010.00012.x.pdf:/home/bolker/Documents/zotero_new/storage/XNG3ZF5X/j.2041-210X.2010.00012.x.pdf:application/pdf},
}

@article{rochon_test_2012,
	title = {To test or not to test: {Preliminary} assessment of normality when comparing two independent samples},
	volume = {12},
	issn = {1471-2288},
	shorttitle = {To test or not to test},
	url = {https://doi.org/10.1186/1471-2288-12-81},
	doi = {10.1186/1471-2288-12-81},
	abstract = {Student’s two-sample t test is generally used for comparing the means of two independent samples, for example, two treatment arms. Under the null hypothesis, the t test assumes that the two samples arise from the same normally distributed population with unknown variance. Adequate control of the Type I error requires that the normality assumption holds, which is often examined by means of a preliminary Shapiro-Wilk test. The following two-stage procedure is widely accepted: If the preliminary test for normality is not significant, the t test is used; if the preliminary test rejects the null hypothesis of normality, a nonparametric test is applied in the main analysis.},
	number = {1},
	urldate = {2020-07-10},
	journal = {BMC Medical Research Methodology},
	author = {Rochon, Justine and Gondan, Matthias and Kieser, Meinhard},
	month = jun,
	year = {2012},
	pages = {81},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/ZY252N86/Rochon et al. - 2012 - To test or not to test Preliminary assessment of .pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/NH5ULAHD/1471-2288-12-81.html:text/html}
}


@article{campbellconsequences2021a,
	title = {The consequences of checking for zero-inflation and overdispersion in the analysis of count data},
	volume = {12},
	copyright = {© 2021 British Ecological Society},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13559},
	doi = {10.1111/2041-210X.13559},
	abstract = {Count data are ubiquitous in ecology and the Poisson generalized linear model (GLM) is commonly used to model the association between counts and explanatory variables of interest. When fitting this model to the data, one typically proceeds by first confirming that the model assumptions are satisfied. If the residuals appear to be overdispersed or if there is zero-inflation, key assumptions of the Poison GLM may be violated and researchers will then typically consider alternatives to the Poison GLM. An important question is whether the potential model selection bias introduced by this data-driven multi-stage procedure merits concern. Here we conduct a large-scale simulation study to investigate the potential consequences of model selection bias that can arise in the simple scenario of analysing a sample of potentially overdispersed, potentially zero-inflated, count data. Specifically, we investigate model selection procedures recently recommended by Blasco-Moreno et al. (2019) using either a series of score tests or information theoretic criteria to select the best model. We find that, when sample sizes are small, model selection based on preliminary score tests (or information theoretic criteria, e.g. AIC, BIC) can lead to potentially substantial inflation of false positive rates (i.e. type 1 error inflation). When sample sizes are sufficiently large, model selection based on preliminary score tests, is not problematic. Ignoring the possibility of overdispersion and zero-inflation during data analyses can lead to invalid inference. However, if one does not have sufficient power to test for overdispersion and zero-inflation, post hoc model selection may also lead to substantial bias. This ‘catch-22’ suggests that, if sample sizes are small, a healthy skepticism is warranted whenever one rejects the null hypothesis of no association between a given outcome and covariate.},
	language = {en},
	number = {4},
	urldate = {2023-09-02},
	journal = {Methods in Ecology and Evolution},
	author = {Campbell, Harlan},
	year = {2021},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13559},
	keywords = {model selection bias, overdispersion, zero-inflated models, zero-inflation},
	pages = {665--680},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/B52K6P2P/Campbell - 2021 - The consequences of checking for zero-inflation an.pdf:application/pdf},
}

@article{campbell_consequences_2014,
	title = {The consequences of proportional hazards based model selection},
	volume = {33},
	copyright = {Copyright © 2013 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6021},
	doi = {10.1002/sim.6021},
	abstract = {For testing the efficacy of a treatment in a clinical trial with survival data, the Cox proportional hazards (PH) model is the well-accepted, conventional tool. When using this model, one typically proceeds by confirming that the required PH assumption holds true. If the PH assumption fails to hold, there are many options available, proposed as alternatives to the Cox PH model. An important question which arises is whether the potential bias introduced by this sequential model fitting procedure merits concern and, if so, what are effective mechanisms for correction. We investigate by means of simulation study and draw attention to the considerable drawbacks, with regard to power, of a simple resampling technique, the permutation adjustment, a natural recourse for addressing such challenges. We also consider a recently proposed two-stage testing strategy (2008) for ameliorating these effects. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2020-07-10},
	journal = {Statistics in Medicine},
	author = {Campbell, H. and Dean, C. B.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6021},
	keywords = {model selection bias, proportional hazards, two-stage approach},
	pages = {1042--1056}
}


@article{box_non-normality_1953,
	title = {Non-{Normality} and {Tests} on {Variances}},
	volume = {40},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2333350},
	doi = {10.2307/2333350},
	number = {3/4},
	urldate = {2020-07-11},
	journal = {Biometrika},
	author = {Box, G. E. P.},
	year = {1953},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {318--335}
}


@article{zimmermannote2004,
	title = {A note on preliminary tests of equality of variances},
	volume = {57},
	copyright = {2004 The British Psychological Society},
	issn = {2044-8317},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/000711004849222},
	doi = {10.1348/000711004849222},
	abstract = {Preliminary tests of equality of variances used before a test of location are no longer widely recommended by statisticians, although they persist in some textbooks and software packages. The present study extends the findings of previous studies and provides further reasons for discontinuing the use of preliminary tests. The study found Type I error rates of a two-stage procedure, consisting of a preliminary Levene test on samples of different sizes with unequal variances, followed by either a Student pooled-variances t test or a Welch separate-variances t test. Simulations disclosed that the twostage procedure fails to protect the significance level and usually makes the situation worse. Earlier studies have shown that preliminary tests often adversely affect the size of the test, and also that the Welch test is superior to the t test when variances are unequal. The present simulations reveal that changes in Type I error rates are greater when sample sizes are smaller, when the difference in variances is slight rather than extreme, and when the significance level is more stringent. Furthermore, the validity of the Welch test deteriorates if it is used only on those occasions where a preliminary test indicates it is needed. Optimum protection is assured by using a separate-variances test unconditionally whenever sample sizes are unequal.},
	language = {en},
	number = {1},
	urldate = {2020-10-01},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Zimmerman, Donald W.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000711004849222},
	pages = {173--181},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/W8RPHCGT/Zimmerman - 2004 - A note on preliminary tests of equality of varianc.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/2S6W6AZG/000711004849222.html:text/html},
}


@article{mengStatistical2018,
	title = {Statistical paradises and paradoxes in big data ({I}): {Law} of large populations, big data paradox, and the 2016 {US} presidential election},
	volume = {12},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Statistical paradises and paradoxes in big data ({I})},
	url = {https://projecteuclid.org/euclid.aoas/1532743473},
	doi = {10.1214/18-AOAS1161SF},
	abstract = {Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualifications for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: “Which one should I trust more: a 1\% survey with 60\% response rate or a self-reported administrative dataset covering 80\% of the population?” A 5-element Euler-formula-like identity shows that for any dataset of size nnn, probabilistic or not, the difference between the sample average X¯¯¯¯nX¯n{\textbackslash}overline\{X\}\_\{n\} and the population average X¯¯¯¯NX¯N{\textbackslash}overline\{X\}\_\{N\} is the product of three terms: (1) a data quality measure, ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\}, the correlation between XjXjX\_\{j\} and the response/recording indicator RjRjR\_\{j\}; (2) a data quantity measure, (N−n)/n−−−−−−−−−√(N−n)/n{\textbackslash}sqrt\{(N-n)/n\}, where NNN is the population size; and (3) a problem difficulty measure, σXσX{\textbackslash}sigma\_\{X\}, the standard deviation of XXX. This decomposition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\} at the level of N−1/2N−1/2N{\textasciicircum}\{-1/2\}; (II) When we lose this control, the impact of NNN is no longer canceled by ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\}, leading to a Law of Large Populations (LLP), that is, our estimation error, relative to the benchmarking rate 1/n−−√1/n1/{\textbackslash}sqrt\{n\}, increases with N−−√N{\textbackslash}sqrt\{N\}; and (III) the “bigness” of such Big Data (for population inferences) should be measured by the relative size f=n/Nf=n/Nf=n/N, not the absolute size nnn; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by their sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a ρR,X≈−0.005ρR,X≈−0.005{\textbackslash}rho\_\{\{R,X\}\}{\textbackslash}approx-0.005 for self-reporting to vote for Donald Trump. Because of LLP, this seemingly minuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from 1\%1\%1{\textbackslash}\% of the US eligible voters, that is, n≈2,300,000n≈2,300,000n{\textbackslash}approx2{\textbackslash}mbox\{,\}300{\textbackslash}mbox\{,\}000, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size n≈400n≈400n{\textbackslash}approx400, a 99.98\%99.98\%99.98{\textbackslash}\% reduction of sample size (and hence our confidence). The CCES data demonstrate LLP vividly: on average, the larger the state’s voter populations, the further away the actual Trump vote shares from the usual 95\%95\%95{\textbackslash}\% confidence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves.},
	language = {EN},
	number = {2},
	urldate = {2020-08-01},
	journal = {Annals of Applied Statistics},
	author = {Meng, Xiao-Li},
	month = jun,
	year = {2018},
	mrnumber = {MR3834282},
	zmnumber = {06980472},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bias-variance tradeoff, data confidentiality and privacy, data defect correlation, data defect index (d.d.i.), data quality-quantity tradeoff, Euler identity, Monte Carlo and Quasi Monte Carlo (MCQMC), non-response bias},
	pages = {685--726},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/NGH7IJCE/1532743473.html:text/html;Submitted Version:/home/bolker/Documents/zotero_new/storage/MNETFS2E/Meng - 2018 - Statistical paradises and paradoxes in big data (I.pdf:application/pdf},
}



@article{oksanenLogic2001,
	title = {Logic of {Experiments} in {Ecology}: {Is} {Pseudoreplication} a {Pseudoissue}?},
	volume = {94},
	issn = {0030-1299},
	shorttitle = {Logic of {Experiments} in {Ecology}},
	url = {http://www.jstor.org/stable/3547252},
	abstract = {Hurlbert divides experimental ecologist into 'those who do not see any need for dispersion (of replicated treatments and controls), and those who do recognize its importance and take whatever measures are necessary to achieve a good dose of it'. Experimental ecologists could also be divided into those who do not see any problems with sacrificing spatial and temporal scales in order to obtain replication, and those who understand that appropriate scale must always have priority over replication. If an experiment is conducted in a spatial or temporal scale, where the predictions of contesting hypotheses are convergent or ambiguous, no amount of technical impeccability can make the work instructive. Conversely, replication can always be obtained afterwards, by conducting more experiments with basically similar design in different areas and by using meta-analysis. This approach even reduces the sampling bias obtained if resources are allocated to a small number of well-replicated experiments. For a strict advocate of the hypothetico-deductive method, replication is unnecessary even as a matter of principle, unless the predicted response is so weak that random background noise is a plausible excuse for a discrepancy between predictions and results. By definition, a prediction is an 'all-statement', referring to all systems within a well-defined category. What applies to all must apply to any. Hence, choosing two systems and assigning them randomly to a treatment and a control is normally an adequate design for a deductive experiment. The strength of such experiments depends on the firmness of the predictions and their a priori probability of corroboration. Replication is but one of many ways of reducing this probability. Whether the experiment is replicated or not, inferential statistics should always be used, to enable the reader to judge how well the apparent patterns in samples reflect real patterns in statistical populations. The concept 'pseudoreplication' amounts to entirely unwarranted stigmatization of a reasonable way to test predictions referring to large-scale systems.},
	number = {1},
	urldate = {2016-12-31},
	journal = {Oikos},
	author = {Oksanen, Lauri},
	year = {2001},
	pages = {27--38},
	file = {JSTOR Full Text PDF:/home/bolker/Documents/zotero_new/storage/PNS94Z3Q/Oksanen - 2001 - Logic of Experiments in Ecology Is Pseudoreplicat.pdf:application/pdf},
}

@article{hurlbertPseudoreplication1984,
	title = {Pseudoreplication and the {Design} of {Ecological} {Field} {Experiments}},
	volume = {54},
	issn = {0012-9615},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.2307/1942661},
	doi = {10.2307/1942661},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27\% of them, or 48\% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing pre?layout (or conventional) and layout?specific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved.},
	number = {2},
	urldate = {2019-03-23},
	journal = {Ecological Monographs},
	author = {Hurlbert, Stuart H.},
	month = feb,
	year = {1984},
	pages = {187--211},
}
