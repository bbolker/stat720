@misc{shamsudheenShould2021,
  title = {Should We Test the Model Assumptions before Running a Model-Based Test?},
  author = {Shamsudheen, M. Iqbal and Hennig, Christian},
  year = {2021},
  month = mar,
  number = {arXiv:1908.02218},
  eprint = {1908.02218},
  primaryclass = {stat},
  institution = {{arXiv}},
  urldate = {2022-05-21},
  abstract = {Statistical methods are based on model assumptions, and it is statistical folklore that a method's model assumptions should be checked before applying it. This can be formally done by running one or more misspecification tests testing model assumptions before running a method that makes these assumptions; here we focus on model-based tests. A combined test procedure can be defined by specifying a protocol in which first model assumptions are tested and then, conditionally on the outcome, a test is run that requires or does not require the tested assumptions. Although such an approach is often taken in practice, much of the literature that investigated this is surprisingly critical of it. Our aim is to explore conditions under which model checking is advisable or not advisable. For this, we review results regarding such "combined procedures" in the literature, we review and discuss controversial views on the role of model checking in statistics, and we present a general setup in which we can show that preliminary model checking is advantageous, which implies conditions for making model checking worthwhile.},
  archiveprefix = {arxiv},
  keywords = {62F03,Statistics - Methodology},
  file = {/home/bolker/Zotero/storage/2WKT8FX9/Shamsudheen and Hennig - 2021 - Should we test the model assumptions before runnin.pdf;/home/bolker/Zotero/storage/V4P9CJQ7/1908.html}
}


@misc{rossFasteR2013,
  title = {{{FasteR}}! {{HigheR}}! {{StrongeR}}! - {{A Guide}} to {{Speeding Up R Code}} for {{Busy People}}},
  author = {Ross, Noam},
  year = {2013},
  month = apr,
  journal = {Noam Ross},
  urldate = {2016-09-02},
  howpublished = {http://www.noamross.net/blog/2013/4/25/faster-talk.html},
  file = {/home/bolker/Zotero/storage/64SSBRVJ/faster-talk.html}
}

@techreport{bryanExcuse2017,
  title = {Excuse Me, Do You Have a Moment to Talk about Version Control?},
  author = {Bryan, Jennifer},
  year = {2017},
  month = aug,
  number = {e3159v2},
  institution = {{PeerJ Inc.}},
  issn = {2167-9843},
  doi = {10.7287/peerj.preprints.3159v2},
  urldate = {2022-10-18},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  langid = {english},
  file = {/home/bolker/Zotero/storage/9YR9ZKL3/Bryan - 2017 - Excuse me, do you have a moment to talk about vers.pdf;/home/bolker/Zotero/storage/M9SGPHLK/3159v2.html}
}

@misc{bryanProjectoriented2017,
  title = {Project-Oriented Workflow},
  author = {Bryan, Jenny},
  year = {2017},
  month = dec,
  journal = {Tidyverse},
  urldate = {2021-01-14},
  abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
  howpublished = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
  langid = {american},
  file = {/home/bolker/Zotero/storage/2UCXRWUV/workflow-vs-script.html}
}

@article{wilsonGood2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2017},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {6},
  pages = {e1005510},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  urldate = {2019-03-16},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  langid = {english},
  keywords = {Computer software,Control systems,Data management,Data processing,Programming languages,Reproducibility,Software tools,Source code},
  file = {/home/bolker/Zotero/storage/7FVVG9HG/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/home/bolker/Zotero/storage/MRE6NA97/article.html}
}


@book{harrellRegression2015,
  title = {Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis},
  shorttitle = {Regression {{Modeling Strategies}}},
  author = {Harrell Jr., Frank E.},
  year = {2015},
  month = aug,
  edition = {2d},
  publisher = {Springer},
  isbn = {978-3-319-19424-0},
  note = {This book presents the best blend I've ever found of practical yet rigorous advice for statistical modeling (testing model assumptions, limiting model complexity in a way that preserves reliable inference, dealing with missing data, etc.). It covers linear and generalized linear regression, logistic regression, ordinal responses, and survival analysis thoroughly; it has a chapter on longitudinal data. It does not go deeply into mixed models, nor does it cover more exotic responses (count or non-binary binomial responses, zero-inflation). It uses the author's own \texttt{rms} package, which is useful but doesn't always fit in perfectly with other frameworks.}
  }

@book{farawayExtending2016,
  title = {Extending the {{Linear Model}} with {{R}}: {{Generalized Linear}}, {{Mixed Effects}} and {{Nonparametric Regression Models}}, {{Second Edition}}},
  shorttitle = {Extending the {{Linear Model}} with {{R}}},
  author = {Faraway, Julian J.},
  year = {2016},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Start Analyzing a Wide Range of Problems  Since the publication of the bestselling, highly recommended first edition, R has considerably expanded both in popularity and in the number of packages available. Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models, Second Edition takes advantage of the greater functionality now available in R and substantially revises and adds several topics. New to the Second Edition  Expanded coverage of binary and binomial responses, including proportion responses, quasibinomial and beta regression, and applied considerations regarding these models  New sections on Poisson models with dispersion, zero inflated count models, linear discriminant analysis, and sandwich and robust estimation for generalized linear models (GLMs)  Revised chapters on random effects and repeated measures that reflect changes in the lme4 package and show how to perform hypothesis testing for the models using other methods New chapter on the Bayesian analysis of mixed effect models that illustrates the use of STAN and presents the approximation method of INLA  Revised chapter on generalized linear mixed models to reflect the much richer choice of fitting software now available Updated coverage of splines and confidence bands in the chapter on nonparametric regression New material on random forests for regression and classification  Revamped R code throughout, particularly the many plots using the ggplot2 package Revised and expanded exercises with solutions now included Demonstrates the Interplay of Theory and Practice This textbook continues to cover a range of techniques that grow from the linear regression model. It presents three extensions to the linear framework: GLMs, mixed effect models, and nonparametric regression models. The book explains data analysis using real examples and includes all the R commands necessary to reproduce the analyses. The level is good for upper-level undergraduate statisticians and beyond, possibly tough for biologists.},
  googlebooks = {XAzYCwAAQBAJ},
  isbn = {978-1-4987-2098-4},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
  note = {A good, broad book that covers most of the material we'll discuss in this class. It starts with a little bit more basic coverage of (G)LMs, but has good discussions of intermediate-level GLM issues (conditional logistic regression, alternative link functions, etc.). It goes on to cover mixed models and additive models, as well as a brief introduction to tree-based models and neural networks (which we'll skip in this class).}
}

@book{gelmanData2006,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, England}},
  note = {Clear, comprehensive discussion of multilevel/hierarchical models. 99\% Bayesian treatment; examples are mostly from social science. Covers linear, logistic, count-data (Poisson/negative binomial) regressions. Causal inference, regression diagnostics, etc.. Reliance on the BUGS language is now slightly old-fashioned (Gelman et al 2020 is an update for the non-mixed model material; these authors and others are working on an updated mixed model book, nominally available \href{http://www.stat.columbia.edu/~gelman/armm/}{around the end of 2024}).}
}

@book{gelmanRegression2020,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year = {2020},
  month = jul,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore}},
  abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
  isbn = {978-1-107-67651-0},
  langid = {english},
  note = {I haven't read this book yet but based on the authors' past work I expect it to be excellent.}
}


@book{hodgesRichly2013,
  title = {Richly Parameterized Linear Models: Additive, Time Series, and Spatial Models Using Random Effects},
  shorttitle = {Richly Parameterized Linear Models},
  author = {Hodges, James S.},
  year = {2013},
  publisher = {{CRC Press}},
  note = {A technical, in-depth exploration of the ways that hierarchical Gaussian linear models can be extended to a huge variety of different kinds of correlation structures and smooth functions. Worked examples of tricky real-world examples; discusses details you won't find anywhere else.}
}

@book{mcelreathStatistical2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {6H\_WDwAAQBAJ},
  isbn = {978-0-429-63914-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
  note = {Bayesian statistics from first principles. Aimed at non-statisticians. Relies on Stan for analysis (the \texttt{rethinking} package provides a nice front end for graphical models). Heavy on mechanistic models and causal inference.}
}

@book{woodGeneralized2017,
  title = {Generalized {{Additive Models}}: {{An Introduction}} with {{R}}},
  author = {Wood, Simon N.},
  year = {2017},
  series = {{{CRC Texts}} in {{Statistical Science}}},
  publisher = {{Chapman \& Hall}},
  urldate = {2017-11-28},
  file = {/home/bolker/Zotero/storage/Q79P94BB/ref=sr_1_1.html},
  note = {Comprehensive coverage of additive models from a penalized-basis point of view, from the master (the author of R's \texttt{mgcv} package). Includes brief reviews of the theory behind linear and GLMs. Technical bits are tough for non-statisticians.}
}


@misc{navarroScience2019,
  title = {Science and Statistics},
  author = {Navarro, Danielle},
  year = {2019},
  month = mar,
  address = {{Aarhus University}},
  abstract = {http://interactingminds.au.dk/events/single-events/artikel/2-day-workshop-open-science-and-reproducibility/},
  langid = {english},
  file = {/home/bolker/Zotero/storage/UX69QA8G/scienceandstatistics.html},
  url = {https://slides.com/djnavarro/scienceandstatistics}
}


@article{boxScience1976b,
	title = {Science and {Statistics}},
	volume = {71},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949},
	doi = {10.1080/01621459.1976.10480949},
	abstract = {Aspects of scientific method are discussed: In particular, its representation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires sufficient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
	number = {356},
	urldate = {2023-09-02},
	journal = {Journal of the American Statistical Association},
	author = {Box, George E. P.},
	month = dec,
	year = {1976},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1976.10480949},
	pages = {791--799}
}


@book{mccullaghGeneralized1989,
	address = {London},
	title = {Generalized {Linear} {Models}},
	publisher = {Chapman and Hall},
	author = {McCullagh, P. and Nelder, J. A.},
	year = {1989},
}


@article{gelman_statistical_2014,
	title = {The statistical crisis in science: data-dependent analysis--a "garden of forking paths"--explains why many statistically significant comparisons don't hold up},
	volume = {102},
	issn = {0003-0996},
	shorttitle = {The statistical crisis in science},
	url = {http://link.galegroup.com/apps/doc/A389260653/AONE?u=ocul_mcmaster&sid=AONE&xid=4f4562c0},
	language = {English},
	number = {6},
	urldate = {2019-01-07},
	journal = {American Scientist},
	author = {Gelman, Andrew and Loken, Eric},
	year = {2014},
	note = {460},
	keywords = {Periodical publishing, Science journals},
	pages = {460--}
}

@article{simmons_false-positive_2011,
	title = {False-Positive Psychology: Undisclosed Flexibility  in Data Collection and Analysis Allows Presenting Anything as Significant},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/11/1359},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2015-11-08},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	keywords = {disclosure, methodology, motivated reasoning, publication},
	pages = {1359--1366},
	file = {Full Text PDF:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/TNSUZHFS/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility .pdf:application/pdf;Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/BATW66XJ/1359.html:text/html}
}
