@misc{shamsudheenShould2021,
  title = {Should We Test the Model Assumptions before Running a Model-Based Test?},
  author = {Shamsudheen, M. Iqbal and Hennig, Christian},
  year = {2021},
  month = mar,
  number = {arXiv:1908.02218},
  eprint = {1908.02218},
  primaryclass = {stat},
  institution = {{arXiv}},
  urldate = {2022-05-21},
  abstract = {Statistical methods are based on model assumptions, and it is statistical folklore that a method's model assumptions should be checked before applying it. This can be formally done by running one or more misspecification tests testing model assumptions before running a method that makes these assumptions; here we focus on model-based tests. A combined test procedure can be defined by specifying a protocol in which first model assumptions are tested and then, conditionally on the outcome, a test is run that requires or does not require the tested assumptions. Although such an approach is often taken in practice, much of the literature that investigated this is surprisingly critical of it. Our aim is to explore conditions under which model checking is advisable or not advisable. For this, we review results regarding such "combined procedures" in the literature, we review and discuss controversial views on the role of model checking in statistics, and we present a general setup in which we can show that preliminary model checking is advantageous, which implies conditions for making model checking worthwhile.},
  archiveprefix = {arxiv},
  keywords = {62F03,Statistics - Methodology},
  file = {/home/bolker/Zotero/storage/2WKT8FX9/Shamsudheen and Hennig - 2021 - Should we test the model assumptions before runnin.pdf;/home/bolker/Zotero/storage/V4P9CJQ7/1908.html}
}


@misc{rossFasteR2013,
  title = {{{FasteR}}! {{HigheR}}! {{StrongeR}}! - {{A Guide}} to {{Speeding Up R Code}} for {{Busy People}}},
  author = {Ross, Noam},
  year = {2013},
  month = apr,
  journal = {Noam Ross},
  urldate = {2016-09-02},
  howpublished = {http://www.noamross.net/blog/2013/4/25/faster-talk.html},
  file = {/home/bolker/Zotero/storage/64SSBRVJ/faster-talk.html}
}

@techreport{bryanExcuse2017,
  title = {Excuse Me, Do You Have a Moment to Talk about Version Control?},
  author = {Bryan, Jennifer},
  year = {2017},
  month = aug,
  number = {e3159v2},
  institution = {{PeerJ Inc.}},
  issn = {2167-9843},
  doi = {10.7287/peerj.preprints.3159v2},
  urldate = {2022-10-18},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  langid = {english},
  file = {/home/bolker/Zotero/storage/9YR9ZKL3/Bryan - 2017 - Excuse me, do you have a moment to talk about vers.pdf;/home/bolker/Zotero/storage/M9SGPHLK/3159v2.html}
}

@misc{bryanProjectoriented2017,
  title = {Project-Oriented Workflow},
  author = {Bryan, Jenny},
  year = {2017},
  month = dec,
  journal = {Tidyverse},
  urldate = {2021-01-14},
  abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
  url = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
  langid = {american},
  file = {/home/bolker/Zotero/storage/2UCXRWUV/workflow-vs-script.html}
}

@article{wilsonGood2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2017},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {6},
  pages = {e1005510},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  urldate = {2019-03-16},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  langid = {english},
  keywords = {Computer software,Control systems,Data management,Data processing,Programming languages,Reproducibility,Software tools,Source code},
  file = {/home/bolker/Zotero/storage/7FVVG9HG/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/home/bolker/Zotero/storage/MRE6NA97/article.html}
}


@book{harrellRegression2015,
  title = {Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis},
  shorttitle = {Regression {{Modeling Strategies}}},
  author = {Harrell Jr., Frank E.},
  year = {2015},
  month = aug,
  edition = {2d},
  publisher = {Springer},
  isbn = {978-3-319-19424-0},
  note = {This book presents the best blend I've ever found of practical yet rigorous advice for statistical modeling (testing model assumptions, limiting model complexity in a way that preserves reliable inference, dealing with missing data, etc.). It covers linear and generalized linear regression, logistic regression, ordinal responses, and survival analysis thoroughly; it has a chapter on longitudinal data. It does not go deeply into mixed models, nor does it cover more exotic responses (count or non-binary binomial responses, zero-inflation). It uses the author's own \texttt{rms} package, which is useful but doesn't always fit in perfectly with other frameworks.}
  }

@book{farawayExtending2016,
  title = {Extending the {{Linear Model}} with {{R}}: {{Generalized Linear}}, {{Mixed Effects}} and {{Nonparametric Regression Models}}, {{Second Edition}}},
  shorttitle = {Extending the {{Linear Model}} with {{R}}},
  author = {Faraway, Julian J.},
  year = {2016},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Start Analyzing a Wide Range of Problems  Since the publication of the bestselling, highly recommended first edition, R has considerably expanded both in popularity and in the number of packages available. Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models, Second Edition takes advantage of the greater functionality now available in R and substantially revises and adds several topics. New to the Second Edition  Expanded coverage of binary and binomial responses, including proportion responses, quasibinomial and beta regression, and applied considerations regarding these models  New sections on Poisson models with dispersion, zero inflated count models, linear discriminant analysis, and sandwich and robust estimation for generalized linear models (GLMs)  Revised chapters on random effects and repeated measures that reflect changes in the lme4 package and show how to perform hypothesis testing for the models using other methods New chapter on the Bayesian analysis of mixed effect models that illustrates the use of STAN and presents the approximation method of INLA  Revised chapter on generalized linear mixed models to reflect the much richer choice of fitting software now available Updated coverage of splines and confidence bands in the chapter on nonparametric regression New material on random forests for regression and classification  Revamped R code throughout, particularly the many plots using the ggplot2 package Revised and expanded exercises with solutions now included Demonstrates the Interplay of Theory and Practice This textbook continues to cover a range of techniques that grow from the linear regression model. It presents three extensions to the linear framework: GLMs, mixed effect models, and nonparametric regression models. The book explains data analysis using real examples and includes all the R commands necessary to reproduce the analyses. The level is good for upper-level undergraduate statisticians and beyond, possibly tough for biologists.},
  googlebooks = {XAzYCwAAQBAJ},
  isbn = {978-1-4987-2098-4},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
  note = {A good, broad book that covers most of the material we'll discuss in this class. It starts with a little bit more basic coverage of (G)LMs, but has good discussions of intermediate-level GLM issues (conditional logistic regression, alternative link functions, etc.). It goes on to cover mixed models and additive models, as well as a brief introduction to tree-based models and neural networks (which we'll skip in this class).}
}

@book{gelmanData2006,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, England}},
  note = {Clear, comprehensive discussion of multilevel/hierarchical models. 99\% Bayesian treatment; examples are mostly from social science. Covers linear, logistic, count-data (Poisson/negative binomial) regressions. Causal inference, regression diagnostics, etc.. Reliance on the BUGS language is now slightly old-fashioned (Gelman et al 2020 is an update for the non-mixed model material; these authors and others are working on an updated mixed model book, nominally available \href{http://www.stat.columbia.edu/~gelman/armm/}{around the end of 2024}).}
}

@book{gelmanRegression2020,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year = {2020},
  month = jul,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore}},
  abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
  isbn = {978-1-107-67651-0},
  langid = {english},
  note = {I haven't read this book yet but based on the authors' past work I expect it to be excellent.}
}


@book{hodgesRichly2013,
  title = {Richly Parameterized Linear Models: Additive, Time Series, and Spatial Models Using Random Effects},
  shorttitle = {Richly Parameterized Linear Models},
  author = {Hodges, James S.},
  year = {2013},
  publisher = {{CRC Press}},
  note = {A technical, in-depth exploration of the ways that hierarchical Gaussian linear models can be extended to a huge variety of different kinds of correlation structures and smooth functions. Worked examples of tricky real-world examples; discusses details you won't find anywhere else.}
}

@book{mcelreathStatistical2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {6H\_WDwAAQBAJ},
  isbn = {978-0-429-63914-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
  note = {Bayesian statistics from first principles. Aimed at non-statisticians. Relies on Stan for analysis (the \texttt{rethinking} package provides a nice front end for graphical models). Heavy on mechanistic models and causal inference.}
}

@book{woodGeneralized2017,
  title = {Generalized {{Additive Models}}: {{An Introduction}} with {{R}}},
  author = {Wood, Simon N.},
  year = {2017},
  series = {{{CRC Texts}} in {{Statistical Science}}},
  publisher = {{Chapman \& Hall}},
  urldate = {2017-11-28},
  file = {/home/bolker/Zotero/storage/Q79P94BB/ref=sr_1_1.html},
  note = {Comprehensive coverage of additive models from a penalized-basis point of view, from the master (the author of R's \texttt{mgcv} package). Includes brief reviews of the theory behind linear and GLMs. Technical bits are tough for non-statisticians.}
}


@misc{navarroScience2019,
  title = {Science and Statistics},
  author = {Navarro, Danielle},
  year = {2019},
  month = mar,
  address = {{Aarhus University}},
  abstract = {http://interactingminds.au.dk/events/single-events/artikel/2-day-workshop-open-science-and-reproducibility/},
  langid = {english},
  file = {/home/bolker/Zotero/storage/UX69QA8G/scienceandstatistics.html},
  url = {https://slides.com/djnavarro/scienceandstatistics}
}


@article{boxScience1976b,
	title = {Science and {Statistics}},
	volume = {71},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949},
	doi = {10.1080/01621459.1976.10480949},
	abstract = {Aspects of scientific method are discussed: In particular, its representation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires sufficient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
	number = {356},
	urldate = {2023-09-02},
	journal = {Journal of the American Statistical Association},
	author = {Box, George E. P.},
	month = dec,
	year = {1976},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1976.10480949},
	pages = {791--799}
}


@book{mccullaghGeneralized1989,
	address = {London},
	title = {Generalized {Linear} {Models}},
	publisher = {Chapman \& Hall},
	author = {McCullagh, P. and Nelder, J. A.},
	year = {1989},
}


@article{gelman_statistical_2014,
	title = {The statistical crisis in science: data-dependent analysis--a "garden of forking paths"--explains why many statistically significant comparisons don't hold up},
	volume = {102},
	issn = {0003-0996},
	shorttitle = {The statistical crisis in science},
	url = {http://link.galegroup.com/apps/doc/A389260653/AONE?u=ocul_mcmaster&sid=AONE&xid=4f4562c0},
	language = {English},
	number = {6},
	urldate = {2019-01-07},
	journal = {American Scientist},
	author = {Gelman, Andrew and Loken, Eric},
	year = {2014},
	note = {460},
	keywords = {Periodical publishing, Science journals},
	pages = {460--}
}

@article{simmons_false-positive_2011,
	title = {False-Positive Psychology: Undisclosed Flexibility  in Data Collection and Analysis Allows Presenting Anything as Significant},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/11/1359},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2015-11-08},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	keywords = {disclosure, methodology, motivated reasoning, publication},
	pages = {1359--1366},
	file = {Full Text PDF:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/TNSUZHFS/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility .pdf:application/pdf;Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/BATW66XJ/1359.html:text/html}
}


@article{schielzethSimple2010,
	title = {Simple means to improve the interpretability of regression coefficients: {Interpretation} of regression coefficients},
	volume = {1},
	issn = {2041210X, 2041210X},
	shorttitle = {Simple means to improve the interpretability of regression coefficients},
	url = {http://doi.wiley.com/10.1111/j.2041-210X.2010.00012.x},
	doi = {10.1111/j.2041-210X.2010.00012.x},
	language = {en},
	number = {2},
	urldate = {2016-06-08},
	journal = {Methods in Ecology and Evolution},
	author = {Schielzeth, Holger},
	month = feb,
	year = {2010},
	pages = {103--113},
	file = {j.2041-210X.2010.00012.x.pdf:/home/bolker/Documents/zotero_new/storage/XNG3ZF5X/j.2041-210X.2010.00012.x.pdf:application/pdf},
}

@article{rochon_test_2012,
	title = {To test or not to test: {Preliminary} assessment of normality when comparing two independent samples},
	volume = {12},
	issn = {1471-2288},
	shorttitle = {To test or not to test},
	url = {https://doi.org/10.1186/1471-2288-12-81},
	doi = {10.1186/1471-2288-12-81},
	abstract = {Student’s two-sample t test is generally used for comparing the means of two independent samples, for example, two treatment arms. Under the null hypothesis, the t test assumes that the two samples arise from the same normally distributed population with unknown variance. Adequate control of the Type I error requires that the normality assumption holds, which is often examined by means of a preliminary Shapiro-Wilk test. The following two-stage procedure is widely accepted: If the preliminary test for normality is not significant, the t test is used; if the preliminary test rejects the null hypothesis of normality, a nonparametric test is applied in the main analysis.},
	number = {1},
	urldate = {2020-07-10},
	journal = {BMC Medical Research Methodology},
	author = {Rochon, Justine and Gondan, Matthias and Kieser, Meinhard},
	month = jun,
	year = {2012},
	pages = {81},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/ZY252N86/Rochon et al. - 2012 - To test or not to test Preliminary assessment of .pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/NH5ULAHD/1471-2288-12-81.html:text/html}
}


@article{campbellconsequences2021a,
	title = {The consequences of checking for zero-inflation and overdispersion in the analysis of count data},
	volume = {12},
	copyright = {© 2021 British Ecological Society},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13559},
	doi = {10.1111/2041-210X.13559},
	abstract = {Count data are ubiquitous in ecology and the Poisson generalized linear model (GLM) is commonly used to model the association between counts and explanatory variables of interest. When fitting this model to the data, one typically proceeds by first confirming that the model assumptions are satisfied. If the residuals appear to be overdispersed or if there is zero-inflation, key assumptions of the Poison GLM may be violated and researchers will then typically consider alternatives to the Poison GLM. An important question is whether the potential model selection bias introduced by this data-driven multi-stage procedure merits concern. Here we conduct a large-scale simulation study to investigate the potential consequences of model selection bias that can arise in the simple scenario of analysing a sample of potentially overdispersed, potentially zero-inflated, count data. Specifically, we investigate model selection procedures recently recommended by Blasco-Moreno et al. (2019) using either a series of score tests or information theoretic criteria to select the best model. We find that, when sample sizes are small, model selection based on preliminary score tests (or information theoretic criteria, e.g. AIC, BIC) can lead to potentially substantial inflation of false positive rates (i.e. type 1 error inflation). When sample sizes are sufficiently large, model selection based on preliminary score tests, is not problematic. Ignoring the possibility of overdispersion and zero-inflation during data analyses can lead to invalid inference. However, if one does not have sufficient power to test for overdispersion and zero-inflation, post hoc model selection may also lead to substantial bias. This ‘catch-22’ suggests that, if sample sizes are small, a healthy skepticism is warranted whenever one rejects the null hypothesis of no association between a given outcome and covariate.},
	language = {en},
	number = {4},
	urldate = {2023-09-02},
	journal = {Methods in Ecology and Evolution},
	author = {Campbell, Harlan},
	year = {2021},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13559},
	keywords = {model selection bias, overdispersion, zero-inflated models, zero-inflation},
	pages = {665--680},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/B52K6P2P/Campbell - 2021 - The consequences of checking for zero-inflation an.pdf:application/pdf},
}

@article{campbell_consequences_2014,
	title = {The consequences of proportional hazards based model selection},
	volume = {33},
	copyright = {Copyright © 2013 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6021},
	doi = {10.1002/sim.6021},
	abstract = {For testing the efficacy of a treatment in a clinical trial with survival data, the Cox proportional hazards (PH) model is the well-accepted, conventional tool. When using this model, one typically proceeds by confirming that the required PH assumption holds true. If the PH assumption fails to hold, there are many options available, proposed as alternatives to the Cox PH model. An important question which arises is whether the potential bias introduced by this sequential model fitting procedure merits concern and, if so, what are effective mechanisms for correction. We investigate by means of simulation study and draw attention to the considerable drawbacks, with regard to power, of a simple resampling technique, the permutation adjustment, a natural recourse for addressing such challenges. We also consider a recently proposed two-stage testing strategy (2008) for ameliorating these effects. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2020-07-10},
	journal = {Statistics in Medicine},
	author = {Campbell, H. and Dean, C. B.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6021},
	keywords = {model selection bias, proportional hazards, two-stage approach},
	pages = {1042--1056}
}


@article{box_non-normality_1953,
	title = {Non-{Normality} and {Tests} on {Variances}},
	volume = {40},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2333350},
	doi = {10.2307/2333350},
	number = {3/4},
	urldate = {2020-07-11},
	journal = {Biometrika},
	author = {Box, G. E. P.},
	year = {1953},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {318--335}
}


@article{zimmermannote2004,
	title = {A note on preliminary tests of equality of variances},
	volume = {57},
	copyright = {2004 The British Psychological Society},
	issn = {2044-8317},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/000711004849222},
	doi = {10.1348/000711004849222},
	abstract = {Preliminary tests of equality of variances used before a test of location are no longer widely recommended by statisticians, although they persist in some textbooks and software packages. The present study extends the findings of previous studies and provides further reasons for discontinuing the use of preliminary tests. The study found Type I error rates of a two-stage procedure, consisting of a preliminary Levene test on samples of different sizes with unequal variances, followed by either a Student pooled-variances t test or a Welch separate-variances t test. Simulations disclosed that the twostage procedure fails to protect the significance level and usually makes the situation worse. Earlier studies have shown that preliminary tests often adversely affect the size of the test, and also that the Welch test is superior to the t test when variances are unequal. The present simulations reveal that changes in Type I error rates are greater when sample sizes are smaller, when the difference in variances is slight rather than extreme, and when the significance level is more stringent. Furthermore, the validity of the Welch test deteriorates if it is used only on those occasions where a preliminary test indicates it is needed. Optimum protection is assured by using a separate-variances test unconditionally whenever sample sizes are unequal.},
	language = {en},
	number = {1},
	urldate = {2020-10-01},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Zimmerman, Donald W.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000711004849222},
	pages = {173--181},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/W8RPHCGT/Zimmerman - 2004 - A note on preliminary tests of equality of varianc.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/2S6W6AZG/000711004849222.html:text/html},
}


@article{mengStatistical2018,
	title = {Statistical paradises and paradoxes in big data ({I}): {Law} of large populations, big data paradox, and the 2016 {US} presidential election},
	volume = {12},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Statistical paradises and paradoxes in big data ({I})},
	url = {https://projecteuclid.org/euclid.aoas/1532743473},
	doi = {10.1214/18-AOAS1161SF},
	abstract = {Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualifications for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: “Which one should I trust more: a 1\% survey with 60\% response rate or a self-reported administrative dataset covering 80\% of the population?” A 5-element Euler-formula-like identity shows that for any dataset of size nnn, probabilistic or not, the difference between the sample average X¯¯¯¯nX¯n{\textbackslash}overline\{X\}\_\{n\} and the population average X¯¯¯¯NX¯N{\textbackslash}overline\{X\}\_\{N\} is the product of three terms: (1) a data quality measure, ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\}, the correlation between XjXjX\_\{j\} and the response/recording indicator RjRjR\_\{j\}; (2) a data quantity measure, (N−n)/n−−−−−−−−−√(N−n)/n{\textbackslash}sqrt\{(N-n)/n\}, where NNN is the population size; and (3) a problem difficulty measure, σXσX{\textbackslash}sigma\_\{X\}, the standard deviation of XXX. This decomposition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\} at the level of N−1/2N−1/2N{\textasciicircum}\{-1/2\}; (II) When we lose this control, the impact of NNN is no longer canceled by ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\}, leading to a Law of Large Populations (LLP), that is, our estimation error, relative to the benchmarking rate 1/n−−√1/n1/{\textbackslash}sqrt\{n\}, increases with N−−√N{\textbackslash}sqrt\{N\}; and (III) the “bigness” of such Big Data (for population inferences) should be measured by the relative size f=n/Nf=n/Nf=n/N, not the absolute size nnn; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by their sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a ρR,X≈−0.005ρR,X≈−0.005{\textbackslash}rho\_\{\{R,X\}\}{\textbackslash}approx-0.005 for self-reporting to vote for Donald Trump. Because of LLP, this seemingly minuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from 1\%1\%1{\textbackslash}\% of the US eligible voters, that is, n≈2,300,000n≈2,300,000n{\textbackslash}approx2{\textbackslash}mbox\{,\}300{\textbackslash}mbox\{,\}000, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size n≈400n≈400n{\textbackslash}approx400, a 99.98\%99.98\%99.98{\textbackslash}\% reduction of sample size (and hence our confidence). The CCES data demonstrate LLP vividly: on average, the larger the state’s voter populations, the further away the actual Trump vote shares from the usual 95\%95\%95{\textbackslash}\% confidence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves.},
	language = {EN},
	number = {2},
	urldate = {2020-08-01},
	journal = {Annals of Applied Statistics},
	author = {Meng, Xiao-Li},
	month = jun,
	year = {2018},
	mrnumber = {MR3834282},
	zmnumber = {06980472},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bias-variance tradeoff, data confidentiality and privacy, data defect correlation, data defect index (d.d.i.), data quality-quantity tradeoff, Euler identity, Monte Carlo and Quasi Monte Carlo (MCQMC), non-response bias},
	pages = {685--726},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/NGH7IJCE/1532743473.html:text/html;Submitted Version:/home/bolker/Documents/zotero_new/storage/MNETFS2E/Meng - 2018 - Statistical paradises and paradoxes in big data (I.pdf:application/pdf},
}



@article{oksanenLogic2001,
	title = {Logic of {Experiments} in {Ecology}: {Is} {Pseudoreplication} a {Pseudoissue}?},
	volume = {94},
	issn = {0030-1299},
	shorttitle = {Logic of {Experiments} in {Ecology}},
	url = {http://www.jstor.org/stable/3547252},
	abstract = {Hurlbert divides experimental ecologist into 'those who do not see any need for dispersion (of replicated treatments and controls), and those who do recognize its importance and take whatever measures are necessary to achieve a good dose of it'. Experimental ecologists could also be divided into those who do not see any problems with sacrificing spatial and temporal scales in order to obtain replication, and those who understand that appropriate scale must always have priority over replication. If an experiment is conducted in a spatial or temporal scale, where the predictions of contesting hypotheses are convergent or ambiguous, no amount of technical impeccability can make the work instructive. Conversely, replication can always be obtained afterwards, by conducting more experiments with basically similar design in different areas and by using meta-analysis. This approach even reduces the sampling bias obtained if resources are allocated to a small number of well-replicated experiments. For a strict advocate of the hypothetico-deductive method, replication is unnecessary even as a matter of principle, unless the predicted response is so weak that random background noise is a plausible excuse for a discrepancy between predictions and results. By definition, a prediction is an 'all-statement', referring to all systems within a well-defined category. What applies to all must apply to any. Hence, choosing two systems and assigning them randomly to a treatment and a control is normally an adequate design for a deductive experiment. The strength of such experiments depends on the firmness of the predictions and their a priori probability of corroboration. Replication is but one of many ways of reducing this probability. Whether the experiment is replicated or not, inferential statistics should always be used, to enable the reader to judge how well the apparent patterns in samples reflect real patterns in statistical populations. The concept 'pseudoreplication' amounts to entirely unwarranted stigmatization of a reasonable way to test predictions referring to large-scale systems.},
	number = {1},
	urldate = {2016-12-31},
	journal = {Oikos},
	author = {Oksanen, Lauri},
	year = {2001},
	pages = {27--38},
	file = {JSTOR Full Text PDF:/home/bolker/Documents/zotero_new/storage/PNS94Z3Q/Oksanen - 2001 - Logic of Experiments in Ecology Is Pseudoreplicat.pdf:application/pdf},
}

@article{hurlbertPseudoreplication1984,
	title = {Pseudoreplication and the {Design} of {Ecological} {Field} {Experiments}},
	volume = {54},
	issn = {0012-9615},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.2307/1942661},
	doi = {10.2307/1942661},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27\% of them, or 48\% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing pre?layout (or conventional) and layout?specific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved.},
	number = {2},
	urldate = {2019-03-23},
	journal = {Ecological Monographs},
	author = {Hurlbert, Stuart H.},
	month = feb,
	year = {1984},
	pages = {187--211},
}



@article{wilkinsonSymbolic1973a,
	title = {Symbolic {Description} of {Factorial} {Models} for {Analysis} of {Variance}},
	volume = {22},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2346786},
	doi = {10.2307/2346786},
	abstract = {The paper describes the symbolic notation and syntax for specifying factorial models for analysis of variance in the control language of the GENSTAT statistical program system at Rothamsted. The notation generalizes that of Nelder (1965). Algorithm AS 65 (Rogers, 1973) converts factorial model formulae in this notation to a list of model terms represented as binary integers. A further extension of the syntax is discussed for specifying models generally (including non-linear forms).},
	number = {3},
	urldate = {2011-01-21},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Wilkinson, G. N. and Rogers, C. E.},
	month = jan,
	year = {1973},
	note = {ArticleType: research-article / Full publication date: 1973 / Copyright © 1973 Royal Statistical Society},
	pages = {392--399},
	file = {JSTOR Full Text PDF:/home/bolker/Documents/zotero_new/storage/JRWFNFZT/Wilkinson and Rogers - 1973 - Symbolic Description of Factorial Models for Analy.pdf:application/pdf},
}


@book{chambersStatistical1991,
	edition = {1},
	title = {Statistical {Models} in {S}},
	isbn = {0-412-83040-X},
	publisher = {Chapman \& Hall/CRC},
	editor = {Chambers, J. M. and Hastie, T. J.},
	month = oct,
	year = {1991},
}


@inproceedings{venablesExegeses1998,
	address = {Washington, DC},
	series = {1998 {International} {S}-{PLUS} {User} {Conference}},
	title = {Exegeses on {Linear} {Models}},
	url = {http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf},
	author = {Venables, W. N},
	year = {1998},
}



@article{dushoffcan2018,
	title = {I can see clearly now: reinterpreting statistical significance},
	shorttitle = {I can see clearly now},
	url = {https://arxiv.org/abs/1810.06387},
	language = {en},
	urldate = {2018-11-14},
	author = {Dushoff, Jonathan and Kain, Morgan P. and Bolker, Benjamin M.},
	month = oct,
	year = {2018},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/8T5WJIZI/Dushoff et al. - 2018 - I can see clearly now reinterpreting statistical .pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/IHD9I8RZ/1810.html:text/html},
}


@misc{hennigTesting2022,
	address = {University of Bologna},
	title = {Testing in models that are not true},
	url = {https://www.wu.ac.at/fileadmin/wu/d/i/statmath/Research_Seminar/SS_2022/2022_04_Hennig.pdf},
	language = {en},
	author = {Hennig, Christian},
	year = {2022},
	file = {Hennig - Testing in models that are not true.pdf:/home/bolker/Documents/zotero_new/storage/IAY8FNB5/Hennig - Testing in models that are not true.pdf:application/pdf},
}



@article{schoenerNonsynchronous1970,
	title = {Nonsynchronous {Spatial} {Overlap} of {Lizards} in {Patchy} {Habitats}},
	volume = {51},
	issn = {0012-9658},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.2307/1935376},
	doi = {10.2307/1935376},
	abstract = {Sympatric native Anolis species with similar structural habitats but contrasting climatic habitats are closer in head and body size on species?rich than on depauperate islands. In two localities, sympatric Anolis species with differential occurrences in sun or shade sought lower, more shaded perches during midday, resulting in partly nonsynchronous utilization of the vegetation by the two species. The second observation may be related to the first in the following way: nonsynchronous spatial overlap could dictate relatively great resource overlap for species coinhabiting patchy or edge areas, requiring great differences between the species in prey size in addition to those in climatic habitat. The extent of such overlap on small depauperate islands could be greater if these contained a greater proportion of patchy or edge habitats (with respect to insolation), or if climatic preferences were broader and more overlapping than on large, species?rich islands. In each locality, the relatively more shade?inhabiting species occurred more often on larger perches and on lower perches than did the other species. In both species of the Bermudan pair, adult males occupied higher and larger perches, and in grahami, shadier perches, than did female?sized individuals. The statistical significance of these and other differences was evaluated using several unweighted g2 procedures, Cochran's weighted g2 test and a partitioning technique for analyzing interactions among variables in complex contingency tables. The last method is described in detail in the papaer by Fienberg, immediately following this one.},
	number = {3},
	urldate = {2018-04-21},
	journal = {Ecology},
	author = {Schoener, Thomas W.},
	month = may,
	year = {1970},
	pages = {408--418},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/Z25TPT39/Schoener Thomas W. - 1970 - Nonsynchronous Spatial Overlap of Lizards in Patch.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/JUIRP249/1935376.html:text/html},
}


@article{mckeonMultiple2012b,
	title = {Multiple defender effects: synergistic coral defense by mutualist crustaceans},
	volume = {169},
	issn = {0029-8549},
	shorttitle = {Multiple defender effects},
	url = {http://www.springerlink.com/content/nm20758r6557v448/abstract/},
	doi = {10.1007/s00442-012-2275-2},
	abstract = {The majority of our understanding of mutualisms comes from studies of pairwise interactions. However, many hosts support mutualist guilds, and interactions among mutualists make the prediction of aggregate effects difficult. Here, we apply a factorial experiment to interactions of ‘guard’ crustaceans that defend their coral host from seastar predators. Predation was reduced by the presence of mutualists (15\% reduction in predation frequency and 45\% in volume of coral consumed). The frequency of attacks with both mutualists was lower than with a single species, but it did not differ significantly from the expected frequency of independent effects. In contrast, the combined defensive efficacy of both mutualist species reduced the volume of coral tissue lost by 73\%, significantly more than the 38\% reduction expected from independent defensive efforts, suggesting the existence of a cooperative synergy in defensive behaviors of ‘guard’ crustaceans. These emergent ‘multiple defender effects’ are statistically and ecologically analogous to the emergent concept of ‘multiple predator effects’ known from the predation literature.},
	number = {4},
	urldate = {2012-10-18},
	journal = {Oecologia},
	author = {McKeon, C. and Stier, Adrian and McIlroy, Shelby and Bolker, Benjamin},
	year = {2012},
	keywords = {Biomedical and Life Sciences},
	pages = {1095--1103},
	file = {SpringerLink Full Text PDF:/home/bolker/Documents/zotero_new/storage/HZRFQ9UN/McKeon et al. - 2012 - Multiple defender effects synergistic coral defen.pdf:application/pdf;SpringerLink Snapshot:/home/bolker/Documents/zotero_new/storage/CN9PWJDG/abstract.html:text/html},
}

@article{vanhoveCollinearity2021,
  title = {Collinearity Isn't a Disease That Needs Curing},
  author = {Vanhove, Jan},
  year = {2021},
  month = apr,
  journal = {Meta-Psychology},
  volume = {5},
  issn = {2003-2714},
  doi = {10.15626/MP.2021.2548},
  urldate = {2023-06-21},
  abstract = {Once they have learnt about the effects of collinearity on the output of multiple regression models, researchers may unduly worry about these and resort to (sometimes dubious) modelling techniques to mitigate them. I argue that, to the extent that problems occur in the presence of collinearity, they are not caused by it but rather by common mental shortcuts that researchers take when interpreting statistical models and that can also lead them astray in the absence of collinearity. Moreover, I illustrate that common strategies for dealing with collinearity only sidestep the perceived problem by biasing parameter estimates, reformulating the model in such a way that it maps onto different research questions, or both. I conclude that collinearity in itself is not a problem and that researchers should be aware of what their approaches for addressing it actually achieve.},
  copyright = {Copyright (c) 2021 Jan Vanhove},
  langid = {english},
  keywords = {interpreting regression models,multiple regression,regression assumptions},
  file = {/home/bolker/Zotero/storage/TKJW5RM6/Vanhove - 2021 - Collinearity isn't a disease that needs curing.pdf}
}

@article{morrisseyMultiple2018a,
  title = {Multiple {{Regression Is Not Multiple Regressions}}: {{The Meaning}} of {{Multiple Regression}} and the {{Non-Problem}} of {{Collinearity}}},
  shorttitle = {Multiple {{Regression Is Not Multiple Regressions}}},
  author = {Morrissey, Michael B. and Ruxton, Graeme D.},
  year = {2018},
  journal = {Philosophy, Theory, and Practice in Biology},
  volume = {10},
  number = {3}
}

@article{dormannCollinearity2012,
  title = {Collinearity: A Review of Methods to Deal with It and a Simulation Study Evaluating Their Performance},
  shorttitle = {Collinearity},
  author = {Dormann, Carsten F. and Elith, Jane and Bacher, Sven and Buchmann, Carsten and Carl, Gudrun and Carr{\'e}, Gabriel and Marqu{\'e}z, Jaime R. Garc{\'i}a and Gruber, Bernd and Lafourcade, Bruno and Leit{\~a}o, Pedro J. and M{\"u}nkem{\"u}ller, Tamara and McClean, Colin and Osborne, Patrick E. and Reineking, Bj{\"o}rn and Schr{\"o}der, Boris and Skidmore, Andrew K. and Zurell, Damaris and Lautenbach, Sven},
  year = {2012},
  journal = {Ecography},
  pages = {no--no},
  issn = {1600-0587},
  doi = {10.1111/j.1600-0587.2012.07348.x},
  urldate = {2012-09-24},
  abstract = {Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the `folk lore'-thresholds of correlation coefficients between predictor variables of |r| {$>$}0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them.},
  copyright = {\textcopyright{} 2012 The Authors},
  langid = {english},
  file = {/home/bolker/Zotero/storage/WIDQWNDX/abstract.html}
}

@article{grahamConfronting2003,
  title = {Confronting {{Multicollinearity}} in {{Ecological Multiple Regression}}},
  author = {Graham, Michael H.},
  year = {2003},
  journal = {Ecology},
  volume = {84},
  number = {11},
  pages = {2809--2815},
  issn = {1939-9170},
  doi = {10.1890/02-3114},
  urldate = {2019-08-18},
  abstract = {The natural complexity of ecological communities regularly lures ecologists to collect elaborate data sets in which confounding factors are often present. Although multiple regression is commonly used in such cases to test the individual effects of many explanatory variables on a continuous response, the inherent collinearity (multicollinearity) of confounded explanatory variables encumbers analyses and threatens their statistical and inferential interpretation. Using numerical simulations, I quantified the impact of multicollinearity on ecological multiple regression and found that even low levels of collinearity bias analyses (r {$\geq$} 0.28 or r2 {$\geq$} 0.08), causing (1) inaccurate model parameterization, (2) decreased statistical power, and (3) exclusion of significant predictor variables during model creation. Then, using real ecological data, I demonstrated the utility of various statistical techniques for enhancing the reliability and interpretation of ecological multiple regression in the presence of multicollinearity.},
  copyright = {\textcopyright{} 2003 by the Ecological Society of America},
  langid = {english},
  keywords = {confounding factors,multicollinearity,multiple regression,principal components regression,sequential regression,structural equation modeling},
  file = {/home/bolker/Zotero/storage/LA74BRQP/Graham - 2003 - Confronting Multicollinearity in Ecological Multip.pdf;/home/bolker/Zotero/storage/A9CGKQES/02-3114.html}
}

@article{gelmanScaling2008,
  title = {Scaling Regression Inputs by Dividing by Two Standard Deviations},
  author = {Gelman, Andrew},
  year = {2008},
  month = jul,
  journal = {Statistics in Medicine},
  volume = {27},
  number = {15},
  pages = {2865--2873},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.3107},
  urldate = {2018-02-04},
  langid = {english},
  file = {/home/bolker/Zotero/storage/HSZXL82A/standardizing7.pdf}
}

@book{hastieGeneralized1990,
  title = {Generalized {{Additive Models}}},
  author = {Hastie, T. J. and Tibshirani, R. J.},
  year = {1990},
  month = jun,
  publisher = {{CRC Press}},
  abstract = {This book describes an array of power tools for data analysis that are based on nonparametric regression and smoothing techniques. These methods relax the linear assumption of many standard models and allow analysts to uncover structure in the data that might otherwise have been missed. While McCullagh and Nelder's Generalized Linear Models shows how to extend the usual linear methodology to cover analysis of a range of data types, Generalized Additive Models enhances this methodology even further by incorporating the flexibility of nonparametric regression. Clear prose, exercises in each chapter, and case studies enhance this popular text.},
  googlebooks = {qa29r1Ze1coC},
  isbn = {978-0-412-34390-2},
  langid = {english},
  keywords = {Computers / Mathematical \& Statistical Software,Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biology}
}

@article{ozgulUpper2009b,
  title = {Upper Respiratory Tract Disease, Force of Infection, and Effects on Survival of Gopher Tortoises},
  author = {Ozgul, Arpat and Oli, Madan K. and Bolker, Benjamin M. and {Perez-Heydrich}, Carolina},
  year = {2009},
  journal = {Ecological Applications},
  volume = {19},
  number = {3},
  pages = {786--798},
  issn = {1939-5582},
  doi = {10.1890/08-0219.1},
  urldate = {2023-10-01},
  abstract = {Upper respiratory tract disease (URTD) caused by Mycoplasma agassizii has been hypothesized to contribute to the decline of some wild populations of gopher tortoises (Gopherus polyphemus). However, the force of infection (FOI) and the effect of URTD on survival in free-ranging tortoise populations remain unknown. Using four years (2003\textendash 2006) of mark\textendash recapture and epidemiological data collected from 10 populations of gopher tortoises in central Florida, USA, we estimated the FOI (probability per year of a susceptible tortoise becoming infected) and the effect of URTD (i.e., seropositivity to M. agassizii) on apparent survival rates. Sites with high ({$\geq$}25\%) seroprevalence had substantially higher FOI (0.22 {$\pm$} 0.03; mean {$\pm$} SE) than low ({$<$}25\%) seroprevalence sites (0.04 {$\pm$} 0.01). Our results provide the first quantitative evidence that the rate of transmission of M. agassizii is directly related to the seroprevalence of the population. Seropositive tortoises had higher apparent survival (0.99 {$\pm$} 0.0001) than seronegatives (0.88 {$\pm$} 0.03), possibly because seropositive tortoises represent individuals that survived the initial infection, developed chronic disease, and experienced lower mortality during the four-year span of our study. However, two lines of evidence suggested possible effects of mycoplasmal URTD on tortoise survival. First, one plausible model suggested that susceptible (seronegative) tortoises in high seroprevalence sites had lower apparent survival rates than did susceptible tortoises in low seroprevalence sites, indicating a possible acute effect of infection. Second, the number of dead tortoise remains detected during annual site surveys increased significantly with increasing site seroprevalence, from {$\sim$}1 to {$\sim$}5 shell remains per 100 individuals. If (as our results suggest) URTD in fact reduces adult survival, it could adversely influence the population dynamics and persistence of this late-maturing, long-lived species.},
  copyright = {\textcopyright{} 2009 by the Ecological Society of America},
  langid = {english},
  keywords = {apparent survival,Florida,force of infection,gopher tortoise,Gopherus polyphemus,multistate mark\textendash recapture models,Mycoplasma agassizii,pathogen transmission,upper respiratory tract disease (URTD),USA,wildlife diseases},
  file = {/home/bolker/Zotero/storage/LQBQXVGD/Ozgul et al. - 2009 - Upper respiratory tract disease, force of infectio.pdf;/home/bolker/Zotero/storage/UCDBWFKJ/08-0219.html}
}

@article{heinzeSolution2002,
  title = {A Solution to the Problem of Separation in Logistic Regression},
  author = {Heinze, Georg and Schemper, Michael},
  year = {2002},
  journal = {Statistics in Medicine},
  volume = {21},
  number = {16},
  pages = {2409--2419},
  issn = {1097-0258},
  doi = {10.1002/sim.1047},
  urldate = {2023-09-29},
  abstract = {The phenomenon of separation or monotone likelihood is observed in the fitting process of a logistic model if the likelihood converges while at least one parameter estimate diverges to {$\pm$} infinity. Separation primarily occurs in small samples with several unbalanced and highly predictive risk factors. A procedure by Firth originally developed to reduce the bias of maximum likelihood estimates is shown to provide an ideal solution to separation. It produces finite parameter estimates by means of penalized maximum likelihood estimation. Corresponding Wald tests and confidence intervals are available but it is shown that penalized likelihood ratio tests and profile penalized likelihood confidence intervals are often preferable. The clear advantage of the procedure over previous options of analysis is impressively demonstrated by the statistical analysis of two cancer studies. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {bias reduction,case-control studies,infinite estimates,monotone likelihood,penalized likelihood,profile likelihood},
  file = {/home/bolker/Zotero/storage/ZXM7TGB3/Heinze and Schemper - 2002 - A solution to the problem of separation in logisti.pdf;/home/bolker/Zotero/storage/5DBQN2DT/sim.html}
}

@ARTICLE{Crowder1978,
  author = {Crowder, M. J.},
  title = {Beta-binomial {Anova} for proportions},
  journal = {Applied Statistics},
  year = {1978},
  volume = {27},
  pages = {34-37}
}

@ARTICLE{Morris1997,
  author = {William F. Morris},
  title = {Disentangling Effects of Induced Plant Defenses and Food Quantity
	on Herbivores by Fitting Nonlinear Models},
  journal = {American Naturalist},
  year = {1997},
  volume = {150},
  pages = {299-327},
  number = {3}
}


@book{bolkerEcological2008,
	address = {Princeton, NJ},
	title = {Ecological {Models} and {Data} in {R}},
	publisher = {Princeton University Press},
	author = {Bolker, Benjamin M.},
	year = {2008},
}

@article{robertsCrossvalidation2017,
  title = {Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure},
  author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and {Guillera-Arroita}, Gurutzeta and Hauenstein, Severin and {Lahoz-Monfort}, Jos{\'e} J. and Schr{\"o}der, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
  year = {2017},
  month = aug,
  journal = {Ecography},
  volume = {40},
  number = {8},
  pages = {913--929},
  issn = {09067590},
  doi = {10.1111/ecog.02881},
  urldate = {2017-09-13},
  langid = {english},
  file = {/home/bolker/Zotero/storage/REZ7BUNJ/ecog2881.pdf}
}

@article{wengerAssessing2012,
  title = {Assessing Transferability of Ecological Models: An Underappreciated Aspect of Statistical Validation},
  shorttitle = {Assessing Transferability of Ecological Models},
  author = {Wenger, Seth J. and Olden, Julian D.},
  year = {2012},
  month = apr,
  journal = {Methods in Ecology and Evolution},
  volume = {3},
  number = {2},
  pages = {260--267},
  issn = {2041210X},
  doi = {10.1111/j.2041-210X.2011.00170.x},
  urldate = {2013-06-29},
  file = {/home/bolker/Zotero/storage/JHTX2G5N/Wenger_Olden_MEE2012.pdf}
}

@article{obenchainClassical1977,
  title = {Classical {{F-Tests}} and {{Confidence Regions}} for {{Ridge Regression}}},
  author = {Obenchain, R. L.},
  year = {1977},
  month = nov,
  journal = {Technometrics},
  volume = {19},
  number = {4},
  pages = {429--439},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1977.10489582},
  urldate = {2023-03-19},
  abstract = {For testing general linear hypotheses in multiple regression models. it is shown that non-stochastically shrunken ridge estimators yield the same central F-ratios and t-statistics as does the least squares estimator. Thus although ridge regression does produce biased point estimates which deviate from the least squares solution, ridge techniques do not generally yield ``new'' normal theory statistical inferences: in particular, ridging does not necessarily produce ``shifted'' confidence regions. A concept, the ASSOCIATFD PROBABILITY of a ridge estimate, is defined using the usual, hyperellipsoidal confidence region centered at the least squares estimator, and it is argued that ridge estimates are of relatively little interest when they are so ``extreme'' that they lie outside of the least squares region of say 90 percent confidence.},
  keywords = {Associated probability of a ridge estimate,F-ratios and t-statistics,Generalized ridge regression,Multicollinearity allowance parameter}
}

@article{barberPredictive2021,
  title = {Predictive Inference with the Jackknife+},
  author = {Barber, Rina Foygel and Cand{\`e}s, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
  year = {2021},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {49},
  number = {1},
  pages = {486--507},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/20-AOS1965},
  urldate = {2023-02-21},
  abstract = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to \$K\$-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk (Ann. Math. Artif. Intell. 74 (2015) 9\textendash 28) and we discuss connections.},
  keywords = {62F40,62G08,62G09,conformal inference,cross-validation,distribution-free,jackknife,leave-one-out,stability},
  file = {/home/bolker/Zotero/storage/7UD6XJM5/Barber et al. - 2021 - Predictive inference with the jackknife+.pdf}
}

@misc{antogniniUnderstanding2021,
  title = {Understanding {{Stein}}'s Paradox},
  author = {Antognini, Joe},
  year = {2021},
  month = jan,
  urldate = {2023-10-21},
  abstract = {An intuitive explanation of the James-Stein estimator.},
  url = {https://joe-antognini.github.io/machine-learning/steins-paradox},
  langid = {english},
  file = {/home/bolker/Zotero/storage/SRTF5PN3/steins-paradox.html}
}

@misc{harrisVisualizing2013,
  title = {Visualizing the {{James-Stein Estimator}}},
  author = {Harris, Naftali},
  year = {2013},
  month = may,
  urldate = {2023-10-21},
  url = {https://www.naftaliharris.com/blog/steinviz/},
  file = {/home/bolker/Zotero/storage/HFIF44AB/steinviz.html}
}

@article{vanhouwelingenShrinkage2001,
  title = {Shrinkage and {{Penalized Likelihood}} as {{Methods}} to {{Improve Predictive Accuracy}}},
  author = {{van Houwelingen}, J. C},
  year = {2001},
  journal = {Statistica Neerlandica},
  volume = {55},
  number = {1},
  pages = {17--34},
  issn = {0039-0402},
  doi = {10.1111/1467-9574.00154},
  urldate = {2018-01-05},
  abstract = {A review is given of shrinkage and penalization as tools to improve predictive accuracy of regression models. The James-Stein estimator is taken as starting point. Procedures covered are Pre-test Estimation, the Ridge Regression of Hoerl and Kennard, the Shrinkage Estimators of Copas and Van Houwelingen and Le Cessie, the LASSO of Tibshirani and the Garotte of Breiman. An attempt is made to place all these procedures in a unifying framework of semi-Bayesian methodology. Applications are briefly mentioned, but not amply discussed.},
  keywords = {Garotte,LASSO,Pre-test Estimation,Ridge Regression},
  file = {/home/bolker/Zotero/storage/DAFUHND7/van Houwelingen - 2001 - Shrinkage and Penalized Likelihood as Methods to I.pdf}
}

@article{batesFitting2015a,
  title = {Fitting {{Linear Mixed-Effects Models Using}} Lme4},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {67},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2023-10-24},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  copyright = {Copyright (c) 2015 Douglas Bates, Martin M\"achler, Ben Bolker, Steve Walker},
  langid = {english},
  keywords = {Cholesky decomposition,linear mixed models,penalized least squares,sparse matrix methods},
  file = {/home/bolker/Zotero/storage/6N5W7R9C/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{pinheiroUnconstrained1996a,
  title = {Unconstrained Parametrizations for Variance-Covariance Matrices},
  author = {Pinheiro, Jos{\'e} C. and Bates, Douglas M.},
  year = {1996},
  journal = {Statistics and Computing},
  volume = {6},
  number = {3},
  pages = {289--296},
  doi = {10.1007/BF00140873},
  urldate = {2010-01-05},
  abstract = {The estimation of variance-covariance matrices through optimization of an objective function, such as a log-likelihood function, is usually a difficult numerical problem. Since the estimates should be positive semi-definite matrices, we must use constrained optimization, or employ a parametrization that enforces this condition. We describe here five different parametrizations for variance-covariance matrices that ensure positive definiteness, thus leaving the estimation problem unconstrained. We compare the parametrizations based on their computational efficiency and statistical interpretability. The results described here are particularly useful in maximum likelihood and restricted maximum likelihood estimation in linear and non-linear mixed-effects models, but are also applicable to other areas of statistics.},
  file = {/home/bolker/Zotero/storage/PPJFZWDT/xl21637258528666.html}
}

@article{burchinalEarly1997,
  title = {Early {{Intervention}} and {{Mediating Processes}} in {{Cognitive Performance}} of {{Children}} of {{Low-Income African American Families}}},
  author = {Burchinal, Margaret R. and Campbell, Frances A. and Bryant, Donna M. and Wasik, Barbara H. and Ramey, Craig T.},
  year = {1997},
  journal = {Child Development},
  volume = {68},
  number = {5},
  eprint = {1132043},
  eprinttype = {jstor},
  pages = {935--954},
  publisher = {{[Wiley, Society for Research in Child Development]}},
  issn = {0009-3920},
  doi = {10.2307/1132043},
  urldate = {2023-10-30},
  abstract = {This longitudinal study of 161 African American children from low-income families examined multiple influences, including early childhood interventions and characteristics of the child and family, on longitudinal patterns of children's cognitive performance measured between 6 months and 8 years of age. Results indicate that more optimal patterns of cognitive development were associated with intensive early educational child care, responsive stimulating care at home, and higher maternal IQ. In accordance with a general systems model, analyses also suggested that child care experiences were related to better cognitive performance in part through enhancing the infant's responsiveness to his or her environment. Maternal IQ had both a direct effect on cognitive performance during early childhood and, also, an indirect effect through its influence on the family environment.},
  file = {/home/bolker/Zotero/storage/UPJTQEKS/Burchinal et al. - 1997 - Early Intervention and Mediating Processes in Cogn.pdf}
}

@book{singerApplied2003,
  title = {Applied {{Longitudinal Data Analysis}}: {{Modeling Change}} and {{Event Occurrence}}},
  shorttitle = {Applied {{Longitudinal Data Analysis}}},
  author = {Singer, Judith D. and Willett, John B.},
  year = {2003},
  month = mar,
  publisher = {{Oxford University Press, USA}},
  abstract = {Change is constant in everyday life. Infants crawl and then walk, children learn to read and write, teenagers mature in myriad ways, the elderly become frail and forgetful. Beyond these natural processes and events, external forces and interventions instigate and disrupt change: test scores may rise after a coaching course, drug abusers may remain abstinent after residential treatment. By charting changes over time and investigating whether and when events occur, researchers reveal the temporal rhythms of our lives. Applied Longitudinal Data Analysis is a much-needed professional book for empirical researchers and graduate students in the behavioral, social, and biomedical sciences. It offers the first accessible in-depth presentation of two of today's most popular statistical methods: multilevel models for individual change and hazard/survival models for event occurrence (in both discrete- and continuous-time). Using clear, concise prose and real data sets from published studies, the authors take you step by step through complete analyses, from simple exploratory displays that reveal underlying patterns through sophisticated specifications of complex statistical models.Applied Longitudinal Data Analysis offers readers a private consultation session with internationally recognized experts and represents a unique contribution to the literature on quantitative empirical methods.Visit http://www.ats.ucla.edu/stat/examples/alda.htm for:DT Downloadable data setsDT Library of computer programs in SAS, SPSS, Stata, HLM, MLwiN, and moreDT Additional material for data analysis},
  googlebooks = {PpnA1M8VwR8C},
  isbn = {978-0-19-515296-8},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Medical / Epidemiology,Psychology / Statistics}
}
