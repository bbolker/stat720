@misc{shamsudheenShould2021,
  title = {Should We Test the Model Assumptions before Running a Model-Based Test?},
  author = {Shamsudheen, M. Iqbal and Hennig, Christian},
  year = {2021},
  month = mar,
  number = {arXiv:1908.02218},
  eprint = {1908.02218},
  primaryclass = {stat},
  institution = {{arXiv}},
  urldate = {2022-05-21},
  abstract = {Statistical methods are based on model assumptions, and it is statistical folklore that a method's model assumptions should be checked before applying it. This can be formally done by running one or more misspecification tests testing model assumptions before running a method that makes these assumptions; here we focus on model-based tests. A combined test procedure can be defined by specifying a protocol in which first model assumptions are tested and then, conditionally on the outcome, a test is run that requires or does not require the tested assumptions. Although such an approach is often taken in practice, much of the literature that investigated this is surprisingly critical of it. Our aim is to explore conditions under which model checking is advisable or not advisable. For this, we review results regarding such "combined procedures" in the literature, we review and discuss controversial views on the role of model checking in statistics, and we present a general setup in which we can show that preliminary model checking is advantageous, which implies conditions for making model checking worthwhile.},
  archiveprefix = {arxiv},
  keywords = {62F03,Statistics - Methodology},
  file = {/home/bolker/Zotero/storage/2WKT8FX9/Shamsudheen and Hennig - 2021 - Should we test the model assumptions before runnin.pdf;/home/bolker/Zotero/storage/V4P9CJQ7/1908.html}
}


@misc{rossFasteR2013,
  title = {{{FasteR}}! {{HigheR}}! {{StrongeR}}! - {{A Guide}} to {{Speeding Up R Code}} for {{Busy People}}},
  author = {Ross, Noam},
  year = {2013},
  month = apr,
  journal = {Noam Ross},
  urldate = {2016-09-02},
  howpublished = {http://www.noamross.net/blog/2013/4/25/faster-talk.html},
  file = {/home/bolker/Zotero/storage/64SSBRVJ/faster-talk.html}
}

@techreport{bryanExcuse2017,
  title = {Excuse Me, Do You Have a Moment to Talk about Version Control?},
  author = {Bryan, Jennifer},
  year = {2017},
  month = aug,
  number = {e3159v2},
  institution = {{PeerJ Inc.}},
  issn = {2167-9843},
  doi = {10.7287/peerj.preprints.3159v2},
  urldate = {2022-10-18},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  langid = {english},
  file = {/home/bolker/Zotero/storage/9YR9ZKL3/Bryan - 2017 - Excuse me, do you have a moment to talk about vers.pdf;/home/bolker/Zotero/storage/M9SGPHLK/3159v2.html}
}

@misc{bryanProjectoriented2017,
  title = {Project-Oriented Workflow},
  author = {Bryan, Jenny},
  year = {2017},
  month = dec,
  journal = {Tidyverse},
  urldate = {2021-01-14},
  abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
  url = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
  langid = {american},
  file = {/home/bolker/Zotero/storage/2UCXRWUV/workflow-vs-script.html}
}

@article{wilsonGood2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2017},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {6},
  pages = {e1005510},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  urldate = {2019-03-16},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  langid = {english},
  keywords = {Computer software,Control systems,Data management,Data processing,Programming languages,Reproducibility,Software tools,Source code},
  file = {/home/bolker/Zotero/storage/7FVVG9HG/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/home/bolker/Zotero/storage/MRE6NA97/article.html}
}


@book{harrellRegression2015,
  title = {Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis},
  shorttitle = {Regression {{Modeling Strategies}}},
  author = {Harrell Jr., Frank E.},
  year = {2015},
  month = aug,
  edition = {2d},
  publisher = {Springer},
  isbn = {978-3-319-19424-0},
  note = {This book presents the best blend I've ever found of practical yet rigorous advice for statistical modeling (testing model assumptions, limiting model complexity in a way that preserves reliable inference, dealing with missing data, etc.). It covers linear and generalized linear regression, logistic regression, ordinal responses, and survival analysis thoroughly; it has a chapter on longitudinal data. It does not go deeply into mixed models, nor does it cover more exotic responses (count or non-binary binomial responses, zero-inflation). It uses the author's own \texttt{rms} package, which is useful but doesn't always fit in perfectly with other frameworks.}
  }

@book{farawayExtending2016,
  title = {Extending the {{Linear Model}} with {{R}}: {{Generalized Linear}}, {{Mixed Effects}} and {{Nonparametric Regression Models}}, {{Second Edition}}},
  shorttitle = {Extending the {{Linear Model}} with {{R}}},
  author = {Faraway, Julian J.},
  year = {2016},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Start Analyzing a Wide Range of Problems  Since the publication of the bestselling, highly recommended first edition, R has considerably expanded both in popularity and in the number of packages available. Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models, Second Edition takes advantage of the greater functionality now available in R and substantially revises and adds several topics. New to the Second Edition  Expanded coverage of binary and binomial responses, including proportion responses, quasibinomial and beta regression, and applied considerations regarding these models  New sections on Poisson models with dispersion, zero inflated count models, linear discriminant analysis, and sandwich and robust estimation for generalized linear models (GLMs)  Revised chapters on random effects and repeated measures that reflect changes in the lme4 package and show how to perform hypothesis testing for the models using other methods New chapter on the Bayesian analysis of mixed effect models that illustrates the use of STAN and presents the approximation method of INLA  Revised chapter on generalized linear mixed models to reflect the much richer choice of fitting software now available Updated coverage of splines and confidence bands in the chapter on nonparametric regression New material on random forests for regression and classification  Revamped R code throughout, particularly the many plots using the ggplot2 package Revised and expanded exercises with solutions now included Demonstrates the Interplay of Theory and Practice This textbook continues to cover a range of techniques that grow from the linear regression model. It presents three extensions to the linear framework: GLMs, mixed effect models, and nonparametric regression models. The book explains data analysis using real examples and includes all the R commands necessary to reproduce the analyses. The level is good for upper-level undergraduate statisticians and beyond, possibly tough for biologists.},
  googlebooks = {XAzYCwAAQBAJ},
  isbn = {978-1-4987-2098-4},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
  note = {A good, broad book that covers most of the material we'll discuss in this class. It starts with a little bit more basic coverage of (G)LMs, but has good discussions of intermediate-level GLM issues (conditional logistic regression, alternative link functions, etc.). It goes on to cover mixed models and additive models, as well as a brief introduction to tree-based models and neural networks (which we'll skip in this class).}
}

@book{gelmanData2006,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, England}},
  note = {Clear, comprehensive discussion of multilevel/hierarchical models. 99\% Bayesian treatment; examples are mostly from social science. Covers linear, logistic, count-data (Poisson/negative binomial) regressions. Causal inference, regression diagnostics, etc.. Reliance on the BUGS language is now slightly old-fashioned (Gelman et al 2020 is an update for the non-mixed model material; these authors and others are working on an updated mixed model book, nominally available \href{http://www.stat.columbia.edu/~gelman/armm/}{around the end of 2024}).}
}

@book{gelmanRegression2020,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year = {2020},
  month = jul,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore}},
  abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
  isbn = {978-1-107-67651-0},
  langid = {english},
  note = {I haven't read this book yet but based on the authors' past work I expect it to be excellent.}
}


@book{hodgesRichly2013,
  title = {Richly Parameterized Linear Models: Additive, Time Series, and Spatial Models Using Random Effects},
  shorttitle = {Richly Parameterized Linear Models},
  author = {Hodges, James S.},
  year = {2013},
  publisher = {{CRC Press}},
  note = {A technical, in-depth exploration of the ways that hierarchical Gaussian linear models can be extended to a huge variety of different kinds of correlation structures and smooth functions. Worked examples of tricky real-world examples; discusses details you won't find anywhere else.}
}

@book{mcelreathStatistical2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {6H\_WDwAAQBAJ},
  isbn = {978-0-429-63914-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
  note = {Bayesian statistics from first principles. Aimed at non-statisticians. Relies on Stan for analysis (the \texttt{rethinking} package provides a nice front end for graphical models). Heavy on mechanistic models and causal inference.}
}

@book{woodGeneralized2017,
  title = {Generalized {{Additive Models}}: {{An Introduction}} with {{R}}},
  author = {Wood, Simon N.},
  year = {2017},
  series = {{{CRC Texts}} in {{Statistical Science}}},
  publisher = {{Chapman \& Hall}},
  urldate = {2017-11-28},
  file = {/home/bolker/Zotero/storage/Q79P94BB/ref=sr_1_1.html},
  note = {Comprehensive coverage of additive models from a penalized-basis point of view, from the master (the author of R's \texttt{mgcv} package). Includes brief reviews of the theory behind linear and GLMs. Technical bits are tough for non-statisticians.}
}


@misc{navarroScience2019,
  title = {Science and Statistics},
  author = {Navarro, Danielle},
  year = {2019},
  month = mar,
  address = {{Aarhus University}},
  abstract = {http://interactingminds.au.dk/events/single-events/artikel/2-day-workshop-open-science-and-reproducibility/},
  langid = {english},
  file = {/home/bolker/Zotero/storage/UX69QA8G/scienceandstatistics.html},
  url = {https://slides.com/djnavarro/scienceandstatistics}
}


@article{boxScience1976b,
	title = {Science and {Statistics}},
	volume = {71},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949},
	doi = {10.1080/01621459.1976.10480949},
	abstract = {Aspects of scientific method are discussed: In particular, its representation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires sufficient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
	number = {356},
	urldate = {2023-09-02},
	journal = {Journal of the American Statistical Association},
	author = {Box, George E. P.},
	month = dec,
	year = {1976},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1976.10480949},
	pages = {791--799}
}


@book{mccullaghGeneralized1989,
	address = {London},
	title = {Generalized {Linear} {Models}},
	publisher = {Chapman \& Hall},
	author = {McCullagh, P. and Nelder, J. A.},
	year = {1989},
}


@article{gelman_statistical_2014,
	title = {The statistical crisis in science: data-dependent analysis--a "garden of forking paths"--explains why many statistically significant comparisons don't hold up},
	volume = {102},
	issn = {0003-0996},
	shorttitle = {The statistical crisis in science},
	url = {http://link.galegroup.com/apps/doc/A389260653/AONE?u=ocul_mcmaster&sid=AONE&xid=4f4562c0},
	language = {English},
	number = {6},
	urldate = {2019-01-07},
	journal = {American Scientist},
	author = {Gelman, Andrew and Loken, Eric},
	year = {2014},
	note = {460},
	keywords = {Periodical publishing, Science journals},
	pages = {460--}
}

@article{simmons_false-positive_2011,
	title = {False-Positive Psychology: Undisclosed Flexibility  in Data Collection and Analysis Allows Presenting Anything as Significant},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/11/1359},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2015-11-08},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	keywords = {disclosure, methodology, motivated reasoning, publication},
	pages = {1359--1366},
	file = {Full Text PDF:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/TNSUZHFS/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility .pdf:application/pdf;Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/BATW66XJ/1359.html:text/html}
}


@article{schielzethSimple2010,
	title = {Simple means to improve the interpretability of regression coefficients: {Interpretation} of regression coefficients},
	volume = {1},
	issn = {2041210X, 2041210X},
	shorttitle = {Simple means to improve the interpretability of regression coefficients},
	url = {http://doi.wiley.com/10.1111/j.2041-210X.2010.00012.x},
	doi = {10.1111/j.2041-210X.2010.00012.x},
	language = {en},
	number = {2},
	urldate = {2016-06-08},
	journal = {Methods in Ecology and Evolution},
	author = {Schielzeth, Holger},
	month = feb,
	year = {2010},
	pages = {103--113},
	file = {j.2041-210X.2010.00012.x.pdf:/home/bolker/Documents/zotero_new/storage/XNG3ZF5X/j.2041-210X.2010.00012.x.pdf:application/pdf},
}

@article{rochon_test_2012,
	title = {To test or not to test: {Preliminary} assessment of normality when comparing two independent samples},
	volume = {12},
	issn = {1471-2288},
	shorttitle = {To test or not to test},
	url = {https://doi.org/10.1186/1471-2288-12-81},
	doi = {10.1186/1471-2288-12-81},
	abstract = {Student’s two-sample t test is generally used for comparing the means of two independent samples, for example, two treatment arms. Under the null hypothesis, the t test assumes that the two samples arise from the same normally distributed population with unknown variance. Adequate control of the Type I error requires that the normality assumption holds, which is often examined by means of a preliminary Shapiro-Wilk test. The following two-stage procedure is widely accepted: If the preliminary test for normality is not significant, the t test is used; if the preliminary test rejects the null hypothesis of normality, a nonparametric test is applied in the main analysis.},
	number = {1},
	urldate = {2020-07-10},
	journal = {BMC Medical Research Methodology},
	author = {Rochon, Justine and Gondan, Matthias and Kieser, Meinhard},
	month = jun,
	year = {2012},
	pages = {81},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/ZY252N86/Rochon et al. - 2012 - To test or not to test Preliminary assessment of .pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/NH5ULAHD/1471-2288-12-81.html:text/html}
}


@article{campbellconsequences2021a,
	title = {The consequences of checking for zero-inflation and overdispersion in the analysis of count data},
	volume = {12},
	copyright = {© 2021 British Ecological Society},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13559},
	doi = {10.1111/2041-210X.13559},
	abstract = {Count data are ubiquitous in ecology and the Poisson generalized linear model (GLM) is commonly used to model the association between counts and explanatory variables of interest. When fitting this model to the data, one typically proceeds by first confirming that the model assumptions are satisfied. If the residuals appear to be overdispersed or if there is zero-inflation, key assumptions of the Poison GLM may be violated and researchers will then typically consider alternatives to the Poison GLM. An important question is whether the potential model selection bias introduced by this data-driven multi-stage procedure merits concern. Here we conduct a large-scale simulation study to investigate the potential consequences of model selection bias that can arise in the simple scenario of analysing a sample of potentially overdispersed, potentially zero-inflated, count data. Specifically, we investigate model selection procedures recently recommended by Blasco-Moreno et al. (2019) using either a series of score tests or information theoretic criteria to select the best model. We find that, when sample sizes are small, model selection based on preliminary score tests (or information theoretic criteria, e.g. AIC, BIC) can lead to potentially substantial inflation of false positive rates (i.e. type 1 error inflation). When sample sizes are sufficiently large, model selection based on preliminary score tests, is not problematic. Ignoring the possibility of overdispersion and zero-inflation during data analyses can lead to invalid inference. However, if one does not have sufficient power to test for overdispersion and zero-inflation, post hoc model selection may also lead to substantial bias. This ‘catch-22’ suggests that, if sample sizes are small, a healthy skepticism is warranted whenever one rejects the null hypothesis of no association between a given outcome and covariate.},
	language = {en},
	number = {4},
	urldate = {2023-09-02},
	journal = {Methods in Ecology and Evolution},
	author = {Campbell, Harlan},
	year = {2021},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13559},
	keywords = {model selection bias, overdispersion, zero-inflated models, zero-inflation},
	pages = {665--680},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/B52K6P2P/Campbell - 2021 - The consequences of checking for zero-inflation an.pdf:application/pdf},
}

@article{campbell_consequences_2014,
	title = {The consequences of proportional hazards based model selection},
	volume = {33},
	copyright = {Copyright © 2013 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6021},
	doi = {10.1002/sim.6021},
	abstract = {For testing the efficacy of a treatment in a clinical trial with survival data, the Cox proportional hazards (PH) model is the well-accepted, conventional tool. When using this model, one typically proceeds by confirming that the required PH assumption holds true. If the PH assumption fails to hold, there are many options available, proposed as alternatives to the Cox PH model. An important question which arises is whether the potential bias introduced by this sequential model fitting procedure merits concern and, if so, what are effective mechanisms for correction. We investigate by means of simulation study and draw attention to the considerable drawbacks, with regard to power, of a simple resampling technique, the permutation adjustment, a natural recourse for addressing such challenges. We also consider a recently proposed two-stage testing strategy (2008) for ameliorating these effects. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2020-07-10},
	journal = {Statistics in Medicine},
	author = {Campbell, H. and Dean, C. B.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6021},
	keywords = {model selection bias, proportional hazards, two-stage approach},
	pages = {1042--1056}
}


@article{box_non-normality_1953,
	title = {Non-{Normality} and {Tests} on {Variances}},
	volume = {40},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2333350},
	doi = {10.2307/2333350},
	number = {3/4},
	urldate = {2020-07-11},
	journal = {Biometrika},
	author = {Box, G. E. P.},
	year = {1953},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {318--335}
}


@article{zimmermannote2004,
	title = {A note on preliminary tests of equality of variances},
	volume = {57},
	copyright = {2004 The British Psychological Society},
	issn = {2044-8317},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/000711004849222},
	doi = {10.1348/000711004849222},
	abstract = {Preliminary tests of equality of variances used before a test of location are no longer widely recommended by statisticians, although they persist in some textbooks and software packages. The present study extends the findings of previous studies and provides further reasons for discontinuing the use of preliminary tests. The study found Type I error rates of a two-stage procedure, consisting of a preliminary Levene test on samples of different sizes with unequal variances, followed by either a Student pooled-variances t test or a Welch separate-variances t test. Simulations disclosed that the twostage procedure fails to protect the significance level and usually makes the situation worse. Earlier studies have shown that preliminary tests often adversely affect the size of the test, and also that the Welch test is superior to the t test when variances are unequal. The present simulations reveal that changes in Type I error rates are greater when sample sizes are smaller, when the difference in variances is slight rather than extreme, and when the significance level is more stringent. Furthermore, the validity of the Welch test deteriorates if it is used only on those occasions where a preliminary test indicates it is needed. Optimum protection is assured by using a separate-variances test unconditionally whenever sample sizes are unequal.},
	language = {en},
	number = {1},
	urldate = {2020-10-01},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Zimmerman, Donald W.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000711004849222},
	pages = {173--181},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/W8RPHCGT/Zimmerman - 2004 - A note on preliminary tests of equality of varianc.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/2S6W6AZG/000711004849222.html:text/html},
}


@article{mengStatistical2018,
	title = {Statistical paradises and paradoxes in big data ({I}): {Law} of large populations, big data paradox, and the 2016 {US} presidential election},
	volume = {12},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Statistical paradises and paradoxes in big data ({I})},
	url = {https://projecteuclid.org/euclid.aoas/1532743473},
	doi = {10.1214/18-AOAS1161SF},
	abstract = {Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualifications for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: “Which one should I trust more: a 1\% survey with 60\% response rate or a self-reported administrative dataset covering 80\% of the population?” A 5-element Euler-formula-like identity shows that for any dataset of size nnn, probabilistic or not, the difference between the sample average X¯¯¯¯nX¯n{\textbackslash}overline\{X\}\_\{n\} and the population average X¯¯¯¯NX¯N{\textbackslash}overline\{X\}\_\{N\} is the product of three terms: (1) a data quality measure, ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\}, the correlation between XjXjX\_\{j\} and the response/recording indicator RjRjR\_\{j\}; (2) a data quantity measure, (N−n)/n−−−−−−−−−√(N−n)/n{\textbackslash}sqrt\{(N-n)/n\}, where NNN is the population size; and (3) a problem difficulty measure, σXσX{\textbackslash}sigma\_\{X\}, the standard deviation of XXX. This decomposition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\} at the level of N−1/2N−1/2N{\textasciicircum}\{-1/2\}; (II) When we lose this control, the impact of NNN is no longer canceled by ρR,XρR,X{\textbackslash}rho\_\{\{R,X\}\}, leading to a Law of Large Populations (LLP), that is, our estimation error, relative to the benchmarking rate 1/n−−√1/n1/{\textbackslash}sqrt\{n\}, increases with N−−√N{\textbackslash}sqrt\{N\}; and (III) the “bigness” of such Big Data (for population inferences) should be measured by the relative size f=n/Nf=n/Nf=n/N, not the absolute size nnn; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by their sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a ρR,X≈−0.005ρR,X≈−0.005{\textbackslash}rho\_\{\{R,X\}\}{\textbackslash}approx-0.005 for self-reporting to vote for Donald Trump. Because of LLP, this seemingly minuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from 1\%1\%1{\textbackslash}\% of the US eligible voters, that is, n≈2,300,000n≈2,300,000n{\textbackslash}approx2{\textbackslash}mbox\{,\}300{\textbackslash}mbox\{,\}000, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size n≈400n≈400n{\textbackslash}approx400, a 99.98\%99.98\%99.98{\textbackslash}\% reduction of sample size (and hence our confidence). The CCES data demonstrate LLP vividly: on average, the larger the state’s voter populations, the further away the actual Trump vote shares from the usual 95\%95\%95{\textbackslash}\% confidence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves.},
	language = {EN},
	number = {2},
	urldate = {2020-08-01},
	journal = {Annals of Applied Statistics},
	author = {Meng, Xiao-Li},
	month = jun,
	year = {2018},
	mrnumber = {MR3834282},
	zmnumber = {06980472},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bias-variance tradeoff, data confidentiality and privacy, data defect correlation, data defect index (d.d.i.), data quality-quantity tradeoff, Euler identity, Monte Carlo and Quasi Monte Carlo (MCQMC), non-response bias},
	pages = {685--726},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/NGH7IJCE/1532743473.html:text/html;Submitted Version:/home/bolker/Documents/zotero_new/storage/MNETFS2E/Meng - 2018 - Statistical paradises and paradoxes in big data (I.pdf:application/pdf},
}



@article{oksanenLogic2001,
	title = {Logic of {Experiments} in {Ecology}: {Is} {Pseudoreplication} a {Pseudoissue}?},
	volume = {94},
	issn = {0030-1299},
	shorttitle = {Logic of {Experiments} in {Ecology}},
	url = {http://www.jstor.org/stable/3547252},
	abstract = {Hurlbert divides experimental ecologist into 'those who do not see any need for dispersion (of replicated treatments and controls), and those who do recognize its importance and take whatever measures are necessary to achieve a good dose of it'. Experimental ecologists could also be divided into those who do not see any problems with sacrificing spatial and temporal scales in order to obtain replication, and those who understand that appropriate scale must always have priority over replication. If an experiment is conducted in a spatial or temporal scale, where the predictions of contesting hypotheses are convergent or ambiguous, no amount of technical impeccability can make the work instructive. Conversely, replication can always be obtained afterwards, by conducting more experiments with basically similar design in different areas and by using meta-analysis. This approach even reduces the sampling bias obtained if resources are allocated to a small number of well-replicated experiments. For a strict advocate of the hypothetico-deductive method, replication is unnecessary even as a matter of principle, unless the predicted response is so weak that random background noise is a plausible excuse for a discrepancy between predictions and results. By definition, a prediction is an 'all-statement', referring to all systems within a well-defined category. What applies to all must apply to any. Hence, choosing two systems and assigning them randomly to a treatment and a control is normally an adequate design for a deductive experiment. The strength of such experiments depends on the firmness of the predictions and their a priori probability of corroboration. Replication is but one of many ways of reducing this probability. Whether the experiment is replicated or not, inferential statistics should always be used, to enable the reader to judge how well the apparent patterns in samples reflect real patterns in statistical populations. The concept 'pseudoreplication' amounts to entirely unwarranted stigmatization of a reasonable way to test predictions referring to large-scale systems.},
	number = {1},
	urldate = {2016-12-31},
	journal = {Oikos},
	author = {Oksanen, Lauri},
	year = {2001},
	pages = {27--38},
	file = {JSTOR Full Text PDF:/home/bolker/Documents/zotero_new/storage/PNS94Z3Q/Oksanen - 2001 - Logic of Experiments in Ecology Is Pseudoreplicat.pdf:application/pdf},
}

@article{hurlbertPseudoreplication1984,
	title = {Pseudoreplication and the {Design} of {Ecological} {Field} {Experiments}},
	volume = {54},
	issn = {0012-9615},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.2307/1942661},
	doi = {10.2307/1942661},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27\% of them, or 48\% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing pre?layout (or conventional) and layout?specific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved.},
	number = {2},
	urldate = {2019-03-23},
	journal = {Ecological Monographs},
	author = {Hurlbert, Stuart H.},
	month = feb,
	year = {1984},
	pages = {187--211},
}



@article{wilkinsonSymbolic1973a,
	title = {Symbolic {Description} of {Factorial} {Models} for {Analysis} of {Variance}},
	volume = {22},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2346786},
	doi = {10.2307/2346786},
	abstract = {The paper describes the symbolic notation and syntax for specifying factorial models for analysis of variance in the control language of the GENSTAT statistical program system at Rothamsted. The notation generalizes that of Nelder (1965). Algorithm AS 65 (Rogers, 1973) converts factorial model formulae in this notation to a list of model terms represented as binary integers. A further extension of the syntax is discussed for specifying models generally (including non-linear forms).},
	number = {3},
	urldate = {2011-01-21},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Wilkinson, G. N. and Rogers, C. E.},
	month = jan,
	year = {1973},
	note = {ArticleType: research-article / Full publication date: 1973 / Copyright © 1973 Royal Statistical Society},
	pages = {392--399},
	file = {JSTOR Full Text PDF:/home/bolker/Documents/zotero_new/storage/JRWFNFZT/Wilkinson and Rogers - 1973 - Symbolic Description of Factorial Models for Analy.pdf:application/pdf},
}


@book{chambersStatistical1991,
	edition = {1},
	title = {Statistical {Models} in {S}},
	isbn = {0-412-83040-X},
	publisher = {Chapman \& Hall/CRC},
	editor = {Chambers, J. M. and Hastie, T. J.},
	month = oct,
	year = {1991},
}

@book{venablesModern2002a,
  title = {Modern Applied Statistics with {S}},
  author = {Venables, W. and Ripley, Brian D.},
  year = {2002},
  edition = {4th},
  publisher = {Springer},
  address = {New York}
}

@inproceedings{venablesExegeses1998,
	address = {Washington, DC},
	series = {1998 {International} {S}-{PLUS} {User} {Conference}},
	title = {Exegeses on {Linear} {Models}},
	url = {http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf},
	author = {Venables, W. N},
	year = {1998},
}



@article{dushoffcan2018,
	title = {I can see clearly now: reinterpreting statistical significance},
	shorttitle = {I can see clearly now},
	url = {https://arxiv.org/abs/1810.06387},
	language = {en},
	urldate = {2018-11-14},
	author = {Dushoff, Jonathan and Kain, Morgan P. and Bolker, Benjamin M.},
	month = oct,
	year = {2018},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/8T5WJIZI/Dushoff et al. - 2018 - I can see clearly now reinterpreting statistical .pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/IHD9I8RZ/1810.html:text/html},
}


@misc{hennigTesting2022,
	address = {University of Bologna},
	title = {Testing in models that are not true},
	url = {https://www.wu.ac.at/fileadmin/wu/d/i/statmath/Research_Seminar/SS_2022/2022_04_Hennig.pdf},
	language = {en},
	author = {Hennig, Christian},
	year = {2022},
	file = {Hennig - Testing in models that are not true.pdf:/home/bolker/Documents/zotero_new/storage/IAY8FNB5/Hennig - Testing in models that are not true.pdf:application/pdf},
}



@article{schoenerNonsynchronous1970,
	title = {Nonsynchronous {Spatial} {Overlap} of {Lizards} in {Patchy} {Habitats}},
	volume = {51},
	issn = {0012-9658},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.2307/1935376},
	doi = {10.2307/1935376},
	abstract = {Sympatric native Anolis species with similar structural habitats but contrasting climatic habitats are closer in head and body size on species?rich than on depauperate islands. In two localities, sympatric Anolis species with differential occurrences in sun or shade sought lower, more shaded perches during midday, resulting in partly nonsynchronous utilization of the vegetation by the two species. The second observation may be related to the first in the following way: nonsynchronous spatial overlap could dictate relatively great resource overlap for species coinhabiting patchy or edge areas, requiring great differences between the species in prey size in addition to those in climatic habitat. The extent of such overlap on small depauperate islands could be greater if these contained a greater proportion of patchy or edge habitats (with respect to insolation), or if climatic preferences were broader and more overlapping than on large, species?rich islands. In each locality, the relatively more shade?inhabiting species occurred more often on larger perches and on lower perches than did the other species. In both species of the Bermudan pair, adult males occupied higher and larger perches, and in grahami, shadier perches, than did female?sized individuals. The statistical significance of these and other differences was evaluated using several unweighted g2 procedures, Cochran's weighted g2 test and a partitioning technique for analyzing interactions among variables in complex contingency tables. The last method is described in detail in the papaer by Fienberg, immediately following this one.},
	number = {3},
	urldate = {2018-04-21},
	journal = {Ecology},
	author = {Schoener, Thomas W.},
	month = may,
	year = {1970},
	pages = {408--418},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/Z25TPT39/Schoener Thomas W. - 1970 - Nonsynchronous Spatial Overlap of Lizards in Patch.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/JUIRP249/1935376.html:text/html},
}


@article{mckeonMultiple2012b,
	title = {Multiple defender effects: synergistic coral defense by mutualist crustaceans},
	volume = {169},
	issn = {0029-8549},
	shorttitle = {Multiple defender effects},
	url = {http://www.springerlink.com/content/nm20758r6557v448/abstract/},
	doi = {10.1007/s00442-012-2275-2},
	abstract = {The majority of our understanding of mutualisms comes from studies of pairwise interactions. However, many hosts support mutualist guilds, and interactions among mutualists make the prediction of aggregate effects difficult. Here, we apply a factorial experiment to interactions of ‘guard’ crustaceans that defend their coral host from seastar predators. Predation was reduced by the presence of mutualists (15\% reduction in predation frequency and 45\% in volume of coral consumed). The frequency of attacks with both mutualists was lower than with a single species, but it did not differ significantly from the expected frequency of independent effects. In contrast, the combined defensive efficacy of both mutualist species reduced the volume of coral tissue lost by 73\%, significantly more than the 38\% reduction expected from independent defensive efforts, suggesting the existence of a cooperative synergy in defensive behaviors of ‘guard’ crustaceans. These emergent ‘multiple defender effects’ are statistically and ecologically analogous to the emergent concept of ‘multiple predator effects’ known from the predation literature.},
	number = {4},
	urldate = {2012-10-18},
	journal = {Oecologia},
	author = {McKeon, C. and Stier, Adrian and McIlroy, Shelby and Bolker, Benjamin},
	year = {2012},
	keywords = {Biomedical and Life Sciences},
	pages = {1095--1103},
	file = {SpringerLink Full Text PDF:/home/bolker/Documents/zotero_new/storage/HZRFQ9UN/McKeon et al. - 2012 - Multiple defender effects synergistic coral defen.pdf:application/pdf;SpringerLink Snapshot:/home/bolker/Documents/zotero_new/storage/CN9PWJDG/abstract.html:text/html},
}

@article{vanhoveCollinearity2021,
  title = {Collinearity Isn't a Disease That Needs Curing},
  author = {Vanhove, Jan},
  year = {2021},
  month = apr,
  journal = {Meta-Psychology},
  volume = {5},
  issn = {2003-2714},
  doi = {10.15626/MP.2021.2548},
  urldate = {2023-06-21},
  abstract = {Once they have learnt about the effects of collinearity on the output of multiple regression models, researchers may unduly worry about these and resort to (sometimes dubious) modelling techniques to mitigate them. I argue that, to the extent that problems occur in the presence of collinearity, they are not caused by it but rather by common mental shortcuts that researchers take when interpreting statistical models and that can also lead them astray in the absence of collinearity. Moreover, I illustrate that common strategies for dealing with collinearity only sidestep the perceived problem by biasing parameter estimates, reformulating the model in such a way that it maps onto different research questions, or both. I conclude that collinearity in itself is not a problem and that researchers should be aware of what their approaches for addressing it actually achieve.},
  copyright = {Copyright (c) 2021 Jan Vanhove},
  langid = {english},
  keywords = {interpreting regression models,multiple regression,regression assumptions},
  file = {/home/bolker/Zotero/storage/TKJW5RM6/Vanhove - 2021 - Collinearity isn't a disease that needs curing.pdf}
}

@article{morrisseyMultiple2018a,
  title = {Multiple {{Regression Is Not Multiple Regressions}}: {{The Meaning}} of {{Multiple Regression}} and the {{Non-Problem}} of {{Collinearity}}},
  shorttitle = {Multiple {{Regression Is Not Multiple Regressions}}},
  author = {Morrissey, Michael B. and Ruxton, Graeme D.},
  year = {2018},
  journal = {Philosophy, Theory, and Practice in Biology},
  volume = {10},
  number = {3}
}

@article{dormannCollinearity2012,
  title = {Collinearity: A Review of Methods to Deal with It and a Simulation Study Evaluating Their Performance},
  shorttitle = {Collinearity},
  author = {Dormann, Carsten F. and Elith, Jane and Bacher, Sven and Buchmann, Carsten and Carl, Gudrun and Carr{\'e}, Gabriel and Marqu{\'e}z, Jaime R. Garc{\'i}a and Gruber, Bernd and Lafourcade, Bruno and Leit{\~a}o, Pedro J. and M{\"u}nkem{\"u}ller, Tamara and McClean, Colin and Osborne, Patrick E. and Reineking, Bj{\"o}rn and Schr{\"o}der, Boris and Skidmore, Andrew K. and Zurell, Damaris and Lautenbach, Sven},
  year = {2012},
  journal = {Ecography},
  pages = {no--no},
  issn = {1600-0587},
  doi = {10.1111/j.1600-0587.2012.07348.x},
  urldate = {2012-09-24},
  abstract = {Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the `folk lore'-thresholds of correlation coefficients between predictor variables of |r| {$>$}0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them.},
  copyright = {\textcopyright{} 2012 The Authors},
  langid = {english},
  file = {/home/bolker/Zotero/storage/WIDQWNDX/abstract.html}
}

@article{grahamConfronting2003,
  title = {Confronting {{Multicollinearity}} in {{Ecological Multiple Regression}}},
  author = {Graham, Michael H.},
  year = {2003},
  journal = {Ecology},
  volume = {84},
  number = {11},
  pages = {2809--2815},
  issn = {1939-9170},
  doi = {10.1890/02-3114},
  urldate = {2019-08-18},
  abstract = {The natural complexity of ecological communities regularly lures ecologists to collect elaborate data sets in which confounding factors are often present. Although multiple regression is commonly used in such cases to test the individual effects of many explanatory variables on a continuous response, the inherent collinearity (multicollinearity) of confounded explanatory variables encumbers analyses and threatens their statistical and inferential interpretation. Using numerical simulations, I quantified the impact of multicollinearity on ecological multiple regression and found that even low levels of collinearity bias analyses (r {$\geq$} 0.28 or r2 {$\geq$} 0.08), causing (1) inaccurate model parameterization, (2) decreased statistical power, and (3) exclusion of significant predictor variables during model creation. Then, using real ecological data, I demonstrated the utility of various statistical techniques for enhancing the reliability and interpretation of ecological multiple regression in the presence of multicollinearity.},
  copyright = {\textcopyright{} 2003 by the Ecological Society of America},
  langid = {english},
  keywords = {confounding factors,multicollinearity,multiple regression,principal components regression,sequential regression,structural equation modeling},
  file = {/home/bolker/Zotero/storage/LA74BRQP/Graham - 2003 - Confronting Multicollinearity in Ecological Multip.pdf;/home/bolker/Zotero/storage/A9CGKQES/02-3114.html}
}

@article{gelmanScaling2008,
  title = {Scaling Regression Inputs by Dividing by Two Standard Deviations},
  author = {Gelman, Andrew},
  year = {2008},
  month = jul,
  journal = {Statistics in Medicine},
  volume = {27},
  number = {15},
  pages = {2865--2873},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.3107},
  urldate = {2018-02-04},
  langid = {english},
  file = {/home/bolker/Zotero/storage/HSZXL82A/standardizing7.pdf}
}

@book{hastieGeneralized1990,
  title = {Generalized {{Additive Models}}},
  author = {Hastie, T. J. and Tibshirani, R. J.},
  year = {1990},
  month = jun,
  publisher = {{CRC Press}},
  abstract = {This book describes an array of power tools for data analysis that are based on nonparametric regression and smoothing techniques. These methods relax the linear assumption of many standard models and allow analysts to uncover structure in the data that might otherwise have been missed. While McCullagh and Nelder's Generalized Linear Models shows how to extend the usual linear methodology to cover analysis of a range of data types, Generalized Additive Models enhances this methodology even further by incorporating the flexibility of nonparametric regression. Clear prose, exercises in each chapter, and case studies enhance this popular text.},
  googlebooks = {qa29r1Ze1coC},
  isbn = {978-0-412-34390-2},
  langid = {english},
  keywords = {Computers / Mathematical \& Statistical Software,Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biology}
}

@article{ozgulUpper2009b,
  title = {Upper Respiratory Tract Disease, Force of Infection, and Effects on Survival of Gopher Tortoises},
  author = {Ozgul, Arpat and Oli, Madan K. and Bolker, Benjamin M. and {Perez-Heydrich}, Carolina},
  year = {2009},
  journal = {Ecological Applications},
  volume = {19},
  number = {3},
  pages = {786--798},
  issn = {1939-5582},
  doi = {10.1890/08-0219.1},
  urldate = {2023-10-01},
  abstract = {Upper respiratory tract disease (URTD) caused by Mycoplasma agassizii has been hypothesized to contribute to the decline of some wild populations of gopher tortoises (Gopherus polyphemus). However, the force of infection (FOI) and the effect of URTD on survival in free-ranging tortoise populations remain unknown. Using four years (2003\textendash 2006) of mark\textendash recapture and epidemiological data collected from 10 populations of gopher tortoises in central Florida, USA, we estimated the FOI (probability per year of a susceptible tortoise becoming infected) and the effect of URTD (i.e., seropositivity to M. agassizii) on apparent survival rates. Sites with high ({$\geq$}25\%) seroprevalence had substantially higher FOI (0.22 {$\pm$} 0.03; mean {$\pm$} SE) than low ({$<$}25\%) seroprevalence sites (0.04 {$\pm$} 0.01). Our results provide the first quantitative evidence that the rate of transmission of M. agassizii is directly related to the seroprevalence of the population. Seropositive tortoises had higher apparent survival (0.99 {$\pm$} 0.0001) than seronegatives (0.88 {$\pm$} 0.03), possibly because seropositive tortoises represent individuals that survived the initial infection, developed chronic disease, and experienced lower mortality during the four-year span of our study. However, two lines of evidence suggested possible effects of mycoplasmal URTD on tortoise survival. First, one plausible model suggested that susceptible (seronegative) tortoises in high seroprevalence sites had lower apparent survival rates than did susceptible tortoises in low seroprevalence sites, indicating a possible acute effect of infection. Second, the number of dead tortoise remains detected during annual site surveys increased significantly with increasing site seroprevalence, from {$\sim$}1 to {$\sim$}5 shell remains per 100 individuals. If (as our results suggest) URTD in fact reduces adult survival, it could adversely influence the population dynamics and persistence of this late-maturing, long-lived species.},
  copyright = {\textcopyright{} 2009 by the Ecological Society of America},
  langid = {english},
  keywords = {apparent survival,Florida,force of infection,gopher tortoise,Gopherus polyphemus,multistate mark\textendash recapture models,Mycoplasma agassizii,pathogen transmission,upper respiratory tract disease (URTD),USA,wildlife diseases},
  file = {/home/bolker/Zotero/storage/LQBQXVGD/Ozgul et al. - 2009 - Upper respiratory tract disease, force of infectio.pdf;/home/bolker/Zotero/storage/UCDBWFKJ/08-0219.html}
}

@article{heinzeSolution2002,
  title = {A Solution to the Problem of Separation in Logistic Regression},
  author = {Heinze, Georg and Schemper, Michael},
  year = {2002},
  journal = {Statistics in Medicine},
  volume = {21},
  number = {16},
  pages = {2409--2419},
  issn = {1097-0258},
  doi = {10.1002/sim.1047},
  urldate = {2023-09-29},
  abstract = {The phenomenon of separation or monotone likelihood is observed in the fitting process of a logistic model if the likelihood converges while at least one parameter estimate diverges to {$\pm$} infinity. Separation primarily occurs in small samples with several unbalanced and highly predictive risk factors. A procedure by Firth originally developed to reduce the bias of maximum likelihood estimates is shown to provide an ideal solution to separation. It produces finite parameter estimates by means of penalized maximum likelihood estimation. Corresponding Wald tests and confidence intervals are available but it is shown that penalized likelihood ratio tests and profile penalized likelihood confidence intervals are often preferable. The clear advantage of the procedure over previous options of analysis is impressively demonstrated by the statistical analysis of two cancer studies. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {bias reduction,case-control studies,infinite estimates,monotone likelihood,penalized likelihood,profile likelihood},
  file = {/home/bolker/Zotero/storage/ZXM7TGB3/Heinze and Schemper - 2002 - A solution to the problem of separation in logisti.pdf;/home/bolker/Zotero/storage/5DBQN2DT/sim.html}
}

@ARTICLE{Crowder1978,
  author = {Crowder, M. J.},
  title = {Beta-binomial {Anova} for proportions},
  journal = {Applied Statistics},
  year = {1978},
  volume = {27},
  pages = {34-37}
}

@ARTICLE{Morris1997,
  author = {William F. Morris},
  title = {Disentangling Effects of Induced Plant Defenses and Food Quantity
	on Herbivores by Fitting Nonlinear Models},
  journal = {American Naturalist},
  year = {1997},
  volume = {150},
  pages = {299-327},
  number = {3}
}


@book{bolkerEcological2008,
	address = {Princeton, NJ},
	title = {Ecological {Models} and {Data} in {R}},
	publisher = {Princeton University Press},
	author = {Bolker, Benjamin M.},
	year = {2008},
}

@article{robertsCrossvalidation2017,
  title = {Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure},
  author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and {Guillera-Arroita}, Gurutzeta and Hauenstein, Severin and {Lahoz-Monfort}, Jos{\'e} J. and Schr{\"o}der, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
  year = {2017},
  month = aug,
  journal = {Ecography},
  volume = {40},
  number = {8},
  pages = {913--929},
  issn = {09067590},
  doi = {10.1111/ecog.02881},
  urldate = {2017-09-13},
  langid = {english},
  file = {/home/bolker/Zotero/storage/REZ7BUNJ/ecog2881.pdf}
}

@article{wengerAssessing2012,
  title = {Assessing Transferability of Ecological Models: An Underappreciated Aspect of Statistical Validation},
  shorttitle = {Assessing Transferability of Ecological Models},
  author = {Wenger, Seth J. and Olden, Julian D.},
  year = {2012},
  month = apr,
  journal = {Methods in Ecology and Evolution},
  volume = {3},
  number = {2},
  pages = {260--267},
  issn = {2041210X},
  doi = {10.1111/j.2041-210X.2011.00170.x},
  urldate = {2013-06-29},
  file = {/home/bolker/Zotero/storage/JHTX2G5N/Wenger_Olden_MEE2012.pdf}
}

@article{obenchainClassical1977,
  title = {Classical {{F-Tests}} and {{Confidence Regions}} for {{Ridge Regression}}},
  author = {Obenchain, R. L.},
  year = {1977},
  month = nov,
  journal = {Technometrics},
  volume = {19},
  number = {4},
  pages = {429--439},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1977.10489582},
  urldate = {2023-03-19},
  abstract = {For testing general linear hypotheses in multiple regression models. it is shown that non-stochastically shrunken ridge estimators yield the same central F-ratios and t-statistics as does the least squares estimator. Thus although ridge regression does produce biased point estimates which deviate from the least squares solution, ridge techniques do not generally yield ``new'' normal theory statistical inferences: in particular, ridging does not necessarily produce ``shifted'' confidence regions. A concept, the ASSOCIATFD PROBABILITY of a ridge estimate, is defined using the usual, hyperellipsoidal confidence region centered at the least squares estimator, and it is argued that ridge estimates are of relatively little interest when they are so ``extreme'' that they lie outside of the least squares region of say 90 percent confidence.},
  keywords = {Associated probability of a ridge estimate,F-ratios and t-statistics,Generalized ridge regression,Multicollinearity allowance parameter}
}

@article{barberPredictive2021,
  title = {Predictive Inference with the Jackknife+},
  author = {Barber, Rina Foygel and Cand{\`e}s, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
  year = {2021},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {49},
  number = {1},
  pages = {486--507},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/20-AOS1965},
  urldate = {2023-02-21},
  abstract = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to \$K\$-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk (Ann. Math. Artif. Intell. 74 (2015) 9\textendash 28) and we discuss connections.},
  keywords = {62F40,62G08,62G09,conformal inference,cross-validation,distribution-free,jackknife,leave-one-out,stability},
  file = {/home/bolker/Zotero/storage/7UD6XJM5/Barber et al. - 2021 - Predictive inference with the jackknife+.pdf}
}

@misc{antogniniUnderstanding2021,
  title = {Understanding {{Stein}}'s Paradox},
  author = {Antognini, Joe},
  year = {2021},
  month = jan,
  urldate = {2023-10-21},
  abstract = {An intuitive explanation of the James-Stein estimator.},
  url = {https://joe-antognini.github.io/machine-learning/steins-paradox},
  langid = {english},
  file = {/home/bolker/Zotero/storage/SRTF5PN3/steins-paradox.html}
}

@misc{harrisVisualizing2013,
  title = {Visualizing the {{James-Stein Estimator}}},
  author = {Harris, Naftali},
  year = {2013},
  month = may,
  urldate = {2023-10-21},
  url = {https://www.naftaliharris.com/blog/steinviz/},
  file = {/home/bolker/Zotero/storage/HFIF44AB/steinviz.html}
}

@article{vanhouwelingenShrinkage2001,
  title = {Shrinkage and {{Penalized Likelihood}} as {{Methods}} to {{Improve Predictive Accuracy}}},
  author = {{van Houwelingen}, J. C},
  year = {2001},
  journal = {Statistica Neerlandica},
  volume = {55},
  number = {1},
  pages = {17--34},
  issn = {0039-0402},
  doi = {10.1111/1467-9574.00154},
  urldate = {2018-01-05},
  abstract = {A review is given of shrinkage and penalization as tools to improve predictive accuracy of regression models. The James-Stein estimator is taken as starting point. Procedures covered are Pre-test Estimation, the Ridge Regression of Hoerl and Kennard, the Shrinkage Estimators of Copas and Van Houwelingen and Le Cessie, the LASSO of Tibshirani and the Garotte of Breiman. An attempt is made to place all these procedures in a unifying framework of semi-Bayesian methodology. Applications are briefly mentioned, but not amply discussed.},
  keywords = {Garotte,LASSO,Pre-test Estimation,Ridge Regression},
  file = {/home/bolker/Zotero/storage/DAFUHND7/van Houwelingen - 2001 - Shrinkage and Penalized Likelihood as Methods to I.pdf}
}

@article{batesFitting2015a,
  title = {Fitting {{Linear Mixed-Effects Models Using}} Lme4},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {67},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2023-10-24},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  copyright = {Copyright (c) 2015 Douglas Bates, Martin M\"achler, Ben Bolker, Steve Walker},
  langid = {english},
  keywords = {Cholesky decomposition,linear mixed models,penalized least squares,sparse matrix methods},
  file = {/home/bolker/Zotero/storage/6N5W7R9C/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{pinheiroUnconstrained1996a,
  title = {Unconstrained Parametrizations for Variance-Covariance Matrices},
  author = {Pinheiro, Jos{\'e} C. and Bates, Douglas M.},
  year = {1996},
  journal = {Statistics and Computing},
  volume = {6},
  number = {3},
  pages = {289--296},
  doi = {10.1007/BF00140873},
  urldate = {2010-01-05},
  abstract = {The estimation of variance-covariance matrices through optimization of an objective function, such as a log-likelihood function, is usually a difficult numerical problem. Since the estimates should be positive semi-definite matrices, we must use constrained optimization, or employ a parametrization that enforces this condition. We describe here five different parametrizations for variance-covariance matrices that ensure positive definiteness, thus leaving the estimation problem unconstrained. We compare the parametrizations based on their computational efficiency and statistical interpretability. The results described here are particularly useful in maximum likelihood and restricted maximum likelihood estimation in linear and non-linear mixed-effects models, but are also applicable to other areas of statistics.},
  file = {/home/bolker/Zotero/storage/PPJFZWDT/xl21637258528666.html}
}

@article{burchinalEarly1997,
  title = {Early {{Intervention}} and {{Mediating Processes}} in {{Cognitive Performance}} of {{Children}} of {{Low-Income African American Families}}},
  author = {Burchinal, Margaret R. and Campbell, Frances A. and Bryant, Donna M. and Wasik, Barbara H. and Ramey, Craig T.},
  year = {1997},
  journal = {Child Development},
  volume = {68},
  number = {5},
  eprint = {1132043},
  eprinttype = {jstor},
  pages = {935--954},
  publisher = {{[Wiley, Society for Research in Child Development]}},
  issn = {0009-3920},
  doi = {10.2307/1132043},
  urldate = {2023-10-30},
  abstract = {This longitudinal study of 161 African American children from low-income families examined multiple influences, including early childhood interventions and characteristics of the child and family, on longitudinal patterns of children's cognitive performance measured between 6 months and 8 years of age. Results indicate that more optimal patterns of cognitive development were associated with intensive early educational child care, responsive stimulating care at home, and higher maternal IQ. In accordance with a general systems model, analyses also suggested that child care experiences were related to better cognitive performance in part through enhancing the infant's responsiveness to his or her environment. Maternal IQ had both a direct effect on cognitive performance during early childhood and, also, an indirect effect through its influence on the family environment.},
  file = {/home/bolker/Zotero/storage/UPJTQEKS/Burchinal et al. - 1997 - Early Intervention and Mediating Processes in Cogn.pdf}
}

@book{singerApplied2003,
  title = {Applied {{Longitudinal Data Analysis}}: {{Modeling Change}} and {{Event Occurrence}}},
  shorttitle = {Applied {{Longitudinal Data Analysis}}},
  author = {Singer, Judith D. and Willett, John B.},
  year = {2003},
  month = mar,
  publisher = {{Oxford University Press, USA}},
  abstract = {Change is constant in everyday life. Infants crawl and then walk, children learn to read and write, teenagers mature in myriad ways, the elderly become frail and forgetful. Beyond these natural processes and events, external forces and interventions instigate and disrupt change: test scores may rise after a coaching course, drug abusers may remain abstinent after residential treatment. By charting changes over time and investigating whether and when events occur, researchers reveal the temporal rhythms of our lives. Applied Longitudinal Data Analysis is a much-needed professional book for empirical researchers and graduate students in the behavioral, social, and biomedical sciences. It offers the first accessible in-depth presentation of two of today's most popular statistical methods: multilevel models for individual change and hazard/survival models for event occurrence (in both discrete- and continuous-time). Using clear, concise prose and real data sets from published studies, the authors take you step by step through complete analyses, from simple exploratory displays that reveal underlying patterns through sophisticated specifications of complex statistical models.Applied Longitudinal Data Analysis offers readers a private consultation session with internationally recognized experts and represents a unique contribution to the literature on quantitative empirical methods.Visit http://www.ats.ucla.edu/stat/examples/alda.htm for:DT Downloadable data setsDT Library of computer programs in SAS, SPSS, Stata, HLM, MLwiN, and moreDT Additional material for data analysis},
  googlebooks = {PpnA1M8VwR8C},
  isbn = {978-0-19-515296-8},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Medical / Epidemiology,Psychology / Statistics}
}

@article{mccarthyProfiting2005,
  title = {Profiting from Prior Information in {Bayesian} Analyses of Ecological Data},
  author = {McCarthy, Michael A. and Masters, Pip},
  year = {2005},
  journal = {Journal of Applied Ecology},
  volume = {42},
  number = {6},
  pages = {1012--1019}
}

@book{mcelreathStatistical2020,
  title = {Statistical Rethinking: A {Bayesian} Course with Examples in {R} and {Stan}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {6H\_WDwAAQBAJ},
  isbn = {978-0-429-63914-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General}
}

@book{clarkModels2020,
  title = {Models for {{Ecological Data}}: {{An Introduction}}},
  shorttitle = {Models for {{Ecological Data}}},
  author = {Clark, James S.},
  year = {2020},
  month = oct,
  publisher = {{Princeton University Press}},
  abstract = {The environmental sciences are undergoing a revolution in the use of models and data. Facing ecological data sets of unprecedented size and complexity, environmental scientists are struggling to understand and exploit powerful new statistical tools for making sense of ecological processes.In Models for Ecological Data, James Clark introduces ecologists to these modern methods in modeling and computation. Assuming only basic courses in calculus and statistics, the text introduces readers to basic maximum likelihood and then works up to more advanced topics in Bayesian modeling and computation. Clark covers both classical statistical approaches and powerful new computational tools and describes how complexity can motivate a shift from classical to Bayesian methods. Through an available lab manual, the book introduces readers to the practical work of data modeling and computation in the language R. Based on a successful course at Duke University and National Science Foundation-funded institutes on hierarchical modeling, Models for Ecological Data will enable ecologists and other environmental scientists to develop useful models that make sense of ecological data.Consistent treatment from classical to modern BayesUnderlying distribution theory to algorithm developmentMany examples and applicationsDoes not assume statistical backgroundExtensive supporting appendixesLab manual in R is available separately},
  langid = {english}
}

@book{hobbsBayesian2015,
  title = {Bayesian {{Models}}: {{A Statistical Primer}} for {{Ecologists}}},
  shorttitle = {Bayesian {{Models}}},
  author = {Hobbs, N. Thompson and Hooten, Mevin B.},
  year = {2015},
  month = aug,
  publisher = {{Princeton University Press}},
  address = {{Princeton, New Jersey}},
  abstract = {Bayesian modeling has become an indispensable tool for ecological research because it is uniquely suited to deal with complexity in a statistically coherent way. This textbook provides a comprehensive and accessible introduction to the latest Bayesian methods\textemdash in language ecologists can understand. Unlike other books on the subject, this one emphasizes the principles behind the computations, giving ecologists a big-picture understanding of how to implement this powerful statistical approach.Bayesian Models is an essential primer for non-statisticians. It begins with a definition of probability and develops a step-by-step sequence of connected ideas, including basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and inference from single and multiple models. This unique book places less emphasis on computer coding, favoring instead a concise presentation of the mathematical statistics needed to understand how and why Bayesian analysis works. It also explains how to write out properly formulated hierarchical Bayesian models and use them in computing, research papers, and proposals.This primer enables ecologists to understand the statistical principles behind Bayesian modeling and apply them to research, teaching, policy, and management.Presents the mathematical and statistical foundations of Bayesian modeling in language accessible to non-statisticiansCovers basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and moreDeemphasizes computer coding in favor of basic principlesExplains how to write out properly factored statistical expressions representing Bayesian models},
  isbn = {978-0-691-15928-7},
  langid = {english}
}

@article{elderdUncertainty2006a,
  title = {Uncertainty in Predictions of Disease Spread and Public Health Responses to Bioterrorism and Emerging Diseases},
  author = {Elderd, Bret D. and Dukic, Vanja M. and Dwyer, Greg},
  year = {2006},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {103},
  number = {42},
  pages = {15693--15697},
  doi = {10.1073/pnas.0600816103},
  urldate = {2008-06-01},
  abstract = {Concerns over bioterrorism and emerging diseases have led to the widespread use of epidemic models for evaluating public health strategies. Partly because epidemic models often capture the dynamics of prior epidemics remarkably well, little attention has been paid to how uncertainty in parameter estimates might affect model predictions. To understand such effects, we used Bayesian statistics to rigorously estimate the uncertainty in the parameters of an epidemic model, focusing on smallpox bioterrorism. We then used a vaccination model to translate the uncertainty in the model parameters into uncertainty in which of two vaccination strategies would provide a better response to bioterrorism, mass vaccination, or vaccination of social contacts, so-called "trace vaccination." Our results show that the uncertainty in the model parameters is remarkably high and that this uncertainty has important implications for vaccination strategies. For example, under one plausible scenario, the most likely outcome is that mass vaccination would save \{approx\}100,000 more lives than trace vaccination. Because of the high uncertainty in the parameters, however, there is also a substantial probability that mass vaccination would save 200,000 or more lives than trace vaccination. In addition to providing the best response to the most likely outcome, mass vaccination thus has the advantage of preventing outcomes that are only slightly less likely but that are substantially more horrific. Rigorous estimates of uncertainty thus can reveal hidden advantages of public health strategies, suggesting that formal uncertainty estimation should play a key role in planning for epidemics.},
  file = {/home/bolker/Zotero/storage/J39ME5MK/Elderd et al - 2006 - Uncertainty in predictions of disease spread and p.pdf;/home/bolker/Zotero/storage/ZTAIUQI9/15693.html}
}

@article{ludwigUncertainty1996,
  title = {Uncertainty and the {{Assessment}} of {{Extinction Probabilities}}},
  author = {Ludwig, Donald},
  year = {1996},
  journal = {Ecological Applications},
  volume = {6},
  number = {4},
  pages = {1067--1076},
  issn = {1939-5582},
  doi = {10.2307/2269591},
  urldate = {2021-03-31},
  abstract = {A proper assessment of the probability of early collapse or extinction of a population requires consideration of our uncertainty about crucial parameters and processes. Simple simulation approaches to assessment consider only a single set of parameter values, but what is required is consideration of all more or less plausible combinations of parameters. Bayesian decision theory is an appropriate tool for such assessment. I contrast standard (frequentist) and Bayesian approaches to a simple regression problem. I use these results to calculate the probability of early population collapse for three data sets relating to the Palila, Laysan Finch, and Snow Goose. The Bayesian results imply much higher risk of early collapse than maximum likelihood methods. This difference is due to large probabilities of early collapse for certain parameter values that are plausible in light of the data. Because of simplifying assumptions, these results are not directly applicable to assessment. Nevertheless they imply that maximum likelihood and similar methods based upon point parameter estimates will grossly underestimate the risk of early collapse.},
  copyright = {\textcopyright{} 1996 by the Ecological Society of America},
  langid = {english},
  file = {/home/bolker/Zotero/storage/6LY7PVRC/2269591.html}
}

@article{bannerUse2020,
  title = {The Use of {{Bayesian}} Priors in {{Ecology}}: {{The}} Good, the Bad and the Not Great},
  shorttitle = {The Use of {{Bayesian}} Priors in {{Ecology}}},
  author = {Banner, Katharine M. and Irvine, Kathryn M. and Rodhouse, Thomas J.},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {8},
  pages = {882--889},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13407},
  urldate = {2023-05-19},
  abstract = {Bayesian data analysis (BDA) is a powerful tool for making inference from ecological data, but its full potential has yet to be realized. Despite a generally positive trajectory in research surrounding model development and assessment, far too little attention has been given to prior specification. Default priors, a sub-class of non-informative prior distributions that are often chosen without critical thought or evaluation, are commonly used in practice. We believe the fear of being too `subjective' has prevented many researchers from using any prior information in their analyses despite the fact that defending prior choice (informative or not) promotes good statistical practice. In this commentary, we provide an overview of how BDA is currently being used in a random sample of articles, discuss implications for inference if current bad practices continue, and highlight sub-fields where knowledge about the system has improved inference and promoted good statistical practices through the careful and justified use of informative priors. We hope to inspire a renewed discussion about the use of Bayesian priors in Ecology with particular attention paid to specification and justification. We also emphasize that all priors are the result of a subjective choice, and should be discussed in that way.},
  langid = {english},
  keywords = {Bayesian hierarchical models,good statistical practice,sensitivity analysis,subjective priors},
  file = {/home/bolker/Zotero/storage/5ZGNPEXS/Banner et al. - 2020 - The use of Bayesian priors in Ecology The good, t.pdf;/home/bolker/Zotero/storage/67KI3V63/2041-210X.html}
}

@article{lemoineMoving2019,
  title = {Moving beyond Noninformative Priors: Why and How to Choose Weakly Informative Priors in {{Bayesian}} Analyses},
  shorttitle = {Moving beyond Noninformative Priors},
  author = {Lemoine, Nathan P.},
  year = {2019},
  journal = {Oikos},
  volume = {128},
  number = {7},
  pages = {912--928},
  issn = {1600-0706},
  doi = {10.1111/oik.05985},
  urldate = {2021-06-11},
  abstract = {Throughout the last two decades, Bayesian statistical methods have proliferated throughout ecology and evolution. Numerous previous references established both philosophical and computational guidelines for implementing Bayesian methods. However, protocols for incorporating prior information, the defining characteristic of Bayesian philosophy, are nearly nonexistent in the ecological literature. Here, I hope to encourage the use of weakly informative priors in ecology and evolution by providing a `consumer's guide' to weakly informative priors. The first section outlines three reasons why ecologists should abandon noninformative priors: 1) common flat priors are not always noninformative, 2) noninformative priors provide the same result as simpler frequentist methods, and 3) noninformative priors suffer from the same high type I and type M error rates as frequentist methods. The second section provides a guide for implementing informative priors, wherein I detail convenient `reference' prior distributions for common statistical models (i.e. regression, ANOVA, hierarchical models). I then use simulations to visually demonstrate how informative priors influence posterior parameter estimates. With the guidelines provided here, I hope to encourage the use of weakly informative priors for Bayesian analyses in ecology. Ecologists can and should debate the appropriate form of prior information, but should consider weakly informative priors as the new `default' prior for any Bayesian model.},
  copyright = {\textcopyright{} 2019 The Authors},
  langid = {english},
  keywords = {Bayesian statistics,frequentist statistics,Markov chain Monte Carlo,vague priors},
  file = {/home/bolker/Zotero/storage/C3PL4BG6/Lemoine - 2019 - Moving beyond noninformative priors why and how t.pdf;/home/bolker/Zotero/storage/RHUN3DEC/oik.html}
}


@article{cromeNovel1996,
	title = {A {Novel} {Bayesian} {Approach} to {Assessing} {Impacts} of {Rain} {Forest} {Logging}},
	volume = {6},
	journal = {Ecological Applications},
	author = {Crome, F. H. J. and Thomas, M. R. and Moore, L. A.},
	year = {1996},
	pages = {1104--1123},
}


@article{edwardsComment1996,
	title = {Comment: {The} {First} {Data} {Analysis} {Should} be {Journalistic}},
	volume = {6},
	number = {4},
	journal = {Ecological Applications},
	author = {Edwards, Don},
	year = {1996},
	pages = {1090--1094},
}


@book{nicenboimIntroduction2023,
	title = {An Introduction to {Bayesian} Data Analysis for Cognitive Science},
	url = {https://vasishth.github.io/bayescogsci/book/},
	author = {Nicenboim, Bruno and Schad, Daniel and Vasishth, Shravan}
}


@article{gelmanBayesian2020,
	title = {Bayesian {Workflow}},
	url = {http://arxiv.org/abs/2011.01808},
	abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
	urldate = {2020-11-04},
	journal = {arXiv:2011.01808 [stat]},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.01808},
	keywords = {Statistics - Methodology},
	annote = {Comment: 77 pages, 35 figures},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/HYG8WGKB/Gelman et al. - 2020 - Bayesian Workflow.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/MK6YCMEY/2011.html:text/html},
}

@article{taltsValidating2020,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	url = {http://arxiv.org/abs/1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2021-12-02},
	journal = {arXiv:1804.06788 [stat]},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 1804.06788},
	keywords = {Statistics - Methodology},
	annote = {Comment: 19 pages, 13 figures},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/AK47EJTW/Talts et al. - 2020 - Validating Bayesian Inference Algorithms with Simu.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/YAUKHIW4/1804.html:text/html},
}

@article{grinsztajnBayesian2021,
	title = {Bayesian workflow for disease transmission modeling in {Stan}},
	volume = {40},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9164},
	doi = {10.1002/sim.9164},
	abstract = {This tutorial shows how to build, fit, and criticize disease transmission models in Stan, and should be useful to researchers interested in modeling the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic and other infectious diseases in a Bayesian framework. Bayesian modeling provides a principled way to quantify uncertainty and incorporate both data and prior knowledge into the model estimates. Stan is an expressive probabilistic programming language that abstracts the inference and allows users to focus on the modeling. As a result, Stan code is readable and easily extensible, which makes the modeler's work more transparent. Furthermore, Stan's main inference engine, Hamiltonian Monte Carlo sampling, is amiable to diagnostics, which means the user can verify whether the obtained inference is reliable. In this tutorial, we demonstrate how to formulate, fit, and diagnose a compartmental transmission model in Stan, first with a simple susceptible-infected-recovered model, then with a more elaborate transmission model used during the SARS-CoV-2 pandemic. We also cover advanced topics which can further help practitioners fit sophisticated models; notably, how to use simulations to probe the model and priors, and computational techniques to scale-up models based on ordinary differential equations.},
	language = {en},
	number = {27},
	urldate = {2022-11-14},
	journal = {Statistics in Medicine},
	author = {Grinsztajn, Léo and Semenova, Elizaveta and Margossian, Charles C. and Riou, Julien},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9164},
	keywords = {epidemiology, infectious diseases, Bayesian workflow, compartmental models},
	pages = {6209--6234},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/CPBRGQYS/Grinsztajn et al. - 2021 - Bayesian workflow for disease transmission modelin.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/YIKLLXAF/sim.html:text/html},
}


@article{xieMeasures2006,
	title = {Measures of {Bayesian} learning and identifiability in hierarchical models},
	volume = {136},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375805001278},
	doi = {10.1016/j.jspi.2005.04.003},
	abstract = {Identifiability has long been an important concept in classical statistical estimation. Historically, Bayesians have been less interested in the concept since, strictly speaking, any parameter having a proper prior distribution also has a proper posterior, and is thus estimable. However, the larger statistical community's recent move toward more Bayesian thinking is largely fueled by an interest in Markov chain Monte Carlo-based analyses using vague or even improper priors. As such, Bayesians have been forced to think more carefully about what has been learned about the parameters of interest (given the data so far), or what could possibly be learned (given an infinite amount of data). In this paper, we propose measures of Bayesian learning based on differences in precision and Kullback–Leibler divergence. After investigating them in the context of some familiar Gaussian linear hierarchical models, we consider their use in a more challenging setting involving two sets of random effects (traditional and spatially arranged), only the sum of which is identified by the data. We illustrate this latter model with an example from periodontal data analysis, where the spatial aspect arises from the proximity of various measurements taken in the mouth. Our results suggest our measures behave sensibly and may be useful in even more complicated (e.g., non-Gaussian) model settings.},
	language = {en},
	number = {10},
	urldate = {2023-03-14},
	journal = {Journal of Statistical Planning and Inference},
	author = {Xie, Yang and Carlin, Bradley P.},
	month = oct,
	year = {2006},
	keywords = {Markov chain Monte Carlo (MCMC), Spatial statistics, Conditionally autoregressive (CAR) model, Nonidentifiability, Noninformativity},
	pages = {3458--3477},
}


@misc{kallioinenDetecting2022,
	title = {Detecting and diagnosing prior and likelihood sensitivity with power-scaling},
	url = {http://arxiv.org/abs/2107.14054},
	doi = {10.48550/arXiv.2107.14054},
	abstract = {Determining the sensitivity of the posterior to perturbations of the prior and likelihood is an important part of the Bayesian workflow. We introduce a practical and computationally efficient sensitivity analysis approach using importance sampling to estimate properties of posteriors resulting from power-scaling the prior or likelihood. On this basis, we suggest a diagnostic that can indicate the presence of prior-data conflict or likelihood noninformativity and discuss limitations to this power-scaling approach. The approach can be easily included in Bayesian workflows with minimal effort by the model builder and we present an implementation in our new R package priorsense. We further demonstrate the workflow on case studies of real data using models varying in complexity from simple linear models to Gaussian process models.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Kallioinen, Noa and Paananen, Topi and Bürkner, Paul-Christian and Vehtari, Aki},
	month = dec,
	year = {2022},
	note = {arXiv:2107.14054 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: 31 pages, 15 (+5 suppl) figures},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/FBFDYD2G/Kallioinen et al. - 2022 - Detecting and diagnosing prior and likelihood sens.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/QD6P6ALC/2107.html:text/html},
}


@article{ibrahimpower2015,
	title = {The power prior: theory and applications},
	volume = {34},
	issn = {1097-0258},
	shorttitle = {The power prior},
	doi = {10.1002/sim.6728},
	abstract = {The power prior has been widely used in many applications covering a large number of disciplines. The power prior is intended to be an informative prior constructed from historical data. It has been used in clinical trials, genetics, health care, psychology, environmental health, engineering, economics, and business. It has also been applied for a wide variety of models and settings, both in the experimental design and analysis contexts. In this review article, we give an A-to-Z exposition of the power prior and its applications to date. We review its theoretical properties, variations in its formulation, statistical contexts for which it has been used, applications, and its advantages over other informative priors. We review models for which it has been used, including generalized linear models, survival models, and random effects models. Statistical areas where the power prior has been used include model selection, experimental design, hierarchical modeling, and conjugate priors. Frequentist properties of power priors in posterior inference are established, and a simulation study is conducted to further examine the empirical performance of the posterior estimates with power priors. Real data analyses are given illustrating the power prior as well as the use of the power prior in the Bayesian design of clinical trials.},
	language = {eng},
	number = {28},
	journal = {Statistics in Medicine},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gwon, Yeongjin and Chen, Fang},
	month = dec,
	year = {2015},
	pmid = {26346180},
	pmcid = {PMC4626399},
	keywords = {clinical trials, Research Design, Bayes Theorem, Models, Statistical, Linear Models, Statistics as Topic, Clinical Trials as Topic, Bayesian design, borrowing, discounting, historical data, Historically Controlled Study, informative prior},
	pages = {3724--3749},
	file = {Submitted Version:/home/bolker/Documents/zotero_new/storage/UHNBXBUC/Ibrahim et al. - 2015 - The power prior theory and applications.pdf:application/pdf},
}


@techreport{finkcompendium1997,
	title = {A compendium of conjugate priors},
	url = {https://web.archive.org/web/20090529203101/http://www.people.cornell.edu/pages/df36/CONJINTRnew%20TEX.pdf},
	author = {Fink, Daniel},
	year = {1997},
	note = {Publisher: Citeseer},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/24272TBY/Fink - 1997 - A compendium of conjugate priors.pdf:application/pdf},
}


@article{singmannStatistics2023,
	title = {Statistics in the {Service} of {Science}: {Don}’t {Let} the {Tail} {Wag} the {Dog}},
	volume = {6},
	issn = {2522-0861, 2522-087X},
	shorttitle = {Statistics in the {Service} of {Science}},
	url = {https://link.springer.com/10.1007/s42113-022-00129-2},
	doi = {10.1007/s42113-022-00129-2},
	abstract = {Statistical modeling is generally meant to describe patterns in data in service of the broader scientific goal of developing theories to explain those patterns. Statistical models support meaningful inferences when models are built so as to align parameters of the model with potential causal mechanisms and how they manifest in data. When statistical models are instead based on assumptions chosen by default, attempts to draw inferences can be uninformative or even paradoxical—in essence, the tail is trying to wag the dog. These issues are illustrated by van Doorn et al. (this issue) in the context of using Bayes Factors to identify effects and interactions in linear mixed models. We show that the problems identified in their applications (along with other problems identified here) can be circumvented by using priors over inherently meaningful units instead of default priors on standardized scales. This case study illustrates how researchers must directly engage with a number of substantive issues in order to support meaningful inferences, of which we highlight two: The first is the problem of coordination, which requires a researcher to specify how the theoretical constructs postulated by a model are functionally related to observable variables. The second is the problem of generalization, which requires a researcher to consider how a model may represent theoretical constructs shared across similar but non-identical situations, along with the fact that model comparison metrics like Bayes Factors do not directly address this form of generalization. For statistical modeling to serve the goals of science, models cannot be based on default assumptions, but should instead be based on an understanding of their coordination function and on how they represent causal mechanisms that may be expected to generalize to other related scenarios.},
	language = {en},
	number = {1},
	urldate = {2023-05-22},
	journal = {Computational Brain \& Behavior},
	author = {Singmann, Henrik and Kellen, David and Cox, Gregory E. and Chandramouli, Suyog H. and Davis-Stober, Clintin P. and Dunn, John C. and Gronau, Quentin F. and Kalish, Michael L. and McMullin, Sara D. and Navarro, Danielle J. and Shiffrin, Richard M.},
	month = mar,
	year = {2023},
	pages = {64--83},
	file = {Singmann et al. - 2023 - Statistics in the Service of Science Don’t Let th.pdf:/home/bolker/Documents/zotero_new/storage/VIC89R9H/Singmann et al. - 2023 - Statistics in the Service of Science Don’t Let th.pdf:application/pdf},
}


@article{gelmanPrior2006,
	title = {Prior distributions for variance parameters in hierarchical models (comment on article by {Browne} and {Draper})},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-3/Prior-distributions-for-variance-parameters-in-hierarchical-models-comment-on/10.1214/06-BA117A.full},
	doi = {10.1214/06-BA117A},
	abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-\$t\$ family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of "noninformative" prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-\$t\$ family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-\$t\$ family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
	number = {3},
	urldate = {2021-06-12},
	journal = {Bayesian Analysis},
	author = {Gelman, Andrew},
	month = sep,
	year = {2006},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Bayesian inference, conditional conjugacy, folded-noncentral-\$t\$ distribution, half-\$t\$ distribution, hierarchical model, multilevel model, noninformative prior distribution, weakly informative prior distribution},
	pages = {515--534},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/JQPHQEVI/Gelman - 2006 - Prior distributions for variance parameters in hie.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/LX7F8HD5/06-BA117A.html:text/html},
}


@misc{carpenterComputational2017,
	title = {Computational and statistical issues with uniform interval priors},
	url = {http://andrewgelman.com/2017/11/28/computational-statistical-issues-uniform-interval-priors/},
	abstract = {There are two anti-patterns* for prior specification in Stan programs that can be sourced directly to idioms developed for BUGS. One is the diffuse gamma priors that Andrew’s already written about at length. The second is interval-based priors. Which brings us to today’s post. Interval priors An interval prior is something like this in Stan …},
	language = {en-US},
	urldate = {2018-05-15},
	journal = {Statistical Modeling, Causal Inference, and Social Science},
	author = {Carpenter, Bob},
	month = nov,
	year = {2017},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/UB6BV69E/computational-statistical-issues-uniform-interval-priors.html:text/html},
}


@book{inchaustiStatistical2023,
	title = {Statistical {Modeling} {With} {R}: a dual frequentist and {Bayesian} approach for life scientists},
	isbn = {978-0-19-267503-3},
	shorttitle = {Statistical {Modeling} {With} {R}},
	abstract = {To date, statistics has tended to be neatly divided into two theoretical approaches or frameworks: frequentist (or classical) and Bayesian. Scientists typically choose the statistical framework to analyse their data depending on the nature and complexity of the problem, and based on their personal views and prior training on probability and uncertainty. Although textbooks and courses should reflect and anticipate this dual reality, they rarely do so. This accessible textbook explains, discusses, and applies both the frequentist and Bayesian theoretical frameworks to fit the different types of statistical models that allow an analysis of the types of data most commonly gathered by life scientists. It presents the material in an informal, approachable, and progressive manner suitable for readers with only a basic knowledge of calculus and statistics. Statistical Modeling with R is aimed at senior undergraduate and graduate students, professional researchers, and practitioners throughout the life sciences, seeking to strengthen their understanding of quantitative methods and to apply them successfully to real world scenarios, whether in the fields of ecology, evolution, environmental studies, or computational biology.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Inchausti, Pablo},
	month = jan,
	year = {2023},
	note = {Google-Books-ID: uiyWEAAAQBAJ},
	keywords = {Computers / Database Administration \& Management, Computers / Mathematical \& Statistical Software, Mathematics / Applied, Mathematics / Probability \& Statistics / General, Science / Life Sciences / Taxonomy},
}


@article{lewandowskiGenerating2009a,
	title = {Generating random correlation matrices based on vines and extended onion method},
	volume = {100},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X09000876},
	doi = {10.1016/j.jmva.2009.04.008},
	abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276–294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177–2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
	number = {9},
	urldate = {2017-08-04},
	journal = {Journal of Multivariate Analysis},
	author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
	month = oct,
	year = {2009},
	keywords = {Correlation matrix, Dependence vines, Onion method, Partial correlation},
	pages = {1989--2001},
	file = {ScienceDirect Full Text PDF:/home/bolker/Documents/zotero_new/storage/JZ36GIJN/Lewandowski et al. - 2009 - Generating random correlation matrices based on vi.pdf:application/pdf;ScienceDirect Snapshot:/home/bolker/Documents/zotero_new/storage/G4IX2BJ8/S0047259X09000876.html:text/html},
}


@inproceedings{sarmaPrior2020,
  title = {Prior Setting in Practice: Strategies and Rationales Used in Choosing Prior Distributions for {Bayesian} Analysis},
  shorttitle = {Prior {{Setting}} in {{Practice}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sarma, Abhraneel and Kay, Matthew},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376377},
  urldate = {2023-05-27},
  abstract = {Bayesian statistical analysis is steadily growing in popularity and use. Choosing priors is an integral part of Bayesian inference. While there exist extensive normative recommendations for prior setting, little is known about how priors are chosen in practice. We conducted a survey (N = 50) and interviews (N = 9) where we used interactive visualizations to elicit prior distributions from researchers experienced withBayesian statistics and asked them for rationales for those priors. We found that participants' experience and philosophy influence how much and what information they are willing to incorporate into their priors, manifesting as different levels of informativeness and skepticism. We also identified three broad strategies participants use to set their priors: centrality matching, interval matching, and visual mass allocation. We discovered that participants' understanding of the notion of 'weakly informative priors"-a commonly-recommended normative approach to prior setting-manifests very differently across participants. Our results have implications both for how to develop prior setting recommendations and how to design tools to elicit priors in Bayesian analysis.},
  isbn = {978-1-4503-6708-0},
  keywords = {bayesian inference,descriptive analysis,prior distributions},
  file = {/home/bolker/Zotero/storage/S3K8XVIH/Sarma and Kay - 2020 - Prior Setting in Practice Strategies and Rational.pdf}
}

@article{vatsRevisiting2018,
  title = {Revisiting the {{Gelman-Rubin Diagnostic}}},
  author = {Vats, Dootika and Knudson, Christina},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.09384 [stat]},
  eprint = {1812.09384},
  primaryclass = {stat},
  urldate = {2019-01-29},
  abstract = {Gelman and Rubin's (1992) convergence diagnostic is one of the most popular methods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the seminal paper, researchers have developed sophisticated methods of variance estimation for Monte Carlo averages. We show that this class of estimators find immediate use in the Gelman-Rubin statistic, a connection not established in the literature before. We incorporate these estimators to upgrade both the univariate and multivariate Gelman-Rubin statistics, leading to increased stability in MCMC termination time. An immediate advantage is that our new Gelman-Rubin statistic can be calculated for a single chain. In addition, we establish a relationship between the Gelman-Rubin statistic and effective sample size. Leveraging this relationship, we develop a principled cutoff criterion for the Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved diagnostic via examples.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/bolker/Zotero/storage/39ZWMWD7/Vats and Knudson - 2018 - Revisiting the Gelman-Rubin Diagnostic.pdf;/home/bolker/Zotero/storage/9A2YQJRA/1812.html}
}

@article{lambertRobust2022,
  title = {{{R}}{${_\ast}$}: {{A Robust MCMC Convergence Diagnostic}} with {{Uncertainty Using Decision Tree Classifiers}}},
  shorttitle = {{{R}}{${_\ast}$}},
  author = {Lambert, Ben and Vehtari, Aki},
  year = {2022},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {17},
  number = {2},
  pages = {353--379},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1252},
  urldate = {2022-11-30},
  abstract = {Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over the past three decades: mainly because of this, Bayesian inference is now a workhorse of applied scientists. Under general conditions, MCMC sampling converges asymptotically to the posterior distribution, but this provides no guarantees about its performance in finite time. The predominant method for monitoring convergence is to run multiple chains and monitor individual chains' characteristics and compare these to the population as a whole: if within-chain and between-chain summaries are comparable, then this is taken to indicate that the chains have converged to a common stationary distribution. Here, we introduce a new method for diagnosing convergence based on how well a machine learning classifier model can successfully discriminate the individual chains. We call this convergence measure R{${_\ast}$}. In contrast to the predominant R\textasciicircum, R{${_\ast}$} is a single statistic across all parameters that indicates lack of mixing, although individual variables' importance for this metric can also be determined. Additionally, R{${_\ast}$} is not based on any single characteristic of the sampling distribution; instead it uses all the information in the chain, including that given by the joint sampling distribution, which is currently largely overlooked by existing approaches. We recommend calculating R{${_\ast}$} using two different machine learning classifiers \textemdash{} gradient-boosted regression trees and random forests \textemdash{} which each work well in models of different dimensions. Because each of these methods outputs a classification probability, as a byproduct, we obtain uncertainty in R{${_\ast}$}. The method is straightforward to implement and could be a complementary additional check on MCMC convergence for applied analyses.},
  file = {/home/bolker/Zotero/storage/RX9L2CNB/Lambert and Vehtari - 2022 - R∗ A Robust MCMC Convergence Diagnostic with Unce.pdf;/home/bolker/Zotero/storage/ITVLSUIL/20-BA1252.html}
}

@article{vehtariRankNormalization2021a,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R-hat}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2021},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {2},
  pages = {667--718},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  urldate = {2022-04-18},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic R\textasciicircum{} of Gelman and Rubin (1992) has serious flaws. Traditional R\textasciicircum{} will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  file = {/home/bolker/Zotero/storage/9LQLBVA6/Vehtari et al. - 2021 - Rank-Normalization, Folding, and Localization An .pdf;/home/bolker/Zotero/storage/KW8BZVYG/20-BA1221.html}
}


@misc{Best2019,
  title = {N Best Tips \& Tricks (or the Go-to Checklist) for New {{Stan}} Model Builders?},
  year = {2019},
  month = may,
  journal = {The Stan Forums},
  urldate = {2023-05-29},
  abstract = {Is there a one-page checklist of the captioned written somewhere?  In my recent exploration as a newbie (see this post), I find the following tricks most useful:    specify any known bounds of a parameter (thereby help Stan to pick the appropriate default prior, eg, uniform vs. normal)    explicitly specify a prior for a parameter that better reflects your knowledge about it than Stan's default prior choice could possibly represent in general    use non-centered parametrization EDIT: (eg, to fix...},
  chapter = {Modeling},
  howpublished = {https://discourse.mc-stan.org/t/n-best-tips-tricks-or-the-go-to-checklist-for-new-stan-model-builders/8688},
  langid = {english},
  file = {/home/bolker/Zotero/storage/RMKP3ZVN/8688.html}
}



@article{reimherrPrior2021,
	title = {Prior {Sample} {Size} {Extensions} for {Assessing} {Prior} {Impact} and {Prior}-{Likelihood} {Discordance}},
	volume = {83},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/rssb.12414},
	doi = {10.1111/rssb.12414},
	abstract = {This paper outlines a framework for quantifying the prior’s contribution to posterior inference in the presence of prior-likelihood discordance, a broader concept than the usual notion of prior-likelihood conflict. We achieve this dual purpose by extending the classic notion of prior sample size, M, in three directions: (I) estimating M beyond conjugate families; (II) formulating M as a relative notion that is as a function of the likelihood sample size k, M(k), which also leads naturally to a graphical diagnosis; and (III) permitting negative M, as a measure of prior-likelihood conflict, that is, harmful discordance. Our asymptotic regime permits the prior sample size to grow with the likelihood data size, hence making asymptotic arguments meaningful for investigating the impact of the prior relative to that of likelihood. It leads to a simple asymptotic formula for quantifying the impact of a proper prior that only involves computing a centrality and a spread measure of the prior and the posterior. We use simulated and real data to illustrate the potential of the proposed framework, including quantifying how weak is a ‘weakly informative’ prior adopted in a study of lupus nephritis. Whereas we take a pragmatic perspective in assessing the impact of a prior on a given inference problem under a specific evaluative metric, we also touch upon conceptual and theoretical issues such as using improper priors and permitting priors with asymptotically non-vanishing influence.},
	number = {3},
	urldate = {2023-05-29},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Reimherr, Matthew and Meng, Xiao-Li and Nicolae, Dan L.},
	month = jul,
	year = {2021},
	pages = {413--437},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/Z77GJTSZ/Reimherr et al. - 2021 - Prior Sample Size Extensions for Assessing Prior I.pdf:application/pdf},
}

@article{mullerMeasuring2012,
	title = {Measuring prior sensitivity and prior informativeness in large {Bayesian} models},
	volume = {59},
	issn = {03043932},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S030439321200092X},
	doi = {10.1016/j.jmoneco.2012.09.003},
	abstract = {In large Bayesian models, such as modern DSGE models, it is difﬁcult to assess how much the prior affects the results. This paper derives measures of prior sensitivity and prior informativeness that account for the high dimensional interaction between prior and likelihood information. The basis for both measures is the derivative matrix of the posterior mean with respect to the prior mean, which is easily obtained from Markov Chain Monte Carlo output. We illustrate the approach by examining posterior results in the small model of Lubik and Schorfheide (2004) and the large model of Smets and Wouters (2007).},
	language = {en},
	number = {6},
	urldate = {2023-05-29},
	journal = {Journal of Monetary Economics},
	author = {Müller, Ulrich K.},
	month = oct,
	year = {2012},
	pages = {581--597},
	file = {Müller - 2012 - Measuring prior sensitivity and prior informativen.pdf:/home/bolker/Documents/zotero_new/storage/IRAKCGAK/Müller - 2012 - Measuring prior sensitivity and prior informativen.pdf:application/pdf},
}



@techreport{lindleyBayesian1980,
	title = {The {Bayesian} {Approach} to {Statistics}},
	url = {https://apps.dtic.mil/sti/pdfs/ADA087836.pdf},
	language = {en},
	number = {ORC 80-9},
	institution = {Operations Research Center, University of California, Berkeley},
	author = {Lindley, Dennis V},
	month = may,
	year = {1980},
	file = {Lindley - THE BAYESIAN APPROACH TO STATISTICS.pdf:/home/bolker/Documents/zotero_new/storage/TSW9E7Y7/Lindley - THE BAYESIAN APPROACH TO STATISTICS.pdf:application/pdf},
}

@article{chungNondegenerate2013a,
  title = {A {{Nondegenerate Penalized Likelihood Estimator}} for {{Variance Parameters}} in {{Multilevel Models}}},
  author = {Chung, Yeojin and {Rabe-Hesketh}, Sophia and Dorie, Vincent and Gelman, Andrew and Liu, Jingchen},
  year = {2013},
  month = mar,
  journal = {Psychometrika},
  volume = {78},
  number = {4},
  pages = {685--709},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-013-9328-2},
  urldate = {2015-07-15},
  abstract = {Group-level variance estimates of zero often arise when fitting multilevel or hierarchical linear models, especially when the number of groups is small. For situations where zero variances are implausible a priori, we propose a maximum penalized likelihood approach to avoid such boundary estimates. This approach is equivalent to estimating variance parameters by their posterior mode, given a weakly informative prior distribution. By choosing the penalty from the log-gamma family with shape parameter greater than 1, we ensure that the estimated variance will be positive. We suggest a default log-gamma(2,{$\lambda$}) penalty with {$\lambda\rightarrow$}0, which ensures that the maximum penalized likelihood estimate is approximately one standard error from zero when the maximum likelihood estimate is zero, thus remaining consistent with the data while being nondegenerate. We also show that the maximum penalized likelihood estimator with this default penalty is a good approximation to the posterior median obtained under a noninformative prior. Our default method provides better estimates of model parameters and standard errors than the maximum likelihood or the restricted maximum likelihood estimators. The log-gamma family can also be used to convey substantive prior information. In either case\textemdash pure penalization or prior information\textemdash our recommended procedure gives nondegenerate estimates and in the limit coincides with maximum likelihood as the number of groups increases.},
  langid = {english},
  keywords = {{Assessment, Testing and Evaluation},Bayes modal estimation,hierarchical linear model,Mixed Model,Multilevel model,penalized likelihood,Psychometrics,Statistical Theory and Methods,{Statistics for Social Science, Behavorial Science, Education, Public Policy, and Law},variance estimation,weakly informative prior},
  file = {/home/bolker/Zotero/storage/D5QRECWM/Chung et al_2013_A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in.pdf;/home/bolker/Zotero/storage/BK6MIVBC/10.html}
}

@book{gelmanData2006,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, England}}
}

@article{schielzethSimple2010,
  title = {Simple Means to Improve the Interpretability of Regression Coefficients: {{Interpretation}} of Regression Coefficients},
  shorttitle = {Simple Means to Improve the Interpretability of Regression Coefficients},
  author = {Schielzeth, Holger},
  year = {2010},
  month = feb,
  journal = {Methods in Ecology and Evolution},
  volume = {1},
  number = {2},
  pages = {103--113},
  issn = {2041210X, 2041210X},
  doi = {10.1111/j.2041-210X.2010.00012.x},
  urldate = {2016-06-08},
  langid = {english},
  file = {/home/bolker/Zotero/storage/XNG3ZF5X/j.2041-210X.2010.00012.x.pdf}
}


@article{wanEstimating2014,
  title = {Estimating the Sample Mean and Standard Deviation from the Sample Size, Median, Range and/or Interquartile Range},
  author = {Wan, Xiang and Wang, Wenqian and Liu, Jiming and Tong, Tiejun},
  year = {2014},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {14},
  number = {1},
  pages = {135},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-135},
  urldate = {2023-06-02},
  abstract = {In systematic reviews and meta-analysis, researchers often pool the results of the sample mean and standard deviation from a set of similar clinical trials. A number of the trials, however, reported the study using the median, the minimum and maximum values, and/or the first and third quartiles. Hence, in order to combine results, one may have to estimate the sample mean and standard deviation for such trials.},
  langid = {english},
  keywords = {Interquartile range,Median,Meta-analysis,Sample mean,Sample size,Standard deviation},
  file = {/home/bolker/Zotero/storage/TLREHG4J/Wan et al. - 2014 - Estimating the sample mean and standard deviation .pdf}
}

@article{bolkerStrategies2013,
  title = {Strategies for Fitting Nonlinear Ecological Models in {{R}}, {{AD Model Builder}}, and {{BUGS}}},
  author = {Bolker, Benjamin M. and Gardner, Beth and Maunder, Mark and Berg, Casper W. and Brooks, Mollie and Comita, Liza and Crone, Elizabeth and Cubaynes, Sarah and Davies, Trevor and {de Valpine}, Perry and Ford, Jessica and Gimenez, Olivier and K{\'e}ry, Marc and Kim, Eun Jung and {Lennert-Cody}, Cleridy and Magnusson, Arni and Martell, Steve and Nash, John and Nielsen, Anders and Regetz, Jim and Skaug, Hans and Zipkin, Elise},
  editor = {Ramula, Satu},
  year = {2013},
  month = jun,
  journal = {Methods in Ecology and Evolution},
  volume = {4},
  number = {6},
  pages = {501--512},
  issn = {2041210X},
  doi = {10.1111/2041-210X.12044},
  urldate = {2013-06-11},
  file = {/home/bolker/Zotero/storage/U83CEWVU/Bolker et al_2013_Strategies for fitting nonlinear ecological models in R, AD Model Builder, and.pdf}
}

@book{keryBayesian2011,
  title = {Bayesian {{Population Analysis}} Using {{WinBUGS}}: {{A Hierarchical Perspective}}},
  shorttitle = {Bayesian {{Population Analysis}} Using {{WinBUGS}}},
  author = {K{\'e}ry, Marc and Schaub, Michael},
  year = {2011},
  month = oct,
  edition = {1st edition},
  publisher = {{Academic Press}},
  address = {{Boston}},
  abstract = {Bayesian statistics has exploded into biology and its sub-disciplines, such as ecology, over the past decade. The free software program WinBUGS, and its open-source sister OpenBugs, is currently the only flexible and general-purpose program available with which the average ecologist can conduct standard and non-standard Bayesian statistics.Comprehensive and richly commented examples illustrate a wide range of models that are most relevant to the research of a modern population ecologistAll WinBUGS/OpenBUGS analyses are completely integrated in software RIncludes complete documentation of all R and WinBUGS code required to conduct analyses and shows all the necessary steps from having the data in a text file out of Excel to interpreting and processing the output from WinBUGS in R},
  isbn = {978-0-12-387020-9},
  langid = {english}
}

@book{keryIntroduction2010,
  title = {Introduction to {{WinBUGS}} for Ecologists {{Bayesian}} Approach to Regression, {{ANOVA}}, Mixed Models and Related Analyses},
  author = {K{\'e}ry, Marc},
  year = {2010},
  publisher = {{Elsevier}},
  address = {{Amsterdam; Boston}},
  abstract = {Bayesian statistics has exploded into biology and its sub-disciplines such as ecology over the past decade. The free software program WinBUGS and its open-source sister OpenBugs is currently the only flexible and general-purpose program available with which the average ecologist can conduct their own standard and non-standard Bayesian statistics. Introduction to WINBUGS for Ecologists goes right to the heart of the matter by providing ecologists with a comprehensive, yet concise, guide to applying WinBUGS to the types of models that they use most often: linear (LM), generalized linear (GLM), linear mixed (LMM) and generalized linear mixed models (GLMM). Introduction to WinBUGS for Ecologists combines the use of simulated data sets "paired" analyses using WinBUGS (in a Bayesian framework for analysis) and in R (in a frequentist mode of inference) and uses a very detailed step-by-step tutorial presentation style that really lets the reader repeat every step of the application of a given mode in their own research. - Introduction to the essential theories of key models used by ecologists - Complete juxtaposition of classical analyses in R and Bayesian Analysis of the same models in WinBUGS - Provides every detail of R and WinBUGS code required to conduct all analyses - Written with ecological language and ecological examples - Companion Web Appendix that contains all code contained in the book, additional material (including more code and solutions to exercises) - Tutorial approach shows ecologists how to implement Bayesian analysis in practical problems that they face.},
  isbn = {978-0-12-378605-0 0-12-378605-3 0-12-378606-1 978-0-12-378606-7 1-282-75566-8 978-1-282-75566-6},
  langid = {english}
}

@book{kruschkeDoing2014,
  title = {Doing {{Bayesian Data Analysis}}: {{A Tutorial}} with {{R}}, {{JAGS}}, and {{Stan}}},
  shorttitle = {Doing {{Bayesian Data Analysis}}},
  author = {Kruschke, John},
  year = {2014},
  month = nov,
  edition = {2nd edition},
  publisher = {{Academic Press}},
  address = {{Boston}},
  abstract = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition provides an accessible approach for conducting Bayesian data analysis, as material is explained clearly with concrete examples. Included are step by step instructions on how to carry out Bayesian data analyses in the popular and free software R and WinBugs, as well as new programs in JAGS and Stan. The new programs are designed to be much easier to use than the scripts in the first edition. In particular, there are now compact high level scripts that make it easy to run the programs on your own data sets.The book is divided into three parts and begins with the basics: models, probability, Bayes' rule, and the R programming language. The discussion then moves to the fundamentals applied to inferring a binomial probability, before concluding with chapters on the generalized linear model. Topics include metric predicted variable on one or two groups; metric predicted variable with one metric predictor; metric predicted variable with multiple metric predictors; metric predicted variable with one nominal predictor; and metric predicted variable with multiple nominal predictors. The exercises found in the text have explicit purposes and guidelines for accomplishment.This book is intended for first year graduate students or advanced undergraduates in statistics, data analysis, psychology, cognitive science, social sciences, clinical sciences, and consumer sciences in business. Accessible, including the basics of essential concepts of probability and random samplingExamples with R programming language and JAGS softwareComprehensive coverage of all scenarios addressed by non Bayesian textbooks: t tests, analysis of variance (ANOVA) and comparisons in ANOVA, multiple regression, and chi square (contingency table analysis)Coverage of experiment planningR and JAGS computer programming code on websiteExercises have explicit purposes and guidelines for accomplishmentProvides step by step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs.},
  isbn = {978-0-12-405888-0},
  langid = {english}
}


@book{bolstadIntroduction2016,
  title = {Introduction to {{Bayesian Statistics}}},
  author = {Bolstad, William M. and Curran, James M.},
  year = {2016},
  month = oct,
  edition = {3rd edition},
  publisher = {{Wiley}},
  address = {{Hoboken, New Jersey}},
  abstract = {"...this edition is useful and effective in teaching Bayesian inference at both elementary and intermediate levels. It is a well-written book on elementary Bayesian inference, and the material is easily accessible. It is both concise and timely, and provides a good collection of overviews and reviews of important tools used in Bayesian statistical methods."There is a strong upsurge in the use of Bayesian methods in applied statistical analysis, yet most introductory statistics texts only present frequentist methods. Bayesian statistics has many important advantages that students should learn about if they are going into fields where statistics will be used. In this third Edition, four newly-added chapters address topics that reflect the rapid advances in the field of Bayesian statistics. The authors continue to provide a Bayesian treatment of introductory statistical topics, such as scientific data gathering, discrete random variables, robust Bayesian methods, and Bayesian approaches to inference for discrete random variables, binomial proportions, Poisson, and normal means, and simple linear regression. In addition, more advanced topics in the field are presented in four new chapters: Bayesian inference for a normal with unknown mean and variance; Bayesian inference for a Multivariate Normal mean vector; Bayesian inference for the Multiple Linear Regression Model; and Computational Bayesian Statistics including Markov Chain Monte Carlo. The inclusion of these topics will facilitate readers' ability to advance from a minimal understanding of Statistics to the ability to tackle topics in more applied, advanced level books. Minitab macros and R functions are available on the book's related website to assist with chapter exercises. Introduction to Bayesian Statistics, Third Edition also features:Topics including the Joint Likelihood function and inference using independent Jeffreys priors and join conjugate priorThe cutting-edge topic of computational Bayesian Statistics in a new chapter, with a unique focus on Markov Chain Monte Carlo methodsExercises throughout the book that have been updated to reflect new applications and the latest software applicationsDetailed appendices that guide readers through the use of R and Minitab software for Bayesian analysis and Monte Carlo simulations, with all related macros available on the book's websiteIntroduction to Bayesian Statistics, Third Edition is a textbook for upper-undergraduate or first-year graduate level courses on introductory statistics course with a Bayesian emphasis. It can also be used as a reference work for statisticians who require a working knowledge of Bayesian statistics.},
  isbn = {978-1-118-09156-2},
  langid = {english}
}

@book{hoffFirst2010,
  title = {A {{First Course}} in {{Bayesian Statistical Methods}}},
  author = {Hoff, Peter D.},
  year = {2010},
  month = nov,
  edition = {Softcover reprint of hardcover 1st ed. 2009 edition},
  publisher = {{Springer}},
  address = {{Dordrecht Heidelberg New York}},
  abstract = {A self-contained introduction to probability, exchangeability and Bayes' rule provides a theoretical understanding of the applied material. Numerous examples with R-code that can be run "as-is" allow the reader to perform the data analyses themselves. The development of Monte Carlo and Markov chain Monte Carlo methods in the context of data analysis examples provides motivation for these computational methods.},
  isbn = {978-1-4419-2828-3},
  langid = {english}
}

@article{ngEstimation2006,
  title = {Estimation in Generalised Linear Mixed Models with Binary Outcomes by                 Simulated Maximum Likelihood},
  author = {Ng, Edmond SW and Carpenter, James R and Goldstein, Harvey and Rasbash, Jon},
  year = {2006},
  month = apr,
  journal = {Statistical Modelling},
  volume = {6},
  number = {1},
  pages = {23--42},
  publisher = {{SAGE Publications India}},
  issn = {1471-082X},
  doi = {10.1191/1471082X06st106oa},
  urldate = {2023-11-28},
  abstract = {Fitting multilevel models to discrete outcome data is problematic because the discrete distribution of the response variable implies an analytically intractable log-likelihood function. Among a number of approximate methods proposed, second-order penalised quasi-likelihood (PQL) is commonly used and is one of the most accurate. Unfortunately, even the second-order PQL approximation has been shown to produce estimates biased toward zero in certain circumstances. This bias can be marked especially when the data are sparse. One option to reduce this bias is to use Monte-Carlo simulation. A bootstrap bias correction method proposed by Kuk has been implemented in MLwiN. However, a similar technique based on the Robbins-Monro (RM) algorithm is potentially more efficient. An alternative is to use simulated maximum likelihood (SML), either alone or to refine estimates identified by other methods. In this article, we first compare bias correction using the RM algorithm, Kuk's method and SML. We find that SML performs as efficiently as the other two methods and also yields standard errors of the bias-corrected parameter estimates and an estimate of the log-likelihood at the maximum, with which nested models can be compared. Secondly, using simulated and real data examples, we compare SML, second-order Laplace approximation (as implemented in HLM), Markov Chain Monte-Carlo (MCMC) (in MLwiN) and numerical integration using adaptive quadrature methods (in Stata's GLLAMM and in SAS's proc NLMIXED). We find that when the data are sparse, the second-order Laplace approximation produces markedly lower parameter estimates, whereas the MCMC method produces estimates that are noticeably higher than those from the SML and quadrature methods. Although proc NLMIXED is much faster than GLLAMM, it is not designed to fit models of more than two levels. SML produces parameter estimates and log-likelihoods very similar to those from quadrature methods. Further our SML approach extends to handle other link functions, discrete data distributions, non-normal random effects and higher-level models.},
  langid = {english},
  file = {/home/bolker/Zotero/storage/SNN9XS2P/Ng et al. - 2006 - Estimation in generalised linear mixed models with.pdf}
}

@article{westEffects1997,
  title = {Effects of {{Vitamin A}} on {{Growth}} of {{Vitamin A-Deficient Children}}: {{Field Studies}} in {{Nepal1}},2,3},
  shorttitle = {Effects of {{Vitamin A}} on {{Growth}} of {{Vitamin A-Deficient Children}}},
  author = {West, Keith P. and LeClerq, Steven C. and Shrestha, Sharada R. and Wu, Lee S. -F. and Pradhan, Elizabeth K. and Khatry, Subarna K. and Katz, Joanne and Adhikari, Ramesh and Sommer, Alfred},
  year = {1997},
  month = oct,
  journal = {The Journal of Nutrition},
  volume = {127},
  number = {10},
  pages = {1957--1965},
  issn = {0022-3166},
  doi = {10.1093/jn/127.10.1957},
  urldate = {2023-11-28},
  abstract = {Inconsistencies have been observed in the impact of vitamin A (VA) supplementation on early child growth. To help clarify this issue, a cohort of 3377 rural Nepalese, nonxerophthalmic children 12\textendash 60 mo of age were randomized by ward to receive vitamin A [60,000 {$\mu$}g retinol equivalents (RE)] or placebo-control (300 RE) supplementation once every 4 mo and followed for 16 mo. VA had no impact on annual weight gain or linear growth. However, arm circumference (AC) and muscle area (MA) growth improved in VA recipients, by 0.13 cm and 25 mm2, respectively, over controls. Growth of children with xerophthalmia, who were treated with {$\geq$} 120,000 RE at base line, was also compared to that of nonxerophthalmic children, stratified by initial wasting status, and adjusted for sex, baseline age and measurement status. Among initially nonwasted children (AC {$\geq$} 13.5 cm), VA-treated xerophthalmic children (n = 86) gained 0.7 cm more in linear growth than nonxerophthalmic children. Among initially wasted children (AC {$<$} 13.5 cm), VA-treated children (n = 34) gained additional weight (672 g), height (\textasciitilde 1 cm), muscle (76 mm2) and fat (79 mm2) areas, and subscapular skinfold (1.3 mm) compared to changes observed in nonxerophthalmic children. Relative increments in soft tissue growth occurred within 4 mo of VA treatment, while the effect on linear growth was gradual. Moderate-to-severe VA deficiency, marked by xerophthalmia, is likely to impair normal physical growth, but milder stages of deficiency may not have this effect in rural South Asia.},
  keywords = {children,community trial,growth,vitamin A deficiency,xerophthalmia},
  file = {/home/bolker/Zotero/storage/Z942R3BA/S0022316623016048.html}
}

@article{hefleyBasis2017,
  title = {The Basis Function Approach for Modeling Autocorrelation in Ecological Data},
  author = {Hefley, Trevor J. and Broms, Kristin M. and Brost, Brian M. and Buderman, Frances E. and Kay, Shannon L. and Scharf, Henry R. and Tipton, John R. and Williams, Perry J. and Hooten, Mevin B.},
  year = {2017},
  journal = {Ecology},
  volume = {98},
  number = {3},
  pages = {632--646},
  issn = {1939-9170},
  doi = {10.1002/ecy.1674},
  urldate = {2022-07-14},
  abstract = {Analyzing ecological data often requires modeling the autocorrelation created by spatial and temporal processes. Many seemingly disparate statistical methods used to account for autocorrelation can be expressed as regression models that include basis functions. Basis functions also enable ecologists to modify a wide range of existing ecological models in order to account for autocorrelation, which can improve inference and predictive accuracy. Furthermore, understanding the properties of basis functions is essential for evaluating the fit of spatial or time-series models, detecting a hidden form of collinearity, and analyzing large data sets. We present important concepts and properties related to basis functions and illustrate several tools and techniques ecologists can use when modeling autocorrelation in ecological data.},
  langid = {english},
  keywords = {autocorrelation,Bayesian model,collinearity,dimension reduction,semiparametric regression,spatial statistics,time series},
  file = {/home/bolker/Zotero/storage/L7EU9K59/Hefley et al. - 2017 - The basis function approach for modeling autocorre.pdf;/home/bolker/Zotero/storage/KG7C6YGR/ecy.html}
}

@article{woodStable2004,
  title = {Stable and {{Efficient Multiple Smoothing Parameter Estimation}} for {{Generalized Additive Models}}},
  author = {Wood, Simon N},
  year = {2004},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {467},
  pages = {673--686},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214504000000980},
  urldate = {2023-07-02},
  abstract = {Representation of generalized additive models (GAM's) using penalized regression splines allows GAM's to be employed in a straightforward manner using penalized regression methods. Not only is inference facilitated by this approach, but it is also possible to integrate model selection in the form of smoothing parameter selection into model fitting in a computationally efficient manner using well founded criteria such as generalized cross-validation. The current fitting and smoothing parameter selection methods for such models are usually effective, but do not provide the level of numerical stability to which users of linear regression packages, for example, are accustomed. In particular the existing methods cannot deal adequately with numerical rank deficiency of the GAM fitting problem, and it is not straightforward to produce methods that can do so, given that the degree of rank deficiency can be smoothing parameter dependent. In addition, models with the potential flexibility of GAM's can also present practical fitting difficulties as a result of indeterminacy in the model likelihood: Data with many zeros fitted by a model with a log link are a good example. In this article it is proposed that GAM's with a ridge penalty provide a practical solution in such circumstances, and a multiple smoothing parameter selection method suitable for use in the presence of such a penalty is developed. The method is based on the pivoted QR decomposition and the singular value decomposition, so that with or without a ridge penalty it has good error propagation properties and is capable of detecting and coping elegantly with numerical rank deficiency. The method also allows mixtures of user specified and estimated smoothing parameters and the setting of lower bounds on smoothing parameters. In terms of computational efficiency, the method compares well with existing methods. A simulation study compares the method to existing methods, including treating GAM's as mixed models.},
  keywords = {Generalized additive mixed model,Generalized cross-validation,Penalized quasi-likelihood,Regularization,REML,Ridge regression,Smoothing spline analysis of variance,Spline,Stable computation},
  file = {/home/bolker/Zotero/storage/RW2LNVWR/Wood - 2004 - Stable and Efficient Multiple Smoothing Parameter .pdf}
}

@book{hastieElements2009,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H},
  year = {2009},
  publisher = {{Springer}},
  address = {{New York}},
  urldate = {2013-07-02},
  url = {https://hastie.su.domains/Papers/ESLII.pdf},
  abstract = {"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics."--Jacket.},
  isbn = {978-0-387-84858-7 0-387-84858-4 978-0-387-84857-0 0-387-84857-6},
  langid = {english}
}

@article{golubGeneralized1979,
  title = {Generalized {{Cross-Validation}} as a {{Method}} for {{Choosing}} a {{Good Ridge Parameter}}},
  author = {Golub, Gene H. and Heath, Michael and Wahba, Grace},
  year = {1979},
  month = may,
  journal = {Technometrics},
  volume = {21},
  number = {2},
  pages = {215--223},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1979.10489751},
  abstract = {Consider the ridge estimate ({$\lambda$}) for {$\beta$} in the model unknown, ({$\lambda$}) = (X T X + n{$\lambda$}I)-1 X T y. We study the method of generalized cross-validation (GCV) for choosing a good value for {$\lambda$} from the data. The estimate is the minimizer of V({$\lambda$}) given by where A({$\lambda$}) = X(X T X + n{$\lambda$}I)-1 X T . This estimate is a rotation-invariant version of Allen's PRESS, or ordinary cross-validation. This estimate behaves like a risk improvement estimator, but does not require an estimate of {$\sigma$}2, so can be used when n - p is small, or even if p {$\geq$} 2 n in certain cases. The GCV method can also be used in subset selection and singular value truncation methods for regression, and even to choose from among mixtures of these methods.},
  keywords = {Cross-validation,Ridge parameter,Ridge regression},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1979.10489751},
  file = {/home/bolker/Zotero/storage/4FG7CYQL/Golub et al. - 1979 - Generalized Cross-Validation as a Method for Choos.pdf}
}

@misc{larsenGAM2015,
  title = {{{GAM}}: {{The Predictive Modeling Silver Bullet}} | {{Stitch Fix Technology}} \textendash{} {{Multithreaded}}},
  shorttitle = {{{GAM}}},
  author = {Larsen, Kim},
  year = {2015},
  month = jul,
  journal = {MultiThreaded (StitchFix)},
  abstract = {Imagine that you step into a room of data scientists; the dress code is casual and the scent of strong coffee is hanging in the air. You ask the data scienti...},
  url = {https://multithreaded.stitchfix.com/blog/2015/07/30/gam/},
  file = {/home/bolker/Zotero/storage/5XYN6F7U/gam.html}
}

@article{perperogloureview2019a,
	title = {A review of spline function procedures in {R}},
	volume = {19},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-019-0666-3},
	doi = {10.1186/s12874-019-0666-3},
	abstract = {With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the STRengthening Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R.},
	number = {1},
	urldate = {2023-02-01},
	journal = {BMC Medical Research Methodology},
	author = {Perperoglou, Aris and Sauerbrei, Willi and Abrahamowicz, Michal and Schmid, Matthias},
	month = mar,
	year = {2019},
	keywords = {Functional form of continuous covariates, Multivariable modelling},
	pages = {46},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/6RVM58RB/Perperoglou et al. - 2019 - A review of spline function procedures in R.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/YHYJLI4J/s12874-019-0666-3.html:text/html},
}



@article{dominiciUse2002,
	title = {On the {Use} of {Generalized} {Additive} {Models} in {Time}-{Series} {Studies} of {Air} {Pollution} and {Health}},
	volume = {156},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwf062},
	doi = {10.1093/aje/kwf062},
	abstract = {The widely used generalized additive models (GAM) method is a flexible and effective technique for conducting nonlinear regression analysis in time-series studies of the health effects of air pollution. When the data to which the GAM are being applied have two characteristics—1) the estimated regression coefficients are small and 2) there exist confounding factors that are modeled using at least two nonparametric smooth functions—the default settings in the gam function of the S-Plus software package (version 3.4) do not assure convergence of its iterative estimation procedure and can provide biased estimates of regression coefficients and standard errors. This phenomenon has occurred in time-series analyses of contemporary data on air pollution and mortality. To evaluate the impact of default implementation of the gam software on published analyses, the authors reanalyzed data from the National Morbidity, Mortality, and Air Pollution Study (NMMAPS) using three different methods: 1) Poisson regression with parametric nonlinear adjustments for confounding factors; 2) GAM with default convergence parameters; and 3) GAM with more stringent convergence parameters than the default settings. The authors found that pooled NMMAPS estimates were very similar under the first and third methods but were biased upward under the second method. Am J Epidemiol 2002;156:193–203.},
	number = {3},
	urldate = {2023-11-30},
	journal = {American Journal of Epidemiology},
	author = {Dominici, Francesca and McDermott, Aidan and Zeger, Scott L. and Samet, Jonathan M.},
	month = aug,
	year = {2002},
	pages = {193--203},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/WDL6LKVJ/Dominici et al. - 2002 - On the Use of Generalized Additive Models in Time-.pdf:application/pdf},
}

@article{pengModel2006,
	title = {Model {Choice} in {Time} {Series} {Studies} of {Air} {Pollution} and {Mortality}},
	volume = {169},
	issn = {0964-1998},
	url = {https://doi.org/10.1111/j.1467-985X.2006.00410.x},
	doi = {10.1111/j.1467-985X.2006.00410.x},
	abstract = {Multicity time series studies of particulate matter and mortality and morbidity have provided evidence that daily variation in air pollution levels is associated with daily variation in mortality counts. These findings served as key epidemiological evidence for the recent review of the US national ambient air quality standards for particulate matter. As a result, methodological issues concerning time series analysis of the relationship between air pollution and health have attracted the attention of the scientific community and critics have raised concerns about the adequacy of current model formulations. Time series data on pollution and mortality are generally analysed by using log-linear, Poisson regression models for overdispersed counts with the daily number of deaths as outcome, the (possibly lagged) daily level of pollution as a linear predictor and smooth functions of weather variables and calendar time used to adjust for time-varying confounders. Investigators around the world have used different approaches to adjust for confounding, making it difficult to compare results across studies. To date, the statistical properties of these different approaches have not been comprehensively compared. To address these issues, we quantify and characterize model uncertainty and model choice in adjusting for seasonal and long-term trends in time series models of air pollution and mortality. First, we conduct a simulation study to compare and describe the properties of statistical methods that are commonly used for confounding adjustment. We generate data under several confounding scenarios and systematically compare the performance of the various methods with respect to the mean-squared error of the estimated air pollution coefficient. We find that the bias in the estimates generally decreases with more aggressive smoothing and that model selection methods which optimize prediction may not be suitable for obtaining an estimate with small bias. Second, we apply and compare the modelling approaches with the National Morbidity, Mortality, and Air Pollution Study database which comprises daily time series of several pollutants, weather variables and mortality counts covering the period 1987–2000 for the largest 100 cities in the USA. When applying these approaches to adjusting for seasonal and long-term trends we find that the Study's estimates for the national average effect of PM10 at lag 1 on mortality vary over approximately a twofold range, with 95\% posterior intervals always excluding zero risk.},
	number = {2},
	urldate = {2023-11-30},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Peng, Roger D. and Dominici, Francesca and Louis, Thomas A.},
	month = mar,
	year = {2006},
	pages = {179--203},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/348W44CD/Peng et al. - 2006 - Model Choice in Time Series Studies of Air Polluti.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/XB2BQNDL/7085031.html:text/html},
}

@article{ramsayEffect2003,
	title = {The {Effect} of {Concurvity} in {Generalized} {Additive} {Models} {Linking} {Mortality} to {Ambient} {Particulate} {Matter}},
	volume = {14},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/Fulltext/2003/01000/The_Effect_of_Concurvity_in_Generalized_Additive.9.aspx},
	abstract = {In recent years, a number of studies have applied generalized additive models to time series data to estimate associations between exposure to air pollution and cardiorespiratory morbidity and mortality. If concurvity, the nonparametric analogue of multicollinearity, is present in the data, statistical software such as S-plus can seriously underestimate the variance of fitted model parameters, leading to significance tests with inflated type 1 error. This paper uses computer simulation and analyses of actual epidemiologic data to explore this underestimation of standard errors. We provide a method for assessing concurvity in data and an alternate class of models that is unaffected by concurvity. We argue that some degree of concurvity is likely to be present in all epidemiologic time series datasets and we explore through the use of meta-analysis the possible impact of concurvity on the existing body of work relating ambient levels of sulfate particles to mortality.},
	language = {en-US},
	number = {1},
	urldate = {2023-11-30},
	journal = {Epidemiology},
	author = {Ramsay, Timothy O. and Burnett, Richard T. and Krewski, Daniel},
	month = jan,
	year = {2003},
	pages = {18},
}


@article{pedersenHierarchical2019,
	title = {Hierarchical generalized additive models in ecology: an introduction with mgcv},
	volume = {7},
	issn = {2167-8359},
	shorttitle = {Hierarchical generalized additive models in ecology},
	url = {https://peerj.com/articles/6876},
	doi = {10.7717/peerj.6876},
	abstract = {In this paper, we discuss an extension to two popular approaches to modeling complex structures in ecological data: the generalized additive model (GAM) and the hierarchical model (HGLM). The hierarchical GAM (HGAM), allows modeling of nonlinear functional relationships between covariates and outcomes where the shape of the function itself varies between different grouping levels. We describe the theoretical connection between HGAMs, HGLMs, and GAMs, explain how to model different assumptions about the degree of intergroup variability in functional response, and show how HGAMs can be readily fitted using existing GAM software, the mgcv package in R. We also discuss computational and statistical issues with fitting these models, and demonstrate how to fit HGAMs on example data. All code and data used to generate this paper are available at: github.com/eric-pedersen/mixed-effect-gams.},
	language = {en},
	urldate = {2022-03-07},
	journal = {PeerJ},
	author = {Pedersen, Eric J. and Miller, David L. and Simpson, Gavin L. and Ross, Noam},
	month = may,
	year = {2019},
	note = {Publisher: PeerJ Inc.},
	pages = {e6876},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/RFJAXEKX/Pedersen et al. - 2019 - Hierarchical generalized additive models in ecolog.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/VZB3UTPB/6876.html:text/html},
}



@article{pyaShape2015,
	title = {Shape constrained additive models},
	volume = {25},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-013-9448-7},
	doi = {10.1007/s11222-013-9448-7},
	abstract = {A framework is presented for generalized additive modelling under shape constraints on the component functions of the linear predictor of the GAM. We represent shape constrained model components by mildly non-linear extensions of P-splines. Models can contain multiple shape constrained and unconstrained terms as well as shape constrained multi-dimensional smooths. The constraints considered are on the sign of the first or/and the second derivatives of the smooth terms. A key advantage of the approach is that it facilitates efficient estimation of smoothing parameters as an integral part of model estimation, via GCV or AIC, and numerically robust algorithms for this are presented. We also derive simulation free approximate Bayesian confidence intervals for the smooth components, which are shown to achieve close to nominal coverage probabilities. Applications are presented using real data examples including the risk of disease in relation to proximity to municipal incinerators and the association between air pollution and health.},
	language = {en},
	number = {3},
	urldate = {2023-10-18},
	journal = {Statistics and Computing},
	author = {Pya, Natalya and Wood, Simon N.},
	month = may,
	year = {2015},
	keywords = {Generalized additive model, Convex smoothing, Monotonic smoothing, P-splines},
	pages = {543--559},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/HY75ERFU/Pya and Wood - 2015 - Shape constrained additive models.pdf:application/pdf},

}
@article{quinnStool2021,
  title = {Stool Studies Don't Pass the Sniff Test: A Systematic Review of Human Gut Microbiome Research Suggests Widespread Misuse of Machine Learning},
  shorttitle = {Stool {{Studies Don}}'t {{Pass}} the {{Sniff Test}}},
  author = {Quinn, Thomas P.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.03611 [q-bio]},
  eprint = {2107.03611},
  primaryclass = {q-bio},
  urldate = {2021-11-14},
  abstract = {In the machine learning culture, an independent test set is required for proper model verification. Failures in model verification, including test set omission and test set leakage, make it impossible to know whether or not a trained model is fit for purpose. In this article, we present a systematic review and quantitative analysis of human gut microbiome classification studies, conducted to measure the frequency and impact of test set omission and test set leakage on area under the receiver operating curve (AUC) reporting. Among 102 articles included for analysis, we find that only 12\% of studies report a bona fide test set AUC, meaning that the published AUCs for 88\% of studies cannot be trusted at face value. Our findings cast serious doubt on the general validity of research claiming that the gut microbiome has high diagnostic or prognostic potential in human disease.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Biology - Genomics},
  file = {/home/bolker/Documents/zotero_new/storage/KZXQTVEH/Quinn - 2021 - Stool Studies Don't Pass the Sniff Test A Systema.pdf}
}

@misc{bussolaAI2020,
  title = {{AI} Slipping on Tiles: Data Leakage in Digital Pathology},
  shorttitle = {{{AI}} Slipping on Tiles},
  author = {Bussola, Nicole and Marcolini, Alessia and Maggio, Valerio and Jurman, Giuseppe and Furlanello, Cesare},
  year = {2020},
  month = nov,
  number = {arXiv:1909.06539},
  eprint = {1909.06539},
  primaryclass = {eess, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.06539},
  urldate = {2023-02-24},
  abstract = {Reproducibility of AI models on biomedical data still stays as a major concern for their acceptance into the clinical practice. Initiatives for reproducibility in the development of predictive biomarkers as the MAQC Consortium already underlined the importance of appropriate Data Analysis Plans (DAPs) to control for different types of bias, including data leakage from the training to the test set. In the context of digital pathology, the leakage typically lurks in weakly designed experiments not accounting for the subjects in their data partitioning schemes. This issue is then exacerbated when fractions or subregions of slides (i.e. "tiles") are considered. Despite this aspect is largely recognized by the community, we argue that it is often overlooked. In this study, we assess the impact of data leakage on the performance of machine learning models trained and validated on multiple histology data collection. We prove that, even with a properly designed DAP (10x5 repeated cross-validation), predictive scores can be inflated up to 41\% when tiles from the same subject are used both in training and validation sets by deep learning models. We replicate the experiments for \$4\$ classification tasks on 3 histopathological datasets, for a total of 374 subjects, 556 slides and more than 27,000 tiles. Also, we discuss the effects of data leakage on transfer learning strategies with models pre-trained on general-purpose datasets or off-task digital pathology collections. Finally, we propose a solution that automates the creation of leakage-free deep learning pipelines for digital pathology based on histolab, a novel Python package for histology data preprocessing. We validate the solution on two public datasets (TCGA and GTEx).},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing,Quantitative Biology - Quantitative Methods},
  file = {/home/bolker/Documents/zotero_new/storage/4SX8J7WC/Bussola et al. - 2020 - AI slipping on tiles data leakage in digital path.pdf;/home/bolker/Documents/zotero_new/storage/XH6QD7DJ/1909.html}
}

@article{kapoorLeakageReproducibilityCrisis2023,
  title = {Leakage and the Reproducibility Crisis in Machine-Learning-Based Science},
  author = {Kapoor, Sayash and Narayanan, Arvind},
  year = {2023},
  month = sep,
  journal = {Patterns},
  volume = {4},
  number = {9},
  pages = {100804},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2023.100804},
  urldate = {2024-09-01},
  abstract = {Machine-learning (ML) methods have gained prominence in the quantitative sciences. However, there are~many known methodological pitfalls, including data leakage, in ML-based science. We systematically investigate reproducibility issues in ML-based science. Through a survey of literature in fields that have~adopted ML methods, we find 17 fields where leakage has been found, collectively affecting 294 papers~and, in some cases, leading to wildly overoptimistic conclusions. Based on our survey, we introduce a detailed taxonomy of eight types of leakage, ranging from textbook errors to open research problems. We propose that researchers test for each type of leakage by filling out model info sheets, which we~introduce. Finally, we conduct a reproducibility study of civil war prediction, where complex ML models are believed to vastly outperform traditional statistical models such as logistic regression (LR). When the~errors are corrected, complex ML models do not perform substantively better than decades-old LR models.},
  keywords = {leakage,machine learning,reproducibility},
  file = {/home/bolker/Documents/zotero_new/storage/R8FX2UA3/Kapoor and Narayanan - 2023 - Leakage and the reproducibility crisis in machine-.pdf;/home/bolker/Documents/zotero_new/storage/JXYN8PCB/S2666389923001599.html}
}

@article{heinzeRegressionRegretsInitial2024,
  title = {Regression without Regrets --Initial Data Analysis Is a Prerequisite for Multivariable Regression},
  author = {Heinze, Georg and Baillie, Mark and Lusa, Lara and Sauerbrei, Willi and Schmidt, Carsten Oliver and Harrell, Frank E. and Huebner, Marianne and {on behalf of TG2 and TG3 of the STRATOS initiative}},
  year = {2024},
  month = aug,
  journal = {BMC Medical Research Methodology},
  volume = {24},
  number = {1},
  pages = {178},
  issn = {1471-2288},
  doi = {10.1186/s12874-024-02294-3},
  urldate = {2024-08-22},
  abstract = {Statistical regression models are used for predicting outcomes based on the values of some predictor variables or for describing the association of an outcome with predictors. With a data set at hand, a regression model can be easily fit with standard software packages. This bears the risk that data analysts may rush to perform sophisticated analyses without sufficient knowledge of basic properties, associations in and errors of their data, leading to wrong interpretation and presentation of the modeling results that lacks clarity. Ignorance about special features of the data such as redundancies or particular distributions may even invalidate the chosen analysis strategy. Initial data analysis (IDA) is prerequisite to regression analyses as it provides knowledge about the data needed to confirm the appropriateness of or to refine a chosen model building strategy, to interpret the modeling results correctly, and to guide the presentation of modeling results. In order to facilitate reproducibility, IDA needs to be preplanned, an IDA plan should be included in the general statistical analysis plan of a research project, and results should be well documented. Biased statistical inference of the final regression model can be minimized if IDA abstains from evaluating associations of outcome and predictors, a key principle of IDA. We give advice on which aspects to consider in an IDA plan for data screening in the context of regression modeling to supplement the statistical analysis plan. We illustrate this IDA plan for data screening in an example of a typical diagnostic modeling project and give recommendations for data visualizations.},
  keywords = {Data screening,Functional form,IDA framework,Initial data analysis,Regression models,Reporting,STRATOS Initiative,Variable selection,Variable transformation}
}

@misc{lindelovCommonStatisticalTests2019,
  title = {Common Statistical Tests Are Linear Models (or: How to Teach Stats)},
  author = {Lindel{\o}v, Jonas Kristoffer},
  year = {2019},
  month = jun,
  urldate = {2024-09-05},
  howpublished = {https://lindeloev.github.io/tests-as-linear/},
  file = {/home/bolker/Documents/zotero_new/storage/ET3IPCG9/tests-as-linear.html}
}


@misc{cranmerTransformation,
	title = {Transformation properties of the likelihood and posterior},
	url = {https://cranmer.github.io/stats-ds-book/distributions/invariance-of-likelihood-to-reparameterizaton.html},
	abstract = {In the notebooks How do distributions transform under a change of variables? and Change of variables with autodiff we saw how a distribution p\_X for a random variable X transforms under a change of...},
	urldate = {2024-09-12},
	author = {Cranmer, Kyle},
	date = {2020}
}

@misc{leekDeterministicStatisticalMachine2012,
  title = {A Deterministic Statistical Machine},
  author = {Leek, Jeff and Peng, Roger and Irizarry, Rafa},
  year = {2012},
  month = aug,
  journal = {Simply Statistics},
  urldate = {2013-10-02},
  url= {http://web.archive.org/web/20130312053608/https://simplystatistics.org/2012/08/27/a-deterministic-statistical-machine/}
}

@misc{leekResearcherDegreesFreedom2013,
  title = {The Researcher Degrees of Freedom -- Recipe Tradeoff in Data Analysis},
  author = {Leek, Jeff and Peng, Roger and Irizarry, Rafa},
  year = {2013},
  month = jul,
  journal = {Simply Statistics},
  urldate = {2013-10-02},
  url = {http://web.archive.org/web/20130805213235/http://simplystatistics.org/2013/07/31/the-researcher-degrees-of-freedom-recipe-tradeoff-in-data-analysis/}
}

@misc{gelmanWhassupGlm2011,
  title = {Whassup with glm()?},
  author = {Gelman, Andrew},
  year = {2011},
  month = may,
  journal = {Statistical Modeling, Causal Inference, and Social Science},
  urldate = {2024-09-24},
  url = {https://statmodeling.stat.columbia.edu/2011/05/04/whassup\_with\_gl/}
}

@misc{mountHowRobustLogistic2012,
  title = {How Robust Is Logistic Regression?},
  author = {Mount, John},
  year = {2012},
  month = aug,
  journal = {Win Vector LLC},
  urldate = {2024-09-24},
  abstract = {Logistic Regression is a popular and effective technique for modeling categorical outcomes as a function of both continuous and categorical variables. The question is: how robust is it? Or: how rob{\dots}},
  url = {https://win-vector.com/2012/08/23/how-robust-is-logistic-regression/}
}

@book{dobson_introduction_2008,
	edition = {3},
	title = {An Introduction to Generalized Linear Models, Third Edition},
	isbn = {1584889500},
	publisher = {Chapman and {Hall/CRC}},
	author = {Dobson, Annette J. and Barnett, Adrian},
	month = may,
	year = {2008}
}

@incollection{myers_appendix_2010,
	title = {Appendix {A.6}: Computational Details for {GLMs} for a Noncanonical Link},
	copyright = {Copyright © 2010 John Wiley \& Sons, Inc. All rights reserved.},
	isbn = {9780470556986},
	shorttitle = {Appendix A.6},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470556986.app6/summary},
	language = {en},
	urldate = {2013-09-25},
	booktitle = {Generalized Linear Models},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Myers, Raymond H. and Montgomery, Douglas C. and Vining, G. Geoffrey and Robinson, Timothy J.},
	year = {2010},
	pages = {481–483}
}


@article{marschner_glm2_2011,
	title = {glm2: Fitting Generalized Linear Models with Convergence Problems},
	volume = {3},
	url = {http://journal.r-project.org/archive/2011-2/RJournal_2011-2_Marschner.pdf},
	number = {2},
	journal = {The R Journal},
	author = {Marschner, Ian C.},
	month = dec,
	year = {2011},
	pages = {12–15}
}


@misc{Robinson2010,
    title = {Why do we make a big fuss about using {Fisher} scoring when we fit a {GLM}?},
    date = {2010},
    AUTHOR = {Andrew Robinson},
    HOWPUBLISHED = {Cross Validated},
    URL = {https://stats.stackexchange.com/q/205}
}


@article{larsonFamilyPlanningBangladesh1992,
  title = {Family {{Planning}} in {{Bangladesh}}: {{An Unlikely Success Story}}},
  shorttitle = {Family {{Planning}} in {{Bangladesh}}},
  author = {Larson, Ann and Mitra, S. N.},
  year = {1992},
  journal = {International Family Planning Perspectives},
  volume = {18},
  number = {4},
  eprint = {2133539},
  eprinttype = {jstor},
  pages = {123--144},
  publisher = {Guttmacher Institute},
  issn = {0190-3187},
  doi = {10.2307/2133539},
  urldate = {2024-09-29},
  abstract = {The results of two independent national surveys conducted in 1989 show that Bangladesh has achieved a moderate level of contraceptive use: Among currently married women under 50 years old, 31\% use a contraceptive method and almost 25\% use a modern method. In addition, 44\% of ever-married women younger than 50 have used a method at some time. The method mix, historically characterized by use of a wide range of methods, is increasingly dominated by oral contraceptives, which accounted for 29\% of use in 1989. Reflecting the increase in contraceptive prevalence, the total fertility rate declined from seven lifetime births during the mid-1970s to about five lifetime births by the end of the 1980s. Mean desired family size in 1989, however, was three children, indicating that there may still be considerable unmet need for family planning services.}
}

@article{jorgensenExponentialDispersionModels1987,
  title = {Exponential {{Dispersion Models}}},
  author = {J{\o}rgensen, Bent},
  year = {1987},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {49},
  number = {2},
  pages = {127--145},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1987.tb01685.x},
  urldate = {2024-09-30},
  abstract = {We study general properties of the class of exponential dispersion models, which is the multivariate generalization of the error distribution of Nelder and Wedderburn's (1972) generalized linear models. Since any given moment generating function generates an exponential dispersion model, there exists a multitude of exponential dispersion models, and some new examples are introduced. General results on convolution and asymptotic normality of exponential dispersion models are presented. Asymptotic theory is discussed, including a new small-dispersion asymptotic framework, which extends the domain of application of large-sample theory. Procedures for constructing new exponential dispersion models for correlated data are introduced, including models for longitudinal data and variance components. The results of the paper unify and generalize standard results for distributions such as the Poisson, the binomial, the negative binomial, the normal, the gamma, and the inverse Gaussian distributions.}
}

@book{bolkerEcological2008,
  title = {Ecological {{Models}} and {{Data}} in {{R}}},
  author = {Bolker, Benjamin M.},
  year = {2008},
  publisher = {Princeton University Press},
  address = {Princeton, NJ}
}

@article{bolkerStrategiesFittingNonlinear2013,
  title = {Strategies for Fitting Nonlinear Ecological Models in {{R}}, {{AD Model Builder}}, and {{BUGS}}},
  author = {Bolker, Benjamin M. and Gardner, Beth and Maunder, Mark and Berg, Casper W. and Brooks, Mollie and Comita, Liza and Crone, Elizabeth and Cubaynes, Sarah and Davies, Trevor and {de Valpine}, Perry and Ford, Jessica and Gimenez, Olivier and K{\'e}ry, Marc and Kim, Eun Jung and {Lennert-Cody}, Cleridy and Magnusson, Arni and Martell, Steve and Nash, John and Nielsen, Anders and Regetz, Jim and Skaug, Hans and Zipkin, Elise},
  year = {2013},
  month = jun,
  journal = {Methods in Ecology and Evolution},
  volume = {4},
  number = {6},
  pages = {501--512},
  issn = {2041210X},
  doi = {10.1111/2041-210X.12044},
  urldate = {2013-06-11},
  file = {/home/bolker/Documents/zotero_new/storage/U83CEWVU/Bolker et al_2013_Strategies for fitting nonlinear ecological models in R, AD Model Builder, and.pdf}
}

@book{pressNumerical2007,
  title = {Numerical {{Recipes}} 3rd {{Edition}}: {{The Art}} of {{Scientific Computing}}},
  shorttitle = {Numerical {{Recipes}} 3rd {{Edition}}},
  author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
  year = {2007},
  month = sep,
  edition = {3},
  publisher = {Cambridge University Press},
  isbn = {0-521-88068-8}
}


@article{friedmanRegularization2010b,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  month = feb,
  journal = {Journal of Statistical Software},
  volume = {33},
  pages = {1--22},
  issn = {1548-7660},
  doi = {10.18637/jss.v033.i01},
  urldate = {2023-02-10},
  abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multi- nomial regression problems while the penalties include {$\ell$}1 (the lasso), {$\ell$}2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
  copyright = {Copyright (c) 2009 Jerome H. Friedman, Trevor Hastie, Rob Tibshirani},
  langid = {english},
  note = {https://doi.org/10.18637/jss.v033.i01},
  file = {/home/bolker/Documents/zotero_new/storage/QJ6R5ZJE/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf}
}

@book{jamesintroduction2013,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {112},
  publisher = {Springer},
  file = {/home/bolker/Documents/zotero_new/storage/KY22P8UZ/James et al. - 2013 - An introduction to statistical learning.pdf;/home/bolker/Documents/zotero_new/storage/AQBFJVSN/10.html}
}

@article{kristensenTMB2016,
  title = {{TMB} : Automatic Differentiation and {Laplace} Approximation},
  author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper W. and Skaug, Hans and Bell, Bradley M.},
  year = {2016},
  journal = {Journal of Statistical Software},
  volume = {70},
  number = {5},
  issn = {1548-7660},
  doi = {10.18637/jss.v070.i05},
  urldate = {2017-05-23},
  langid = {english},
  note = {http://www.jstatsoft.org/v70/i05/},
  file = {/home/bolker/Documents/zotero_new/storage/B35XN3DP/v70i05.pdf}

@article{greenlandPenalizationBiasReduction2015,
  title = {Penalization, Bias Reduction, and Default Priors in Logistic and Related Categorical and Survival Regressions},
  author = {Greenland, Sander and Mansournia, Mohammad Ali},
  year = {2015},
  journal = {Statistics in Medicine},
  volume = {34},
  number = {23},
  pages = {3133--3143},
  issn = {1097-0258},
  doi = {10.1002/sim.6537},
  urldate = {2024-10-07},
  abstract = {Penalization is a very general method of stabilizing or regularizing estimates, which has both frequentist and Bayesian rationales. We consider some questions that arise when considering alternative penalties for logistic regression and related models. The most widely programmed penalty appears to be the Firth small-sample bias-reduction method (albeit with small differences among implementations and the results they provide), which corresponds to using the log density of the Jeffreys invariant prior distribution as a penalty function. The latter representation raises some serious contextual objections to the Firth reduction, which also apply to alternative penalties based on t-distributions (including Cauchy priors). Taking simplicity of implementation and interpretation as our chief criteria, we propose that the log-F(1,1) prior provides a better default penalty than other proposals. Penalization based on more general log-F priors is trivial to implement and facilitates mean-squared error reduction and sensitivity analyses of penalty strength by varying the number of prior degrees of freedom. We caution however against penalization of intercepts, which are unduly sensitive to covariate coding and design idiosyncrasies. Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bayes estimators,bias correction,Firth bias reduction,Jeffreys prior,logistic regression,maximum likelihood,penalized likelihood,regularization,shrinkage,sparse data,stabilization},
  file = {/home/bolker/Documents/zotero_new/storage/D4DUY9CU/Greenland and Mansournia - 2015 - Penalization, bias reduction, and default priors in logistic and related categorical and survival re.pdf;/home/bolker/Documents/zotero_new/storage/4XSMVM4R/sim.html}
}

@inproceedings{firthGeneralizedLinearModels1992,
  title = {Generalized Linear Models and {Jeffreys} Priors: An Iterative Weighted Least-Squares Approach},
  booktitle = {Computational Statistics},
  author = {Firth, D.},
  editor = {Dodge, Yadolah and Whittaker, Joe},
  year = {1992},
  pages = {553--557},
  publisher = {Physica-Verlag HD},
  address = {Heidelberg},
  doi = {10.1007/978-3-662-26811-7_76},
  abstract = {Use of the Jeffreys invariant prior in generalized linear models has been studied recently by a number of authors. In models with canonical link, the posterior mode has asymptotic bias of smaller order than that of the maximum likelihood estimate. In this paper a new algorithm for calculation of the posterior mode is developed. The algorithm makes use of the iterative weighted least-squares method commonly used for maximum likelihood calculations, so that implementation is possible in standard regression software, such as GLIM. Comparison is made with a more `direct' approach using Newton's method, found to be easily implemented in the object-oriented modelling environment of LISP-STAT.},
  isbn = {978-3-662-26811-7},
  langid = {english}
}

@article{kosmidisJeffreyspriorPenaltyFiniteness2021,
  title = {Jeffreys-Prior Penalty, Finiteness and Shrinkage in Binomial-Response Generalized Linear Models},
  author = {Kosmidis, Ioannis and Firth, David},
  year = {2021},
  month = mar,
  journal = {Biometrika},
  volume = {108},
  number = {1},
  pages = {71--82},
  issn = {0006-3444},
  doi = {10.1093/biomet/asaa052},
  urldate = {2024-10-07},
  abstract = {Penalization of the likelihood by Jeffreys' invariant prior, or a positive power thereof, is shown to produce finite-valued maximum penalized likelihood estimates in a broad class of binomial generalized linear models. The class of models includes logistic regression, where the Jeffreys-prior penalty is known additionally to reduce the asymptotic bias of the maximum likelihood estimator, and models with other commonly used link functions, such as probit and log-log. Shrinkage towards equiprobability across observations, relative to the maximum likelihood estimator, is established theoretically and studied through illustrative examples. Some implications of finiteness and shrinkage for inference are discussed, particularly when inference is based on Wald-type procedures. A widely applicable procedure is developed for computation of maximum penalized likelihood estimates, by using repeated maximum likelihood fits with iteratively adjusted binomial responses and totals. These theoretical results and methods underpin the increasingly widespread use of reduced-bias and similarly penalized binomial regression models in many applied fields.}
}
